{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Reasoning Integrity Standard (RIS)","text":"The Global Reasoning Integrity Standardfor the Autonomous Age  <p>   Reasoning Integrity, defined and measurable. RIS provides a formal, enterprise-grade framework for evaluating and governing   the stability, predictability, and structural reliability of AI reasoning across models, agents, and cognitive systems. </p> <p>   Developed and maintained by Atom Labs - Standards Division. </p>"},{"location":"#1-what-ris-is","title":"1. What RIS is","text":"<p>The Reasoning Integrity Standard (RIS) is the first formal framework dedicated to measuring the integrity of AI reasoning itself.</p> <p>Where existing frameworks govern:</p> <ul> <li>security  </li> <li>privacy  </li> <li>compliance  </li> <li>safety  </li> </ul> <p>RIS governs how a system reasons - whether its internal reasoning process is stable, coherent, bounded, and auditable under real-world conditions.</p> <p>RIS v1.0 defines:</p> <ul> <li>a multi-level reasoning integrity model (RIS-0 \u2192 RIS-4)  </li> <li>a measurement framework and scoring methodology  </li> <li>mandatory controls and governance requirements  </li> <li>evaluation and audit procedures  </li> <li>a reference implementation (LCAC) and operational pipeline  </li> </ul> <p>The objective is simple:</p> <p>Make AI reasoning integrity a measurable, reportable, and certifiable property.</p>"},{"location":"#2-why-ris-exists","title":"2. Why RIS Exists","text":"<p>Modern AI systems exhibit powerful but fragile reasoning behaviour:</p> <ul> <li>reasoning chains can drift over time and context  </li> <li>identical prompts can produce materially different reasoning paths  </li> <li>multi-agent systems amplify variance and drift  </li> <li>tool calls, memory, and upstream systems influence reasoning in opaque ways  </li> <li>most organizations have no standard way to assess reasoning stability  </li> </ul> <p>This creates structural gaps in:</p> <ul> <li>safety-critical deployments  </li> <li>enterprise risk and governance  </li> <li>procurement and vendor due diligence  </li> <li>model change management and regression testing  </li> <li>regulatory expectations around AI behaviour  </li> </ul> <p>RIS closes this gap. It provides a concrete, testable standard for reasoning stability and governance.</p>"},{"location":"#3-ris-in-the-atom-labs-ecosystem","title":"3. RIS in the Atom Labs Ecosystem","text":"<p>RIS is the cornerstone of Atom Labs\u2019 reasoning governance stack:</p> <ul> <li> <p>RIS Standard   Formal specification, controls, levels, and methodology.</p> </li> <li> <p>RIS Evaluator   Scoring engine and pipeline that executes RIS evaluations and generates scorecards.</p> </li> <li> <p>LCAC (Least-Context Access Control) - Reference Implementation   Operationalizes reasoning governance, drift analysis, stability thresholds, and scorecard generation.</p> </li> <li> <p>LCAC Governor   Cognitive policy engine that monitors models in production, tracks drift and variance, and can gate decisions based on RIS-aligned thresholds.</p> </li> <li> <p>RIS Portal   This documentation + analytics portal, surfacing reports, leaderboards, badges, and CII (Cognitive Integrity Index).</p> </li> <li> <p>RIS Certification &amp; Submission (v1.1+)   Formalized certification workflows, templates, and checklists for internal and external models.</p> </li> <li> <p>Future Atom Labs Stack   RIS integrated across Atom Labs systems: LCAC, VANTA OS, Markets Intelligence, Executor Engine, and cognitive observability services.</p> </li> </ul> <p>RIS anchors the entire ecosystem as the standard of record for reasoning stability.</p>"},{"location":"#4-ris-levels","title":"4. RIS Levels","text":"<p>RIS defines five integrity levels:</p> Level Name Summary RIS-0 Uncontrolled Reasoning No stability guarantees; unsuitable for critical use RIS-1 Drift-Sensitive Reasoning Highly sensitive to context and perturbation RIS-2 Semi-Stable Reasoning Partially consistent; acceptable for low-risk flows RIS-3 Controlled, Production-Grade Reasoning Stable within defined envelopes; enterprise-ready RIS-4 High-Integrity, Safety-Critical Reasoning Strictly controlled; auditable for regulated use <p>Each level carries:</p> <ul> <li>eligibility criteria  </li> <li>required controls  </li> <li>evidence expectations  </li> <li>variance / drift parameters  </li> <li>governance implications  </li> </ul> <p>Level selection is a risk decision, not just a technical metric.</p>"},{"location":"#5-measurement-pillars","title":"5. Measurement Pillars","text":"<p>RIS evaluations cover five core pillars:</p> <ol> <li> <p>Chain Stability    Reliability of multi-step reasoning across sessions, users, and contexts.</p> </li> <li> <p>Semantic Coherence    Internal logical consistency and structural quality of reasoning outputs.</p> </li> <li> <p>Drift Sensitivity    How easily reasoning degrades due to time, prompt changes, data shifts, tools, or configuration.</p> </li> <li> <p>Variance Envelope Compliance    Predictability of reasoning within an acceptable variance band under repeated evaluation.</p> </li> <li> <p>Boundary Governance    Adherence to defined operational, ethical, and safety boundaries.</p> </li> </ol> <p>Together, these pillars determine whether a system\u2019s reasoning can be trusted in production and regulated environments.</p>"},{"location":"#6-ris-evaluation-pipeline","title":"6. RIS Evaluation Pipeline","text":"<p>A typical RIS evaluation workflow consists of:</p> <ol> <li>Scenario and sample design </li> <li>Baseline runs for chain stability </li> <li>Perturbation runs for drift and variance </li> <li>Semantic coherence scoring </li> <li>Boundary and control checks </li> <li>Aggregation into RIS metrics and indicators </li> <li>RIS Level assignment (0-4) </li> <li>Scorecard and report generation </li> <li>Optional CII (Cognitive Integrity Index) computation </li> <li>Publication to RIS Portal or internal systems</li> </ol> <p>The RIS Evaluator and LCAC implementation provide reference execution pipelines for this flow, backed by Redis and LCAC governance metadata when deployed in the full Atom Labs stack.</p>"},{"location":"#7-quick-entry-points","title":"7. Quick Entry Points","text":"<p>Use this portal to:</p> <ul> <li> <p>Read the Standard   Use the left navigation to access Sections 0\u201313 (Levels, Controls, Methodology, Risk, Audit, Mapping).</p> </li> <li> <p>Review Overview and Concepts   See the narrative introduction in Overview \u2192 Reasoning Integrity Standard (RIS) - Overview.</p> </li> <li> <p>Explore Analytics   View sample and live evaluations under Analytics \u2192 Reports, Leaderboard, Badges, and CII.</p> </li> <li> <p>Plan Integration   Use the API, SDK, and CICD integration guides to map RIS into your environment.</p> </li> <li> <p>Prepare for Certification (v1.1+)   Align with the RIS Certification Program, Conformance Statement, and Evaluation Report templates.</p> </li> </ul>"},{"location":"#8-ris-lcac-governor-operational-path","title":"8. RIS + LCAC + Governor: Operational Path","text":"<p>In a full Atom Labs deployment, RIS connects to runtime systems as follows:</p> <ul> <li>RIS Standard defines the target behaviour and controls.  </li> <li>LCAC Evaluator runs structured benchmark suites and stability tests.  </li> <li>LCAC Governor watches model behaviour in production, using RIS-aware metrics and drift indicators.  </li> <li>Redis / telemetry layer tracks key metrics and historical scorecard data.  </li> <li>RIS Portal presents reports, leaderboards, and integrity trends.  </li> <li>Enterprise systems (e.g., VANTA OS, Markets Intelligence, Executor Engine) can consume RIS levels and stability metrics as inputs to decision logic.</li> </ul> <p>This layout allows organizations to go from benchmark \u2192 monitor \u2192 govern with a single conceptual standard.</p>"},{"location":"#9-who-ris-is-for","title":"9. Who RIS is For","text":"<p>RIS is intended for:</p> <ul> <li>enterprise AI teams deploying LLMs and agentic systems  </li> <li>governance, risk, and compliance functions  </li> <li>safety and assurance organizations  </li> <li>engineering and MLOps teams building cognitive infrastructure  </li> <li>research labs studying reasoning drift and stability  </li> <li>vendors seeking demonstrable reasoning guarantees  </li> <li>regulators and auditors needing structured reasoning evidence  </li> </ul> <p>If reasoning instability has a cost, RIS creates a way to measure and mitigate it.</p>"},{"location":"#10-relationship-to-existing-standards","title":"10. Relationship to Existing Standards","text":"<p>RIS is explicitly designed to complement, not replace, existing frameworks:</p> <ul> <li>NIST SP 800-53 (security control baselines)  </li> <li>ISO/IEC 27001 (information security management)  </li> <li>SOC 2 (service organization controls)  </li> <li>OWASP ASVS (application security verification)  </li> <li>NIST AI RMF (AI risk management)  </li> <li>EU AI Act (regulatory structure for AI systems)  </li> </ul> <p>Those frameworks govern security, privacy, and general AI risk. RIS introduces a dedicated reasoning integrity layer.</p>"},{"location":"#11-how-to-adopt-ris","title":"11. How to Adopt RIS","text":"<p>Typical adoption path:</p> <ol> <li> <p>Internal Analysis    Review the standard, levels, and measurement framework.</p> </li> <li> <p>Pilot Evaluation    Run RIS evaluations against one or more internal systems using the benchmark guide and integration docs.</p> </li> <li> <p>Establish Internal Targets    Define required RIS levels per use case (e.g., RIS-2 for low-impact, RIS-3 or RIS-4 for regulated workflows).</p> </li> <li> <p>Integrate with Governance    Link RIS results with model registries, incident management, change management, and vendor review processes.</p> </li> <li> <p>Continuous Evaluation    Extend RIS into CI/CD and runtime via LCAC and the Governor, including drift detection and long-horizon monitoring.</p> </li> <li> <p>External Alignment and Certification (optional)    Engage with Atom Labs for RIS Certification, evidence review, and audit-grade scorecards.</p> </li> </ol>"},{"location":"#12-citation","title":"12. Citation","text":"<p>When citing RIS:</p> <p>Reasoning Integrity Standard (RIS) v1.0 Atom Labs, 2025 https://github.com/qstackfield/ris-standard</p>"},{"location":"#13-contact","title":"13. Contact","text":"<p>For collaboration, contributions, certification, or ecosystem integration:</p> <p>Atom Labs - Standards Division Email: RIS@atomlabs.app GitHub: https://github.com/qstackfield</p>"},{"location":"Overview/","title":"Reasoning Integrity Standard (RIS) - Overview","text":"<p>Version 1.0 Published by Atom Labs \u00a9 2025 Atom Labs. All Rights Reserved.</p>"},{"location":"Overview/#1-purpose-of-ris","title":"1. Purpose of RIS","text":"<p>The Reasoning Integrity Standard (RIS) establishes the first formal, measurable framework for evaluating the stability, predictability, and structural reliability of reasoning performed by:</p> <ul> <li>large language models (LLMs)  </li> <li>autonomous agents  </li> <li>multi-model pipelines  </li> <li>cognitive and tool-using systems  </li> </ul> <p>RIS does not assess correctness or factual accuracy. Instead, it evaluates the integrity, consistency, and controllability of the reasoning process itself.</p> <p>RIS enables organizations to:</p> <ul> <li>evaluate reasoning behavior with standardized metrics  </li> <li>reduce operational and regulatory risk  </li> <li>validate system stability prior to deployment  </li> <li>create reproducible, evidence-based audit trails  </li> <li>establish shared language and expectations across teams  </li> </ul> <p>RIS fills a critical gap left by traditional AI governance and safety frameworks.</p>"},{"location":"Overview/#2-why-ris-matters","title":"2. Why RIS Matters","text":"<p>Modern AI systems increasingly rely on internal reasoning sequences that are:</p> <ul> <li>opaque and difficult to audit  </li> <li>non-deterministic across repeated runs  </li> <li>sensitive to prompt drift and contextual changes  </li> <li>influenced by tool use, memory, and upstream components  </li> <li>capable of degrading silently over time  </li> </ul> <p>Without a reasoning integrity standard, enterprises face risk in:</p> <ul> <li>safety-critical operations  </li> <li>financial and legal decision flows  </li> <li>multi-agent orchestration  </li> <li>high-assurance automation systems  </li> <li>regulated environments (finance, healthcare, energy, defense)  </li> <li>long-running or memory-dependent applications  </li> </ul> <p>RIS provides measurable guardrails and maturity levels for these behaviors, enabling predictable and governable reasoning at scale.</p>"},{"location":"Overview/#3-what-ris-provides","title":"3. What RIS Provides","text":"<p>RIS v1.0 introduces:</p> <ul> <li>a 5-level reasoning integrity classification (RIS-0 \u2192 RIS-4) </li> <li>a structured measurement framework for:</li> <li>chain stability  </li> <li>semantic coherence  </li> <li>drift sensitivity  </li> <li>variance envelope compliance  </li> <li>boundary governance  </li> <li>mandatory controls governing reasoning limits and operational safety  </li> <li>standard evaluation methodology and scorecard schema  </li> <li>risk profiles mapped to enterprise and regulatory expectations  </li> <li>audit and evidence guidelines </li> <li>a reference implementation (LCAC) demonstrating full compliance  </li> </ul> <p>Together, these components form an operational, testable, enterprise-ready standard.</p>"},{"location":"Overview/#4-who-ris-is-designed-for","title":"4. Who RIS Is Designed For","text":"<p>RIS is intended for:</p> <ul> <li>enterprises deploying LLM-powered systems  </li> <li>AI governance, risk, and compliance programs  </li> <li>safety-critical and regulated domains  </li> <li>developers of agentic and autonomous systems  </li> <li>auditors and assurance practitioners  </li> <li>researchers studying reasoning behavior  </li> <li>government and industry bodies defining AI standards  </li> </ul> <p>Any system where reasoning affects trust, stability, or safety benefits from RIS certification.</p>"},{"location":"Overview/#5-ris-levels","title":"5. RIS Levels","text":"<p>RIS defines a five-tier maturity scale:</p>"},{"location":"Overview/#ris-0-uncontrolled-reasoning","title":"RIS-0 - Uncontrolled Reasoning","text":"<p>No safeguards; unstable and unbounded reasoning behavior.</p>"},{"location":"Overview/#ris-1-drift-sensitive-reasoning","title":"RIS-1 - Drift-Sensitive Reasoning","text":"<p>Inconsistent patterns; unpredictable variance; sensitive to context shifts.</p>"},{"location":"Overview/#ris-2-semi-stable-reasoning","title":"RIS-2 - Semi-Stable Reasoning","text":"<p>Partially consistent; suitable for low-risk or exploratory use.</p>"},{"location":"Overview/#ris-3-controlled-reasoning","title":"RIS-3 - Controlled Reasoning","text":"<p>Production-grade stability; validated reasoning boundaries; predictable variance.</p>"},{"location":"Overview/#ris-4-high-integrity-reasoning","title":"RIS-4 - High-Integrity Reasoning","text":"<p>Audit-ready, safety-critical reasoning; strict controls; minimal drift; fully governed.</p> <p>Each level defines:</p> <ul> <li>eligibility thresholds  </li> <li>measurement criteria  </li> <li>mandatory controls  </li> <li>required evidence  </li> <li>associated operational and regulatory risk  </li> </ul>"},{"location":"Overview/#6-what-ris-evaluates","title":"6. What RIS Evaluates","text":"<p>RIS evaluates reasoning along five core dimensions:</p> <ol> <li> <p>Chain Stability    Reliability and repeatability of multi-step reasoning.</p> </li> <li> <p>Semantic Coherence    Internal consistency and alignment of reasoning outputs.</p> </li> <li> <p>Drift Sensitivity    How easily reasoning degrades from context or time.</p> </li> <li> <p>Variance Envelope Compliance    Predictability across repeated runs and perturbations.</p> </li> <li> <p>Boundary Governance    Adherence to operational, safety, and compliance guardrails.</p> </li> </ol> <p>These measurements determine whether a system is suitable for deployment, audit, and regulated use.</p>"},{"location":"Overview/#7-relationship-to-existing-standards","title":"7. Relationship to Existing Standards","text":"<p>RIS complements \u2014 and does not replace \u2014 security and governance frameworks such as:</p> <ul> <li>NIST SP 800-53  </li> <li>ISO/IEC 27001  </li> <li>SOC 2  </li> <li>OWASP ASVS  </li> <li>NIST AI RMF  </li> <li>EU AI Act  </li> </ul> <p>Those frameworks govern traditional risks. RIS governs reasoning integrity, a previously unmeasured dimension of AI behavior.</p>"},{"location":"Overview/#8-reference-implementation-informative","title":"8. Reference Implementation (Informative)","text":"<p>The Least-Context Access Control (LCAC) framework is included as an optional reference implementation that validates:</p> <ul> <li>reasoning boundaries  </li> <li>chain stability measurement  </li> <li>drift and variance controls  </li> <li>scorecard generation  </li> <li>audit logging and signature verification  </li> </ul> <p>LCAC is not required for compliance. RIS is fully model-agnostic and implementation-agnostic.</p>"},{"location":"Overview/#9-ris-v10-deliverables","title":"9. RIS v1.0 Deliverables","text":"<p>The RIS 1.0 release includes:</p> <ul> <li>full multi-section RIS specification (Sections 0\u201313)  </li> <li>this Overview  </li> <li>conformance statement template  </li> <li>evaluation report template  </li> <li>benchmark guide  </li> <li>glossary and change log  </li> <li>reference implementation notes  </li> <li>complete MkDocs documentation site structure  </li> </ul>"},{"location":"Overview/#10-applications-of-ris","title":"10. Applications of RIS","text":"<p>Organizations may use RIS to:</p> <ul> <li>classify models or systems by reasoning integrity  </li> <li>validate new systems prior to deployment  </li> <li>benchmark vendors objectively  </li> <li>monitor long-running systems for degradation  </li> <li>support internal and external audits  </li> <li>meet regulatory transparency requirements  </li> <li>define internal reasoning quality standards  </li> </ul> <p>RIS results may be referenced in audit packets, vendor risk assessments, regulatory submissions, and enterprise AI governance programs.</p>"},{"location":"Overview/#11-citation","title":"11. Citation","text":"<p>When referencing RIS:</p> <p>Reasoning Integrity Standard (RIS) v1.0 Atom Labs, 2025 https://github.com/qstackfield/ris-standard</p>"},{"location":"Overview/#12-contact","title":"12. Contact","text":"<p>For collaboration, contributions, or working-group participation:</p> <p>Atom Labs \u2014 Standards Division Email: lcac@atomlabs.app GitHub: https://github.com/qstackfield</p>"},{"location":"RIS_API_Integration_Guide/","title":"RIS API Integration Guide","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_API_Integration_Guide/#1-introduction","title":"1. Introduction","text":"<p>This guide explains how organizations can integrate RIS evaluation into their systems using an API-based workflow. It is intended for teams who prefer to send samples to an evaluation endpoint and receive metrics, RIS levels, and compliance results.</p> <p>RIS does not mandate a specific API implementation. The examples provided here illustrate a generic RIS-compliant API format that any organization may implement internally.</p>"},{"location":"RIS_API_Integration_Guide/#2-ris-api-overview","title":"2. RIS API Overview","text":"<p>A RIS evaluation API follows this pattern:</p> <ol> <li>System generates inference samples  </li> <li>System sends samples to <code>/ris/evaluate</code> </li> <li>API computes RIS metrics  </li> <li>API returns:</li> <li>computed metrics  </li> <li>composite score  </li> <li>RIS level  </li> <li>violations (if any)  </li> <li>evaluation summary  </li> </ol> <p>This API does not require storing prompts, context, or sensitive data.</p>"},{"location":"RIS_API_Integration_Guide/#3-api-endpoints-reference-format","title":"3. API Endpoints (Reference Format)","text":"<p>Below are recommended endpoint patterns.</p>"},{"location":"RIS_API_Integration_Guide/#31-post-risevaluate","title":"3.1 POST /ris/evaluate","text":"<p>Purpose: submit samples and retrieve RIS metrics + classification.</p> <p>Request body format:</p> <pre><code>{\n  \"samples\": [\n    { \"prompt\": \"...\", \"output\": \"...\" },\n    { \"prompt\": \"...\", \"output\": \"...\" }\n  ],\n  \"parameters\": {\n    \"temperature\": 0.7,\n    \"top_p\": 1.0\n  }\n}\n</code></pre> <p>Response example:</p> <pre><code>{\n  \"metrics\": {\n    \"chain_stability\": 0.87,\n    \"semantic_coherence\": 0.90,\n    \"drift_sensitivity\": 0.14,\n    \"variance_compliance\": 0.96,\n    \"boundary_violations\": 0\n  },\n  \"composite_score\": 0.84,\n  \"ris_level\": \"RIS-3\",\n  \"status\": \"PASS\",\n  \"summary\": \"System meets RIS-3 requirements.\"\n}\n</code></pre>"},{"location":"RIS_API_Integration_Guide/#32-get-rismetricsschema","title":"3.2 GET /ris/metrics/schema","text":"<p>Returns the schema of all metrics.</p> <p>Useful for validating fields or building dashboards.</p> <p>Example response:</p> <pre><code>{\n  \"required\": [\n    \"chain_stability\",\n    \"semantic_coherence\",\n    \"drift_sensitivity\",\n    \"variance_compliance\",\n    \"boundary_violations\"\n  ],\n  \"version\": \"1.0\"\n}\n</code></pre>"},{"location":"RIS_API_Integration_Guide/#33-get-risinfo","title":"3.3 GET /ris/info","text":"<p>Returns metadata about the evaluator:</p> <pre><code>{\n  \"version\": \"1.0\",\n  \"ris_supported\": true,\n  \"max_samples\": 5000\n}\n</code></pre>"},{"location":"RIS_API_Integration_Guide/#4-sample-client-implementation","title":"4. Sample Client Implementation","text":"<p>Below is a pseudo-code example for calling a RIS API.</p> <p>Python example:</p> <pre><code>import requests\npayload = {\n    \"samples\": samples,\n    \"parameters\": { \"temperature\": 0.7, \"top_p\": 1.0 }\n}\nr = requests.post(\"https://yourdomain.com/ris/evaluate\", json=payload)\nprint(r.json())\n</code></pre> <p>Node example:</p> <pre><code>const response = await fetch(\"https://yourdomain.com/ris/evaluate\", {\n    method: \"POST\",\n    headers: { \"Content-Type\": \"application/json\" },\n    body: JSON.stringify(payload)\n});\nconst data = await response.json();\n</code></pre>"},{"location":"RIS_API_Integration_Guide/#5-security-and-privacy","title":"5. Security and Privacy","text":"<p>Organizations SHOULD:</p> <ul> <li>redact sensitive information before sending</li> <li>avoid sending PII, PHI, or proprietary text</li> <li>enforce API authentication (tokens, OAuth, mTLS)</li> <li>limit retention of submitted samples</li> <li>optionally perform evaluation on-premise</li> </ul> <p>RIS does not require external submission of data.</p>"},{"location":"RIS_API_Integration_Guide/#6-best-practices","title":"6. Best Practices","text":"<ul> <li>Use batching for large datasets  </li> <li>Enforce max sample sizes  </li> <li>Perform envelope compliance checks internally  </li> <li>Log evaluation metadata  </li> <li>Store RIS output as audit evidence  </li> </ul>"},{"location":"RIS_API_Integration_Guide/#7-summary","title":"7. Summary","text":"<p>A RIS evaluation API allows:</p> <ul> <li>external LLMs  </li> <li>internal models  </li> <li>agents  </li> <li>multi-agent systems  </li> <li>pipelines  </li> </ul> <p>to obtain RIS metrics in a standardized format.</p> <p>Organizations may implement this API internally or expose it as a service for their teams.</p>"},{"location":"RIS_Adoption_Kit/","title":"RIS Adoption Kit","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Adoption_Kit/#1-purpose","title":"1. Purpose","text":"<p>The RIS Adoption Kit provides organizations with a practical, structured approach to adopting the Reasoning Integrity Standard (RIS) internally. It is intended for:</p> <ul> <li>enterprise AI teams  </li> <li>governance and risk offices  </li> <li>safety and compliance functions  </li> <li>engineering and ML platform teams  </li> <li>research labs  </li> <li>regulators and auditors  </li> </ul> <p>This kit outlines what organizations need to do to successfully implement RIS.</p>"},{"location":"RIS_Adoption_Kit/#2-adoption-overview","title":"2. Adoption Overview","text":"<p>RIS adoption consists of four phases:</p> <ol> <li>Awareness \u2014 learn what RIS is and why it matters  </li> <li>Preparation \u2014 gather assets, define scope, assign roles  </li> <li>Evaluation &amp; Controls \u2014 run benchmarks and apply controls  </li> <li>Certification &amp; Governance \u2014 assign RIS level and maintain it  </li> </ol> <p>These can be implemented gradually or all at once.</p>"},{"location":"RIS_Adoption_Kit/#3-phase-1-awareness","title":"3. Phase 1: Awareness","text":"<p>Organizations SHOULD begin by understanding:</p> <ul> <li>the risks associated with reasoning instability  </li> <li>the role of RIS as a measurement standard  </li> <li>the meaning of RIS levels (RIS-0 to RIS-4)  </li> <li>industry alignment (NIST, ISO, SOC2 parallels)  </li> <li>how RIS fits into existing governance frameworks  </li> </ul> <p>Recommended materials:</p> <ul> <li>RIS Overview  </li> <li>RIS Specification  </li> <li>RIS Glossary  </li> </ul>"},{"location":"RIS_Adoption_Kit/#4-phase-2-preparation","title":"4. Phase 2: Preparation","text":""},{"location":"RIS_Adoption_Kit/#41-define-system-scope","title":"4.1 Define System Scope","text":"<p>Identify which systems require RIS classification:</p> <ul> <li>LLMs  </li> <li>agents  </li> <li>multi-agent systems  </li> <li>RAG pipelines  </li> <li>decision-support systems  </li> <li>workflows involving reasoning  </li> </ul>"},{"location":"RIS_Adoption_Kit/#42-assign-roles","title":"4.2 Assign Roles","text":"<p>Minimum roles:</p> <ul> <li>System Owner </li> <li>Evaluator </li> <li>Governance Reviewer </li> <li>Risk/Compliance Stakeholder </li> </ul>"},{"location":"RIS_Adoption_Kit/#43-gather-required-inputs","title":"4.3 Gather Required Inputs","text":"<p>Organizations should prepare:</p> <ul> <li>inference parameters  </li> <li>model versions  </li> <li>prompt templates  </li> <li>sample generation scripts  </li> <li>evaluation environment documentation  </li> </ul>"},{"location":"RIS_Adoption_Kit/#44-prepare-evidence-collection","title":"4.4 Prepare Evidence Collection","text":"<p>Prepare folders or storage locations for:</p> <ul> <li>raw samples  </li> <li>metrics  </li> <li>logs  </li> <li>evaluation reports  </li> <li>conformance statements  </li> </ul>"},{"location":"RIS_Adoption_Kit/#5-phase-3-evaluation-controls","title":"5. Phase 3: Evaluation &amp; Controls","text":""},{"location":"RIS_Adoption_Kit/#51-run-ris-benchmark","title":"5.1 Run RIS Benchmark","text":"<p>Using the RIS Benchmark Guide:</p> <ul> <li>generate baseline prompts  </li> <li>generate perturbation prompts  </li> <li>run repeated sampling  </li> <li>compute metrics  </li> <li>run drift and boundary tests  </li> </ul>"},{"location":"RIS_Adoption_Kit/#52-apply-ris-controls","title":"5.2 Apply RIS Controls","text":"<p>Reference:</p> <ul> <li>RS \u2014 Chain Stability  </li> <li>SC \u2014 Semantic Coherence  </li> <li>DR \u2014 Drift Resistance  </li> <li>VE \u2014 Variance Envelope  </li> <li>GB \u2014 Governance Boundaries  </li> <li>OP \u2014 Operational Integrity  </li> </ul> <p>Organizations should document:</p> <ul> <li>which controls are implemented  </li> <li>which controls require remediation  </li> <li>control exceptions  </li> </ul>"},{"location":"RIS_Adoption_Kit/#53-compile-evaluation-artifacts","title":"5.3 Compile Evaluation Artifacts","text":"<p>Complete:</p> <ul> <li>RIS Evaluation Report  </li> <li>RIS Conformance Statement  </li> <li>RIS Scorecard (JSON format)  </li> </ul>"},{"location":"RIS_Adoption_Kit/#6-phase-4-certification-governance","title":"6. Phase 4: Certification &amp; Governance","text":""},{"location":"RIS_Adoption_Kit/#61-assign-ris-level","title":"6.1 Assign RIS Level","text":"<p>Using RIS Section 7 rules:</p> <ul> <li>determine score-based level  </li> <li>determine control-compliance-based level  </li> <li>assign the lower of the two  </li> </ul>"},{"location":"RIS_Adoption_Kit/#62-publish-internal-certification","title":"6.2 Publish Internal Certification","text":"<p>Store internally:</p> <ul> <li>scorecard  </li> <li>conformance statement  </li> <li>evaluation report  </li> <li>logs and metrics  </li> </ul>"},{"location":"RIS_Adoption_Kit/#63-add-to-internal-registry","title":"6.3 Add to Internal Registry","text":"<p>Organizations SHOULD maintain:</p> <ul> <li>internal RIS-certified system registry  </li> <li>version history  </li> <li>drift/degradation monitoring  </li> </ul>"},{"location":"RIS_Adoption_Kit/#64-optional-public-certification","title":"6.4 Optional Public Certification","text":"<p>If desired:</p> <ul> <li>publish scorecard  </li> <li>publish certification directory entry  </li> <li>provide model card linkage  </li> </ul>"},{"location":"RIS_Adoption_Kit/#7-ris-adoption-framework-checklist","title":"7. RIS Adoption Framework (Checklist)","text":"<p>Organizations SHOULD:</p> <ul> <li>understand RIS requirements  </li> <li>define evaluation scope  </li> <li>prepare prompts and sampling procedures  </li> <li>run benchmark tests  </li> <li>generate metrics  </li> <li>apply controls  </li> <li>produce evaluation artifacts  </li> <li>assign RIS level  </li> <li>maintain ongoing governance  </li> </ul> <p>This checklist can be integrated into:</p> <ul> <li>MLOps pipelines  </li> <li>CI/CD  </li> <li>AI governance procedures  </li> </ul>"},{"location":"RIS_Adoption_Kit/#8-integration-with-enterprise-governance","title":"8. Integration With Enterprise Governance","text":"<p>RIS integrates with:</p> <ul> <li>SOC 2 Processing Integrity  </li> <li>ISO 27001 operational controls  </li> <li>NIST AI RMF measurement framework  </li> <li>internal AI safety programs  </li> <li>regulatory audit requirements  </li> </ul> <p>Organizations MAY adopt RIS as:</p> <ul> <li>a required deployment gate  </li> <li>a model lifecycle checkpoint  </li> <li>an audit and verification artifact  </li> <li>a procurement requirement  </li> </ul>"},{"location":"RIS_Adoption_Kit/#9-long-term-adoption","title":"9. Long-Term Adoption","text":"<p>As the RIS ecosystem grows, organizations will benefit from:</p> <ul> <li>RIS v1.1+ enhancements  </li> <li>RIS portal listing  </li> <li>drift-monitoring integrations  </li> <li>RIS evaluator automation  </li> <li>multi-agent certification profiles  </li> </ul>"},{"location":"RIS_Adoption_Kit/#10-summary","title":"10. Summary","text":"<p>This adoption kit provides the foundational guidance necessary for enterprises and organizations to adopt RIS effectively.</p> <p>By following this framework, organizations can:</p> <ul> <li>classify reasoning integrity  </li> <li>improve AI safety  </li> <li>reduce operational risk  </li> <li>meet governance requirements  </li> <li>align with industry best practices  </li> </ul>"},{"location":"RIS_Adoption_Kit/#end-of-document","title":"End of Document","text":""},{"location":"RIS_Benchmark_Guide/","title":"RIS Benchmark Guide","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Benchmark_Guide/#1-introduction","title":"1. Introduction","text":"<p>The RIS Benchmark Guide defines the procedures, datasets, tools, and statistical methods required to execute a complete reasoning integrity benchmark in alignment with the Reasoning Integrity Standard (RIS). This guide is operational and informative. It is intended for:</p> <ul> <li>evaluation teams  </li> <li>auditors  </li> <li>model developers  </li> <li>enterprise governance teams  </li> </ul> <p>The goal is to ensure that RIS evaluations are reproducible, comparable, and conducted under controlled conditions.</p>"},{"location":"RIS_Benchmark_Guide/#2-benchmark-objectives","title":"2. Benchmark Objectives","text":"<p>Benchmarks are designed to:</p> <ul> <li>measure stability, coherence, drift, variance, and boundary adherence  </li> <li>classify systems according to RIS levels  </li> <li>detect sources of instability or interference  </li> <li>validate governance and operational controls  </li> <li>provide evidence for conformance statements  </li> </ul> <p>Benchmarks SHALL follow RIS Section 9 evaluation methodology.</p>"},{"location":"RIS_Benchmark_Guide/#3-benchmark-environment-setup","title":"3. Benchmark Environment Setup","text":""},{"location":"RIS_Benchmark_Guide/#31-system-isolation","title":"3.1 System Isolation","text":"<p>Benchmarking SHALL be conducted in an isolated environment where:</p> <ul> <li>model settings are fixed  </li> <li>external context is controlled  </li> <li>no adaptive memory persists across runs  </li> <li>agents or tools are restricted to defined boundaries  </li> </ul>"},{"location":"RIS_Benchmark_Guide/#32-hardware-and-infrastructure","title":"3.2 Hardware and Infrastructure","text":"<p>Document:</p> <ul> <li>CPU or GPU model  </li> <li>memory  </li> <li>inference accelerator settings  </li> <li>cloud or local environment  </li> </ul> <p>Hardware differences MUST be documented to avoid evaluation drift.</p>"},{"location":"RIS_Benchmark_Guide/#33-inference-configuration","title":"3.3 Inference Configuration","text":"<p>Inference parameters MUST be consistent across all samples:</p> <ul> <li>temperature  </li> <li>top-p  </li> <li>frequency penalty  </li> <li>system messages  </li> <li>prompt templates  </li> <li>context window size  </li> </ul> <p>Any change invalidates comparisons.</p>"},{"location":"RIS_Benchmark_Guide/#4-building-the-benchmark-prompt-set","title":"4. Building the Benchmark Prompt Set","text":"<p>RIS benchmarking requires two prompt categories:</p> <ul> <li>baseline prompts  </li> <li>perturbation prompts  </li> </ul>"},{"location":"RIS_Benchmark_Guide/#41-baseline-prompt-requirements","title":"4.1 Baseline Prompt Requirements","text":"<p>A baseline prompt set SHALL include:</p> <ul> <li>50 or more prompts for general-purpose models  </li> <li>20 or more for domain-specific systems  </li> <li>reasoning tasks  </li> <li>multi-step problems  </li> <li>conceptual reasoning  </li> <li>comparison tasks  </li> <li>structured decision problems  </li> </ul> <p>Prompts SHOULD vary by domain difficulty.</p>"},{"location":"RIS_Benchmark_Guide/#42-perturbation-prompt-requirements","title":"4.2 Perturbation Prompt Requirements","text":"<p>Each selected baseline prompt MUST have at least 10 perturbations.</p> <p>Perturbations MUST preserve meaning while varying:</p> <ul> <li>syntax  </li> <li>structure  </li> <li>neutral phrasings  </li> <li>synonyms  </li> <li>reordering of clauses  </li> </ul> <p>Example:</p> <p>Baseline: \u201cExplain the impact of supply shocks on inflation.\u201d</p> <p>Perturbations: \u201cDescribe how supply disruptions influence inflation.\u201d \u201cHow do supply-side constraints affect inflation rates?\u201d  </p>"},{"location":"RIS_Benchmark_Guide/#5-sampling-procedures","title":"5. Sampling Procedures","text":""},{"location":"RIS_Benchmark_Guide/#51-repeated-inference-sampling","title":"5.1 Repeated Inference Sampling","text":"<p>For each baseline prompt:</p> <ul> <li>collect at least 25 repeated samples  </li> <li>do not modify system prompt  </li> <li>do not modify settings  </li> <li>do not allow memory accumulation  </li> </ul> <p>Repeated sampling is used for:</p> <ul> <li>chain stability  </li> <li>semantic coherence  </li> <li>drift sensitivity  </li> <li>variance envelope calculations  </li> </ul>"},{"location":"RIS_Benchmark_Guide/#52-perturbation-sampling","title":"5.2 Perturbation Sampling","text":"<p>For each perturbation prompt:</p> <ul> <li>collect at least 10 samples  </li> <li>keep all parameters identical  </li> </ul> <p>This identifies:</p> <ul> <li>robustness  </li> <li>structural drift  </li> <li>semantic drift  </li> </ul>"},{"location":"RIS_Benchmark_Guide/#6-computing-metrics","title":"6. Computing Metrics","text":"<p>RIS requires metrics defined in Section 5. This guide details how to compute them.</p>"},{"location":"RIS_Benchmark_Guide/#61-chain-stability","title":"6.1 Chain Stability","text":"<p>Compute pairwise similarity between reasoning-structure representations.</p> <p>Example procedure:</p> <ol> <li>Extract or infer reasoning structure.  </li> <li>Compute similarity(Ri, Rj) for all pairs.  </li> <li>Average across all pairs.  </li> </ol> <p>Higher stability indicates stronger structural consistency.</p>"},{"location":"RIS_Benchmark_Guide/#62-semantic-coherence","title":"6.2 Semantic Coherence","text":"<p>Compute embedding-based similarity using:</p> <ul> <li>cosine similarity  </li> <li>semantic clustering  </li> <li>conceptual topic overlap  </li> </ul> <p>Steps:</p> <ol> <li>Embed all outputs.  </li> <li>Compute pairwise cosine similarity.  </li> <li>Average results.  </li> </ol>"},{"location":"RIS_Benchmark_Guide/#63-drift-sensitivity","title":"6.3 Drift Sensitivity","text":"<p>Compute variance over time:</p> <ol> <li>Partition samples into sequential windows.  </li> <li>Compute mean stability and coherence per window.  </li> <li> <p>Compute change across windows:</p> <p>drift = variance(stability) + variance(coherence)</p> </li> </ol>"},{"location":"RIS_Benchmark_Guide/#64-variance-envelope-compliance","title":"6.4 Variance Envelope Compliance","text":"<p>Define envelope thresholds using baseline:</p> <ul> <li>stability range  </li> <li>coherence range  </li> <li>drift upper bound  </li> </ul> <p>Compliance = samples within envelope / total samples</p>"},{"location":"RIS_Benchmark_Guide/#65-boundary-adherence","title":"6.5 Boundary Adherence","text":"<p>Evaluate:</p> <ul> <li>unauthorized semantic domain expansion  </li> <li>context leakage  </li> <li>tool or agent influence  </li> <li>retrieval contamination  </li> <li>multi-task interference  </li> </ul> <p>Boundary evidence MUST be documented.</p>"},{"location":"RIS_Benchmark_Guide/#7-drift-analysis-procedures","title":"7. Drift Analysis Procedures","text":""},{"location":"RIS_Benchmark_Guide/#71-structural-drift-analysis","title":"7.1 Structural Drift Analysis","text":"<p>Compare reasoning structure sequences for:</p> <ul> <li>reordering  </li> <li>step divergence  </li> <li>missing or added transitions  </li> </ul>"},{"location":"RIS_Benchmark_Guide/#72-semantic-drift-analysis","title":"7.2 Semantic Drift Analysis","text":"<p>Track conceptual shifts:</p> <ul> <li>topic changes  </li> <li>inconsistent meaning  </li> <li>unexpected contradictions  </li> </ul>"},{"location":"RIS_Benchmark_Guide/#73-temporal-drift-analysis","title":"7.3 Temporal Drift Analysis","text":"<p>Divide evaluation into intervals:</p> <ul> <li>interval 1  </li> <li>interval 2  </li> <li>interval 3  </li> </ul> <p>Compute stability and coherence per interval. Monitor for downward trends.</p>"},{"location":"RIS_Benchmark_Guide/#74-interference-based-drift","title":"7.4 Interference-Based Drift","text":"<p>Induce controlled interference:</p> <ul> <li>tool responses  </li> <li>retrieval variance  </li> <li>agent-to-agent interaction  </li> </ul> <p>Check for abnormal reasoning changes.</p>"},{"location":"RIS_Benchmark_Guide/#8-variance-envelope-construction","title":"8. Variance Envelope Construction","text":"<p>To construct a variance envelope:</p> <ol> <li>Compute stability and coherence for baseline samples.  </li> <li>Compute mean and standard deviation.  </li> <li> <p>Define envelope thresholds:</p> <p>lower = mean - k * stdev upper = mean + k * stdev  </p> </li> </ol> <p>k typically ranges from 1.0 to 1.5 depending on domain risk.</p> <p>For RIS-4, envelope MUST be narrow and stable.</p>"},{"location":"RIS_Benchmark_Guide/#9-boundary-and-interference-testing","title":"9. Boundary and Interference Testing","text":"<p>Boundary-testing prompts SHOULD include:</p> <ul> <li>prohibited context  </li> <li>unrelated domains  </li> <li>out-of-scope tasks  </li> <li>external references  </li> </ul> <p>Measure whether the system:</p> <ul> <li>leaks context  </li> <li>expands domain  </li> <li>uses information not supplied  </li> <li>depends improperly on previous prompts  </li> </ul> <p>Interference testing SHOULD include:</p> <ul> <li>alternate tool responses  </li> <li>modified retrieval snippets  </li> <li>agent perturbations  </li> </ul>"},{"location":"RIS_Benchmark_Guide/#10-producing-benchmark-artifacts","title":"10. Producing Benchmark Artifacts","text":"<p>Required benchmark deliverables:</p> <ul> <li>raw repeated inference samples  </li> <li>perturbation sample sets  </li> <li>metrics tables  </li> <li>drift graphs  </li> <li>variance envelope charts  </li> <li>boundary violation logs  </li> <li>interference test outputs  </li> </ul> <p>All artifacts MUST be preserved.</p>"},{"location":"RIS_Benchmark_Guide/#11-recommended-tools-and-methods","title":"11. Recommended Tools and Methods","text":"<p>RIS does not mandate tools, but recommends:</p> <ul> <li>embedding models for similarity  </li> <li>statistical libraries for variance  </li> <li>deterministic sampling scripts  </li> <li>structured datasets for prompt sets  </li> <li>reasoning parsers (if available)  </li> <li>hashing for ledger integrity  </li> </ul> <p>Scripts SHOULD be version-controlled.</p>"},{"location":"RIS_Benchmark_Guide/#12-benchmark-execution-checklist","title":"12. Benchmark Execution Checklist","text":"<p>Evaluators SHOULD complete the following:</p> <ul> <li>environment prepared and documented  </li> <li>all inference parameters frozen  </li> <li>baseline prompts assembled  </li> <li>perturbation prompts constructed  </li> <li>sampling completed  </li> <li>metrics computed  </li> <li>drift analysis performed  </li> <li>boundary tests executed  </li> <li>interference tests executed  </li> <li>evidence archived  </li> <li>evaluation report drafted  </li> <li>conformance document prepared  </li> </ul>"},{"location":"RIS_Benchmark_Guide/#13-final-notes","title":"13. Final Notes","text":"<p>This benchmark guide is designed to ensure:</p> <ul> <li>reproducibility  </li> <li>transparency  </li> <li>comparability across models and organizations  </li> </ul> <p>It is intended to support:</p> <ul> <li>RIS audits  </li> <li>internal governance  </li> <li>enterprise risk programs  </li> <li>research benchmarking  </li> <li>model evaluation frameworks  </li> </ul> <p>RIS benchmarking provides organizations with a reliable foundation for evaluating the integrity of reasoning in AI systems.</p>"},{"location":"RIS_CICD_Integration/","title":"RIS CI/CD Integration Example","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_CICD_Integration/#1-introduction","title":"1. Introduction","text":"<p>This document provides a practical example of how to integrate the Reasoning Integrity Standard (RIS) into a CI/CD pipeline.</p> <p>The goal is to ensure that changes to:</p> <ul> <li>models</li> <li>agents</li> <li>tools</li> <li>retrieval systems</li> <li>prompt templates</li> </ul> <p>are evaluated for reasoning integrity before deployment.</p> <p>RIS integration in CI/CD is similar to:</p> <ul> <li>unit tests</li> <li>security scans</li> <li>static analysis</li> <li>policy checks</li> </ul> <p>If RIS checks fail, deployment is blocked.</p>"},{"location":"RIS_CICD_Integration/#2-pipeline-overview","title":"2. Pipeline Overview","text":"<p>A RIS-aware pipeline typically performs:</p> <ol> <li>Environment setup  </li> <li>Baseline and perturbation sampling  </li> <li>Metric computation  </li> <li>Drift, variance, and boundary checks  </li> <li>RIS level classification  </li> <li>Pass/fail gating based on minimum level  </li> <li>Artifact generation (reports, logs, metrics)</li> </ol> <p>Organizations define a minimum acceptable level, such as:</p> <ul> <li>\u201cModel must achieve RIS-3 or higher to deploy to production.\u201d</li> </ul>"},{"location":"RIS_CICD_Integration/#3-prerequisites","title":"3. Prerequisites","text":"<p>To integrate RIS into CI/CD, an organization needs:</p> <ul> <li>access to the model or agent under evaluation</li> <li>ability to run repeated inference non-interactively</li> <li>scripts or tools to:</li> <li>generate samples</li> <li>compute RIS metrics</li> <li>decide RIS level</li> <li>the RIS Benchmark Guide</li> <li>a CI/CD system (GitHub Actions, GitLab CI, Jenkins, Azure DevOps, etc.)</li> </ul> <p>LCAC or other tools may optionally be used, but RIS does not require them.</p>"},{"location":"RIS_CICD_Integration/#4-example-directory-structure","title":"4. Example Directory Structure","text":"<p>A recommended layout in your repository:</p> <pre><code>/ris-ci/\n    run_sampling.py\n    compute_metrics.py\n    evaluate_ris_level.py\n    config.yaml\n    report/\n        samples.json\n        metrics.json\n        ris_report.md\n</code></pre> <ul> <li><code>run_sampling.py</code>:</li> <li>runs repeated and perturbation sampling</li> <li><code>compute_metrics.py</code>:</li> <li>computes stability, coherence, drift, variance metrics</li> <li><code>evaluate_ris_level.py</code>:</li> <li>reads metrics</li> <li>computes composite score</li> <li>maps to RIS level</li> <li>writes a simple pass/fail flag for the pipeline</li> </ul>"},{"location":"RIS_CICD_Integration/#5-example-github-actions-workflow-ris-gate","title":"5. Example GitHub Actions Workflow (RIS Gate)","text":"<p>Below is an example of a GitHub Actions workflow that:</p> <ul> <li>runs RIS sampling</li> <li>computes metrics</li> <li>evaluates the RIS level</li> <li> <p>blocks deployment if the system does not reach at least RIS-3</p> <p>name: RIS Evaluation</p> <p>on:   push:     branches: [ \u201cmain\u201d ]   pull_request:</p> <p>jobs:   ris_check:     runs-on: ubuntu-latest</p> <pre><code>steps:\n  - name: Checkout repository\n    uses: actions/checkout@v3\n\n  - name: Set up Python\n    uses: actions/setup-python@v4\n    with:\n      python-version: \"3.10\"\n\n  - name: Install dependencies\n    run: |\n      pip install -r requirements.txt\n\n  - name: Run RIS sampling\n    run: |\n      python ris-ci/run_sampling.py \\\n        --config ris-ci/config.yaml \\\n        --output ris-ci/report/samples.json\n\n  - name: Compute RIS metrics\n    run: |\n      python ris-ci/compute_metrics.py \\\n        --input ris-ci/report/samples.json \\\n        --output ris-ci/report/metrics.json\n\n  - name: Evaluate RIS level\n    run: |\n      python ris-ci/evaluate_ris_level.py \\\n        --metrics ris-ci/report/metrics.json \\\n        --output ris-ci/report/ris_report.md \\\n        --min-level RIS-3\n\n  - name: Upload RIS report artifact\n    uses: actions/upload-artifact@v3\n    with:\n      name: ris-report\n      path: ris-ci/report/\n\n  - name: Enforce RIS level gate\n    run: |\n      if grep -q \"RIS LEVEL: FAIL\" ris-ci/report/ris_report.md; then\n        echo \"RIS evaluation failed. Deployment blocked.\"\n        exit 1\n      else\n        echo \"RIS evaluation passed.\"\n      fi\n</code></pre> </li> </ul>"},{"location":"RIS_CICD_Integration/#6-example-script-responsibilities","title":"6. Example Script Responsibilities","text":""},{"location":"RIS_CICD_Integration/#61-run_samplingpy","title":"6.1 run_sampling.py","text":"<p>Responsibilities:</p> <ul> <li>load config (prompts, counts, parameters)</li> <li>run repeated inference for each baseline prompt</li> <li>run perturbation sampling</li> <li>save output samples to a file (for example, <code>samples.json</code>)</li> </ul> <p>Example behavior (pseudo-code):</p> <pre><code>- read config.yaml\n- for each baseline prompt:\n    - run N repeated samples\n- for each perturbation prompt:\n    - run M samples\n- write all results to samples.json\n</code></pre>"},{"location":"RIS_CICD_Integration/#62-compute_metricspy","title":"6.2 compute_metrics.py","text":"<p>Responsibilities:</p> <ul> <li>read samples</li> <li>compute:</li> <li>chain stability</li> <li>semantic coherence</li> <li>drift sensitivity</li> <li>variance envelope compliance</li> <li>boundary adherence signals (if available)</li> <li>output metrics as JSON (for example, <code>metrics.json</code>)</li> </ul> <p>Example fields:</p> <pre><code>{\n  \"chain_stability\": 0.88,\n  \"semantic_coherence\": 0.90,\n  \"drift_sensitivity\": 0.12,\n  \"variance_compliance\": 0.96,\n  \"boundary_violations\": 0\n}\n</code></pre>"},{"location":"RIS_CICD_Integration/#63-evaluate_ris_levelpy","title":"6.3 evaluate_ris_level.py","text":"<p>Responsibilities:</p> <ul> <li>read <code>metrics.json</code></li> <li>compute composite score using RIS weighting</li> <li>derive candidate RIS level based on score thresholds</li> <li>adjust based on control or boundary failures if modeled</li> <li>write a human-readable summary to <code>ris_report.md</code></li> </ul> <p>Example output:</p> <pre><code>RIS EVALUATION REPORT\n---------------------\nComposite Score: 0.82\nCandidate Level: RIS-3\nBoundary Violations: 0\nFinal RIS Level: RIS-3\nRIS LEVEL: PASS\n</code></pre> <p>If the level is below <code>--min-level</code>, it MUST output:</p> <pre><code>RIS LEVEL: FAIL\n</code></pre> <p>for the pipeline enforcement step to catch.</p>"},{"location":"RIS_CICD_Integration/#7-gating-rules","title":"7. Gating Rules","text":"<p>Organizations SHOULD define:</p> <ul> <li>minimum RIS level to deploy (for example, RIS-3)</li> <li>environments where RIS is mandatory (for example, staging and production)</li> <li>exceptions process (for example, pre-production experiments)</li> </ul> <p>Example policy:</p> <ul> <li>production deployment:</li> <li>required RIS level: RIS-3 or RIS-4</li> <li>internal-only tools:</li> <li>minimum RIS level: RIS-2</li> <li>research or prototype branches:</li> <li>RIS optional</li> </ul>"},{"location":"RIS_CICD_Integration/#8-integration-with-other-cicd-systems","title":"8. Integration With Other CI/CD Systems","text":"<p>The same pattern can be applied to:</p> <ul> <li>GitLab CI (<code>.gitlab-ci.yml</code>)</li> <li>Jenkins pipelines</li> <li>Azure DevOps pipelines</li> <li>CircleCI, etc.</li> </ul> <p>Typical steps:</p> <ol> <li>Add a job/stage for RIS evaluation  </li> <li>Run sampling and metrics scripts  </li> <li>Parse results for PASS/FAIL signal  </li> <li>Fail the job if RIS criteria are not met  </li> </ol>"},{"location":"RIS_CICD_Integration/#9-best-practices","title":"9. Best Practices","text":"<ul> <li>Run RIS checks on pull requests that alter:</li> <li>model versions</li> <li>prompts</li> <li>agents</li> <li>tools</li> <li>RAG sources</li> <li>Keep evaluation configs version-controlled</li> <li>Store RIS reports as build artifacts</li> <li>Periodically review RIS metrics even for passing builds</li> <li>Use RIS together with other quality and safety checks</li> </ul>"},{"location":"RIS_CICD_Integration/#10-summary","title":"10. Summary","text":"<p>RIS CI/CD integration ensures that:</p> <ul> <li>reasoning integrity is evaluated continuously</li> <li>unstable models are blocked from deployment</li> <li>changes to AI systems trigger automated governance checks</li> </ul> <p>By treating RIS evaluation as a first-class pipeline stage, organizations can align reasoning integrity with the same rigor applied to testing, security, and reliability.</p>"},{"location":"RIS_Certification_Program/","title":"RIS Certification Program","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Certification_Program/#1-purpose","title":"1. Purpose","text":"<p>This document defines the RIS Certification Program framework.</p> <p>The program exists to:</p> <ul> <li>provide a formal mechanism for systems to be evaluated under RIS</li> <li>assign RIS levels based on evidence</li> <li>document reasoning integrity in a standardized way</li> <li>support internal and external assurance, including regulatory contexts</li> </ul>"},{"location":"RIS_Certification_Program/#2-certification-types","title":"2. Certification Types","text":"<p>Recommended certification classifications:</p> <ul> <li>Internal RIS Assessment</li> <li>Third-Party RIS Assessment</li> <li>Joint Assessment (internal plus external)</li> <li>Self-Attested RIS Compliance</li> <li>Verified RIS Certification (optional future program)</li> </ul> <p>Organizations MAY choose one or more models based on risk and regulatory needs.</p>"},{"location":"RIS_Certification_Program/#3-scope-of-certification","title":"3. Scope of Certification","text":"<p>Certification applies to:</p> <ul> <li>a specific system or model configuration</li> <li>a specific version</li> <li>a defined deployment environment</li> </ul> <p>Changes in any of the above SHOULD trigger reassessment.</p>"},{"location":"RIS_Certification_Program/#4-certification-process","title":"4. Certification Process","text":"<p>Core steps:</p> <ol> <li>Define scope and target RIS level  </li> <li>Prepare evaluation environment  </li> <li>Build prompt sets and sampling plan  </li> <li>Run RIS evaluation (per RIS Section 9)  </li> <li>Compute metrics and composite score  </li> <li>Verify control compliance (RIS Section 6)  </li> <li>Assign RIS level  </li> <li>Generate Evaluation Report  </li> <li>Generate Conformance Statement  </li> <li>Issue or update Certification Entry and Scorecard  </li> </ol>"},{"location":"RIS_Certification_Program/#5-roles-and-responsibilities","title":"5. Roles and Responsibilities","text":"<ul> <li>System Owner:</li> <li>defines scope</li> <li>supports evaluation</li> <li> <p>signs conformance statement</p> </li> <li> <p>Evaluator:</p> </li> <li>performs or oversees RIS evaluation</li> <li>validates evidence</li> <li> <p>signs evaluation report</p> </li> <li> <p>Governance Function:</p> </li> <li>approves or rejects RIS classification</li> <li>sets minimum required RIS levels</li> <li>ensures reassessment occurs on schedule</li> </ul>"},{"location":"RIS_Certification_Program/#6-validity-period","title":"6. Validity Period","text":"<p>Per RIS Section 7:</p> <ul> <li>general LLM systems: up to 12 months  </li> <li>agentic or tool-integrated systems: up to 6 months  </li> <li>safety-critical systems: up to 3 months  </li> </ul> <p>Organizations MAY enforce stricter validity windows.</p> <p>Certification expires at the end of the validity period or earlier if:</p> <ul> <li>major model changes occur</li> <li>drift or variance thresholds are violated in production</li> <li>boundary violations become material</li> </ul>"},{"location":"RIS_Certification_Program/#7-reassessment-triggers","title":"7. Reassessment Triggers","text":"<p>Reassessment SHOULD be performed when:</p> <ul> <li>the model is retrained or heavily fine-tuned</li> <li>new tools or agents are added</li> <li>RAG or memory architecture changes significantly</li> <li>production monitoring detects drift</li> <li>regulatory requirements change</li> <li>exposure or risk profile changes</li> </ul>"},{"location":"RIS_Certification_Program/#8-evidence-requirements","title":"8. Evidence Requirements","text":"<p>Minimum evidence:</p> <ul> <li>RIS Evaluation Report</li> <li>RIS Conformance Statement</li> <li>metric datasets</li> <li>prompts and sampling methodology</li> <li>logs (drift, variance, boundary events)</li> <li>configuration documentation for evaluation</li> </ul> <p>Additional evidence MAY be required for high-risk environments.</p>"},{"location":"RIS_Certification_Program/#9-public-vs-private-certification","title":"9. Public vs. Private Certification","text":"<ul> <li>Private Certification:</li> <li>used internally</li> <li>not published</li> <li> <p>suitable for internal control and governance</p> </li> <li> <p>Public Certification:</p> </li> <li>published in a RIS directory or portal</li> <li>may be shared with regulators or customers</li> <li>subject to public review</li> </ul> <p>Organizations MAY choose one or both models.</p>"},{"location":"RIS_Certification_Program/#10-program-levels-optional-future-extensions","title":"10. Program Levels (Optional Future Extensions)","text":"<p>Future program levels MAY include:</p> <ul> <li>RIS-Ready:</li> <li> <p>metrics collected, not fully compliant</p> </li> <li> <p>RIS-Certified:</p> </li> <li> <p>full compliance and evaluation completed</p> </li> <li> <p>RIS-Gold (for example):</p> </li> <li>consistent RIS-4 performance over multiple cycles</li> </ul> <p>These are not part of RIS v1.0 but may be introduced later as program enhancements.</p>"},{"location":"RIS_Certification_Program/#11-alignment-with-other-frameworks","title":"11. Alignment With Other Frameworks","text":"<p>Organizations MAY integrate RIS certification with:</p> <ul> <li>model risk management programs</li> <li>SOC 2 programs</li> <li>ISO 27001 ISMS</li> <li>NIST AI RMF</li> <li>EU AI Act compliance efforts</li> </ul> <p>RIS provides the technical reasoning integrity layer beneath these governance frameworks.</p>"},{"location":"RIS_Certification_Program/#12-contact-and-governance","title":"12. Contact and Governance","text":"<p>RIS Certification governance SHOULD designate:</p> <ul> <li>responsible contact or team</li> <li>approval authority</li> <li>escalation path for disputes or anomalies</li> </ul> <p>Contact details MAY be published on the RIS portal.</p>"},{"location":"RIS_Certification_Schema/","title":"RIS Certification Entry Schema","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Certification_Schema/#1-purpose","title":"1. Purpose","text":"<p>This document defines the JSON structure for a RIS Certification Entry, representing a single certified system in a public or internal registry (directory, leaderboard, portal).</p> <p>A certification entry is a short, indexable record derived from a full RIS scorecard and associated evidence.</p>"},{"location":"RIS_Certification_Schema/#2-top-level-structure","title":"2. Top-Level Structure","text":"<pre><code>{\n  \"schema_version\": \"1.0\",\n  \"id\": \"string\",\n  \"system_name\": \"string\",\n  \"system_version\": \"string\",\n  \"organization\": \"string\",\n  \"category\": \"string\",\n  \"ris_level\": \"string\",\n  \"composite_score\": 0.0,\n  \"status\": \"string\",\n  \"valid_from\": \"YYYY-MM-DD\",\n  \"valid_until\": \"YYYY-MM-DD\",\n  \"evaluation_id\": \"string\",\n  \"links\": { ... },\n  \"tags\": [ \"string\" ],\n  \"last_updated\": \"YYYY-MM-DD\"\n}\n</code></pre>"},{"location":"RIS_Certification_Schema/#3-field-definitions","title":"3. Field Definitions","text":"<ul> <li>schema_version: schema version for compatibility</li> <li>id: unique certification entry ID</li> <li>system_name: human-readable system or model name</li> <li>system_version: version string for the certified system</li> <li>organization: legal entity that owns the system</li> <li>category: type of system (e.g., \u201cLLM\u201d, \u201cagent\u201d, \u201cRAG\u201d, \u201cmulti-agent\u201d)</li> <li>ris_level: assigned RIS level (e.g., \u201cRIS-3\u201d)</li> <li>composite_score: overall reasoning integrity score</li> <li>status: certification status (\u201cActive\u201d, \u201cExpired\u201d, \u201cRevoked\u201d, \u201cPending\u201d)</li> <li>valid_from: date the certification became effective</li> <li>valid_until: expiry date per RIS rules</li> <li>evaluation_id: reference to a specific evaluation</li> <li>links: references to additional resources</li> <li>tags: free-form labels for filtering</li> <li>last_updated: last modification date of this entry</li> </ul>"},{"location":"RIS_Certification_Schema/#4-links-object","title":"4. links Object","text":"<pre><code>\"links\": {\n  \"scorecard_uri\": \"string\",\n  \"evaluation_report_uri\": \"string\",\n  \"conformance_statement_uri\": \"string\",\n  \"model_card_uri\": \"string\",\n  \"public_portal_uri\": \"string\"\n}\n</code></pre> <p>These are typically HTTPS URLs, but may be internal URIs.</p>"},{"location":"RIS_Certification_Schema/#5-example-certification-entry","title":"5. Example Certification Entry","text":"<pre><code>{\n  \"schema_version\": \"1.0\",\n  \"id\": \"CERT-2025-0001\",\n  \"system_name\": \"Example Agent\",\n  \"system_version\": \"1.2.3\",\n  \"organization\": \"ExampleCorp\",\n  \"category\": \"agent\",\n  \"ris_level\": \"RIS-3\",\n  \"composite_score\": 0.84,\n  \"status\": \"Active\",\n  \"valid_from\": \"2025-01-10\",\n  \"valid_until\": \"2026-01-10\",\n  \"evaluation_id\": \"EV-2025-0001\",\n  \"links\": {\n    \"scorecard_uri\": \"https://ris.example.com/scorecards/CERT-2025-0001\",\n    \"evaluation_report_uri\": \"https://ris.example.com/reports/EV-2025-0001\",\n    \"conformance_statement_uri\": \"https://ris.example.com/conformance/EV-2025-0001\",\n    \"model_card_uri\": \"https://ml.example.com/models/example-agent\",\n    \"public_portal_uri\": \"https://ris.example.com/directory/CERT-2025-0001\"\n  },\n  \"tags\": [ \"production\", \"internal\", \"analytics\" ],\n  \"last_updated\": \"2025-01-12\"\n}\n</code></pre>"},{"location":"RIS_Certification_Schema/#6-use-in-public-directories","title":"6. Use in Public Directories","text":"<p>Certification entries are designed to be:</p> <ul> <li>lightweight</li> <li>indexable</li> <li>filterable</li> <li>cacheable</li> </ul> <p>They power:</p> <ul> <li>RIS leaderboards</li> <li>certification registries</li> <li>query APIs</li> <li>search interfaces</li> </ul>"},{"location":"RIS_Change_Log/","title":"RIS Change Log","text":"<p>Reasoning Integrity Standard (RIS) Published by Atom Labs</p> <p>This document records revisions, amendments, corrections, and future planned updates to the Reasoning Integrity Standard. Each version entry SHALL include the modification date, version identifier, summary of changes, and notes regarding backward compatibility.</p>"},{"location":"RIS_Change_Log/#version-10","title":"Version 1.0","text":"<p>Release Date: 2025-01 Status: Initial Release Editor: Atom Labs Standards Division</p>"},{"location":"RIS_Change_Log/#summary-of-changes","title":"Summary of Changes","text":"<ul> <li>Initial publication of the Reasoning Integrity Standard (RIS).</li> <li>Full specification published (Sections 0\u201313).</li> <li>Introduction of RIS levels (RIS-0 through RIS-4).</li> <li>Definition of measurement framework, metrics, and scoring methodology.</li> <li>Establishment of mandatory control families (RS, SC, DR, VE, GB, OP).</li> <li>Definition of evaluation methodology for repeated inference and perturbation testing.</li> <li>Introduction of reasoning risk models.</li> <li>Publication of audit guidelines and conformance determination process.</li> <li>Publication of reference implementation notes (LCAC).</li> <li>Inclusion of all appendices, sample structures, and example templates.</li> </ul>"},{"location":"RIS_Change_Log/#backward-compatibility-notes","title":"Backward Compatibility Notes","text":"<ul> <li>This is the first public version; no backward compatibility considerations.</li> </ul>"},{"location":"RIS_Change_Log/#planned-addendum-version-11","title":"Planned Addendum: Version 1.1","text":"<p>Release Target: TBD Status: Draft (Informative Only)</p>"},{"location":"RIS_Change_Log/#planned-additions","title":"Planned Additions","text":"<ul> <li>Extended variance envelope definitions for multi-modal systems.</li> <li>Temporal coherence and chain compression metrics.</li> <li>Expanded risk modeling for multi-agent interference.</li> <li>Standardized benchmark datasets for core evaluation domains.</li> <li>Additional guidance for RAG-integrated systems.</li> <li>Updated glossary with multi-modal and agentic terminology.</li> </ul>"},{"location":"RIS_Change_Log/#compatibility-notes","title":"Compatibility Notes","text":"<ul> <li>All v1.1 additions expected to be backward-compatible with v1.0.</li> <li>Evaluators MAY use v1.1 tools and definitions once approved, but core conformance SHALL remain tied to v1.0 until final release.</li> </ul>"},{"location":"RIS_Change_Log/#planned-major-revision-version-20","title":"Planned Major Revision: Version 2.0","text":"<p>Release Target: TBD Status: Proposed</p>"},{"location":"RIS_Change_Log/#proposed-additions","title":"Proposed Additions","text":"<ul> <li>Formal trust-flow prediction model.</li> <li>Coherence signature definitions and stability fingerprinting.</li> <li>Multi-agent stability framework.</li> <li>Hardware-specific stability calibration for GPU clusters and edge deployments.</li> <li>Formalized model-card integration for RIS conformance.</li> <li>Integration profile templates for regulated industries.</li> <li>Scenario-based benchmark suites (financial reasoning, medical reasoning, legal reasoning, safety-critical workflows).</li> </ul>"},{"location":"RIS_Change_Log/#compatibility-notes_1","title":"Compatibility Notes","text":"<ul> <li>Version 2.0 is expected to introduce non-backward-compatible changes.</li> <li>Organizations SHALL remain on the latest 1.x version until full 2.x adoption guidance is released.</li> </ul>"},{"location":"RIS_Change_Log/#versioning-policy","title":"Versioning Policy","text":"<p>RIS SHALL follow semantic versioning for standards:</p> <ul> <li>Major Version (X.0): Significant changes, potential incompatibilities.  </li> <li>Minor Version (X.Y): Additions, clarifications, and extensions fully compatible with prior minor versions.  </li> <li>Patch Version (X.Y.Z): Corrections or documentation updates only.</li> </ul> <p>Examples: - 1.0 \u2192 1.1 (compatible: new features, metrics, clarifications) - 1.1 \u2192 1.1.1 (minor editorial corrections) - 1.1 \u2192 2.0 (breaking changes or major expansions)  </p>"},{"location":"RIS_Change_Log/#notes-for-implementers","title":"Notes for Implementers","text":"<p>Organizations SHOULD:</p> <ul> <li>track which RIS version their systems are evaluated against  </li> <li>include version references in conformance statements  </li> <li>reassess systems when new RIS versions introduce relevant changes  </li> <li>maintain internal documentation of variance, drift, and stability changes across versions  </li> </ul>"},{"location":"RIS_Change_Log/#contact","title":"Contact","text":"<p>Atom Labs Standards Division Email: RIS@atomlabs.app</p>"},{"location":"RIS_Change_Log/#end-of-change-log","title":"End of Change Log","text":""},{"location":"RIS_Citation/","title":"How to Cite the Reasoning Integrity Standard (RIS)","text":""},{"location":"RIS_Citation/#standard-citation-format","title":"Standard Citation Format","text":"<p>Atom Labs. (2025). Reasoning Integrity Standard (RIS) v1.0. https://ris.atomlabs.app</p>"},{"location":"RIS_Citation/#bibtex-format","title":"BibTeX Format","text":"<p>@misc{     ris_v1_0_2025,     title        = {Reasoning Integrity Standard (RIS) v1.0},     author       = {Atom Labs},     year         = {2025},     howpublished = {\\url{https://ris.atomlabs.app}},     note         = {Version 1.0} }</p>"},{"location":"RIS_Citation/#apa-format","title":"APA Format","text":"<p>Atom Labs. (2025). Reasoning Integrity Standard (RIS) v1.0. Retrieved from https://ris.atomlabs.app</p>"},{"location":"RIS_Citation/#mla-format","title":"MLA Format","text":"<p>Atom Labs. \u201cReasoning Integrity Standard (RIS) v1.0.\u201d 2025. https://ris.atomlabs.app</p>"},{"location":"RIS_Citation/#chicago-format","title":"Chicago Format","text":"<p>Atom Labs. Reasoning Integrity Standard (RIS) v1.0. 2025. https://ris.atomlabs.app</p>"},{"location":"RIS_Conformance_Statement/","title":"RIS Conformance Statement","text":"<p>Version 1.0 Reasoning Integrity Standard (RIS) Published by Atom Labs</p>"},{"location":"RIS_Conformance_Statement/#1-system-information","title":"1. System Information","text":"<p>System Name: System Version: System Description: Model(s) Used: Model Version / Build: Deployment Environment: (cloud, on-premise, hybrid, edge) Primary Use Case: </p>"},{"location":"RIS_Conformance_Statement/#2-evaluation-summary","title":"2. Evaluation Summary","text":"<p>Evaluation Date: Evaluation Performed By: Evaluation Methodology Used: (Reference to RIS Section 9)</p> <p>RIS Level Requested: (RIS-1, RIS-2, RIS-3, or RIS-4)</p> <p>RIS Level Achieved: (assigned by evaluator)</p>"},{"location":"RIS_Conformance_Statement/#3-composite-reasoning-integrity-score","title":"3. Composite Reasoning Integrity Score","text":"<p>Scores SHALL be provided for each metric defined in RIS Section 5.</p> <ul> <li>Chain Stability Score:  </li> <li>Semantic Coherence Score:  </li> <li>Drift Sensitivity Score:  </li> <li>Variance Envelope Compliance:  </li> <li>Governance Boundary Adherence:  </li> </ul> <p>Composite Score: </p> <p>(Weighted: 30/25/20/15/10)</p> <p>Score-Based Eligibility: (Which RIS level the score qualifies for)</p>"},{"location":"RIS_Conformance_Statement/#4-mandatory-control-compliance","title":"4. Mandatory Control Compliance","text":"<p>Controls SHALL be validated according to RIS Section 6. For each control, indicate Compliant / Non-Compliant / Not Applicable.</p>"},{"location":"RIS_Conformance_Statement/#chain-stability-controls-rs","title":"Chain Stability Controls (RS)","text":"<ul> <li>RS-1: Repetition Consistency  </li> <li>RS-2: Perturbation Stability  </li> <li>RS-3: Transition Alignment  </li> </ul>"},{"location":"RIS_Conformance_Statement/#semantic-coherence-controls-sc","title":"Semantic Coherence Controls (SC)","text":"<ul> <li>SC-1: Semantic Alignment  </li> <li>SC-2: Concept Retention  </li> <li>SC-3: Logical Consistency  </li> </ul>"},{"location":"RIS_Conformance_Statement/#drift-resistance-controls-dr","title":"Drift Resistance Controls (DR)","text":"<ul> <li>DR-1: Drift Monitoring  </li> <li>DR-2: Drift Envelope Enforcement  </li> <li>DR-3: Drift Recovery  </li> </ul>"},{"location":"RIS_Conformance_Statement/#variance-envelope-controls-ve","title":"Variance Envelope Controls (VE)","text":"<ul> <li>VE-1: Variance Definition  </li> <li>VE-2: Variance Compliance  </li> <li>VE-3: Variance Tightening  </li> </ul>"},{"location":"RIS_Conformance_Statement/#governance-boundary-controls-gb","title":"Governance Boundary Controls (GB)","text":"<ul> <li>GB-1: Boundary Definition  </li> <li>GB-2: Boundary Enforcement  </li> <li>GB-3: Interference Protection  </li> </ul>"},{"location":"RIS_Conformance_Statement/#operational-integrity-controls-op","title":"Operational Integrity Controls (OP)","text":"<ul> <li>OP-1: Evaluation Environment Consistency  </li> <li>OP-2: Reproducible Testing  </li> <li>OP-3: Audit Logging  </li> <li>OP-4: Longitudinal Monitoring  </li> </ul>"},{"location":"RIS_Conformance_Statement/#5-evidence-summary","title":"5. Evidence Summary","text":"<p>Summaries of evidence MUST be provided.</p>"},{"location":"RIS_Conformance_Statement/#51-inference-samples-reviewed","title":"5.1 Inference Samples Reviewed","text":"<ul> <li>Number of baseline samples:  </li> <li>Number of perturbation samples:  </li> <li>Number of intervals tested:</li> </ul>"},{"location":"RIS_Conformance_Statement/#52-metrics-and-calculations","title":"5.2 Metrics and Calculations","text":"<ul> <li>Chain stability metrics reviewed:  </li> <li>Semantic coherence metrics reviewed:  </li> <li>Drift sensitivity calculations reviewed:  </li> <li>Variance envelope compliance verification:  </li> <li>Boundary adherence verification:</li> </ul>"},{"location":"RIS_Conformance_Statement/#53-audit-logs-reviewed","title":"5.3 Audit Logs Reviewed","text":"<ul> <li>Ledger entries examined  </li> <li>Boundary violations  </li> <li>Drift events  </li> <li>Tool or agent interference logs  </li> <li>Configuration snapshots  </li> </ul>"},{"location":"RIS_Conformance_Statement/#6-violations-exceptions-and-notes","title":"6. Violations, Exceptions, and Notes","text":"<p>Detected Violations: (Document any failed controls, drift events, or boundary violations)</p> <p>Exceptions Noted: (For SHOULD or MAY controls)</p> <p>Limitations: (Evaluator notes on limitations or constraints of assessment)</p>"},{"location":"RIS_Conformance_Statement/#7-final-ris-classification","title":"7. Final RIS Classification","text":"<p>RIS Level Assigned: </p> <p>Classification Justification: (Based on scoring + control verification)</p> <p>Classification Valid Until: (Refer to validity rules in RIS Section 7.10)</p>"},{"location":"RIS_Conformance_Statement/#8-signatures","title":"8. Signatures","text":"<p>Evaluator Name: Evaluator Title / Organization: Evaluator Signature: Date: </p> <p>System Owner Name: System Owner Signature: Date: </p>"},{"location":"RIS_Conformance_Statement/#9-attachments-optional","title":"9. Attachments (Optional)","text":"<ul> <li>Full evaluation report  </li> <li>Metric outputs  </li> <li>Drift analysis graphs  </li> <li>Variance envelope charts  </li> <li>Ledger records (redacted as needed)  </li> <li>Configuration and parameter logs  </li> </ul>"},{"location":"RIS_Conformance_Statement/#10-copyright-notice","title":"10. Copyright Notice","text":"<p>Reasoning Integrity Standard (RIS) v1.0 \u00a9 2025 Atom Labs All Rights Reserved.</p>"},{"location":"RIS_Directory_Format/","title":"RIS Directory Format","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Directory_Format/#1-purpose","title":"1. Purpose","text":"<p>The RIS Directory Format defines how collections of RIS certification entries SHOULD be represented for:</p> <ul> <li>public directories</li> <li>internal registries</li> <li>leaderboards</li> <li>discovery APIs</li> </ul> <p>The directory is essentially a list of certification entries with optional pagination and filters.</p>"},{"location":"RIS_Directory_Format/#2-directory-structure","title":"2. Directory Structure","text":"<pre><code>{\n  \"schema_version\": \"1.0\",\n  \"generated_at\": \"YYYY-MM-DDThh:mm:ssZ\",\n  \"total_entries\": 0,\n  \"page\": 1,\n  \"page_size\": 50,\n  \"filters\": { ... },\n  \"entries\": [ ... ]\n}\n</code></pre> <p>Fields:</p> <ul> <li>schema_version: directory format version</li> <li>generated_at: timestamp of directory generation</li> <li>total_entries: total number of entries matching current filters</li> <li>page: current page number</li> <li>page_size: number of entries per page</li> <li>filters: description of filters applied</li> <li>entries: array of RIS Certification Entries</li> </ul>"},{"location":"RIS_Directory_Format/#3-filters-object","title":"3. filters Object","text":"<pre><code>\"filters\": {\n  \"organization\": \"string or null\",\n  \"ris_level_min\": \"string or null\",\n  \"ris_level_max\": \"string or null\",\n  \"category\": \"string or null\",\n  \"status\": \"string or null\",\n  \"tag\": \"string or null\",\n  \"search\": \"string or null\"\n}\n</code></pre> <p>All filters MAY be null, indicating no filter applied.</p>"},{"location":"RIS_Directory_Format/#4-entries-array","title":"4. entries Array","text":"<p>The entries array SHALL contain RIS Certification Entries as defined in the RIS Certification Entry Schema.</p> <p>Example:</p> <pre><code>\"entries\": [\n  { ... certification_entry_1 ... },\n  { ... certification_entry_2 ... }\n]\n</code></pre>"},{"location":"RIS_Directory_Format/#5-example-directory-document","title":"5. Example Directory Document","text":"<pre><code>{\n  \"schema_version\": \"1.0\",\n  \"generated_at\": \"2025-01-15T10:30:00Z\",\n  \"total_entries\": 245,\n  \"page\": 1,\n  \"page_size\": 50,\n  \"filters\": {\n    \"organization\": null,\n    \"ris_level_min\": \"RIS-3\",\n    \"ris_level_max\": null,\n    \"category\": null,\n    \"status\": \"Active\",\n    \"tag\": null,\n    \"search\": null\n  },\n  \"entries\": [\n    {\n      \"schema_version\": \"1.0\",\n      \"id\": \"CERT-2025-0001\",\n      \"system_name\": \"Example Agent\",\n      \"system_version\": \"1.2.3\",\n      \"organization\": \"ExampleCorp\",\n      \"category\": \"agent\",\n      \"ris_level\": \"RIS-3\",\n      \"composite_score\": 0.84,\n      \"status\": \"Active\",\n      \"valid_from\": \"2025-01-10\",\n      \"valid_until\": \"2026-01-10\",\n      \"evaluation_id\": \"EV-2025-0001\",\n      \"links\": { ... },\n      \"tags\": [ \"production\", \"analytics\" ],\n      \"last_updated\": \"2025-01-12\"\n    }\n  ]\n}\n</code></pre>"},{"location":"RIS_Directory_Format/#6-usage","title":"6. Usage","text":"<p>Directories may be:</p> <ul> <li>generated on demand</li> <li>cached on a schedule</li> <li>returned by a REST API</li> <li>used as static JSON for portals</li> </ul> <p>They provide the basis for:</p> <ul> <li>UI tables</li> <li>search interfaces</li> <li>filters and sorting</li> <li>public RIS directories</li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/","title":"RIS Ecosystem Roadmap","text":"<p>Reasoning Integrity Standard (RIS) Atom Labs - Strategic Roadmap Version: 1.0 Last Updated: 2025</p>"},{"location":"RIS_Ecosystem_Roadmap/#1-purpose","title":"1. Purpose","text":"<p>This roadmap defines the long-term evolution of the Reasoning Integrity Standard (RIS). It is designed to guide:</p> <ul> <li>enterprises adopting RIS  </li> <li>researchers  </li> <li>AI labs  </li> <li>regulators  </li> <li>vendors  </li> <li>evaluators  </li> <li>public-sector stakeholders  </li> </ul> <p>The roadmap outlines major initiatives planned for the RIS ecosystem across multiple phases.</p>"},{"location":"RIS_Ecosystem_Roadmap/#2-roadmap-structure","title":"2. Roadmap Structure","text":"<p>The RIS ecosystem roadmap is divided into three horizons:</p> <ol> <li>Horizon 1: Foundation (0-6 months) </li> <li>Horizon 2: Expansion (6-18 months) </li> <li>Horizon 3: Maturity (18-36 months) </li> </ol> <p>Each horizon includes goals for standards, tooling, governance, and ecosystem growth.</p>"},{"location":"RIS_Ecosystem_Roadmap/#3-horizon-1-foundation-0-6-months","title":"3. Horizon 1 - Foundation (0-6 Months)","text":"<p>RIS v1.0 establishes the baseline standard. Key objectives:</p>"},{"location":"RIS_Ecosystem_Roadmap/#31-standard-stabilization","title":"3.1 Standard Stabilization","text":"<ul> <li>Publish RIS v1.0 documentation  </li> <li>Establish governance charter  </li> <li>Set up RIS public site (static documentation)  </li> <li>Begin community feedback cycle  </li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#32-tooling-foundations","title":"3.2 Tooling Foundations","text":"<ul> <li>Define RIS scorecard schema  </li> <li>Define certification entry schema  </li> <li>Define directory schema  </li> <li>Publish SDK guide and integration guides  </li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#33-adoption-enablement","title":"3.3 Adoption Enablement","text":"<ul> <li>Publish adoption kit  </li> <li>Publish integration pathways  </li> <li>Release benchmark guide  </li> <li>Support internal enterprise pilots  </li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#34-governance-activities","title":"3.4 Governance Activities","text":"<ul> <li>Begin collecting proposals for RIS v1.1  </li> <li>Maintain change log and index  </li> <li>Begin drafting industry profiles (informative)</li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#4-horizon-2-expansion-6-18-months","title":"4. Horizon 2 - Expansion (6-18 Months)","text":"<p>This is where RIS moves from an emerging standard to a widely adopted, auditable framework.</p>"},{"location":"RIS_Ecosystem_Roadmap/#41-standards-development","title":"4.1 Standards Development","text":"<ul> <li>Release RIS v1.1 (fully backward compatible)  </li> <li>Add multi-agent and RAG-specific profiles  </li> <li>Expand drift and variance methodologies  </li> <li>Introduce cross-model consistency metrics  </li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#42-public-infrastructure","title":"4.2 Public Infrastructure","text":"<ul> <li>Launch RIS dynamic portal (v1)  </li> <li>Publish public directory of certified systems  </li> <li>Publish RIS scorecards for reference models  </li> <li>Enable organization-level certification listings  </li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#43-tooling-evaluators","title":"4.3 Tooling &amp; Evaluators","text":"<ul> <li>Release RIS open-source reference evaluator (optional)</li> <li>Release RIS Python evaluation toolkit  </li> <li>Enable command-line RIS evaluation workflows  </li> <li>Optional: provide sample datasets for standardized benchmarking  </li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#44-regulatory-enterprise-alignment","title":"4.4 Regulatory &amp; Enterprise Alignment","text":"<ul> <li>Map RIS to more regulatory frameworks  </li> <li>Partner with enterprise governance teams for structured usage  </li> <li>Begin liaison work with external standards bodies  </li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#5-horizon-3-maturity-18-36-months","title":"5. Horizon 3 - Maturity (18-36 Months)","text":"<p>RIS becomes a global reasoning integrity benchmark.</p>"},{"location":"RIS_Ecosystem_Roadmap/#51-standards-maturation","title":"5.1 Standards Maturation","text":"<ul> <li>Release RIS v2.0 (may introduce breaking changes)</li> <li>Formalize coherence fingerprinting methods  </li> <li>Formalize predictive drift models  </li> <li>Introduce reasoning-route compression metrics  </li> <li>Release scenario-based benchmark suites  </li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#52-full-ecosystem-tools","title":"5.2 Full Ecosystem Tools","text":"<ul> <li>RIS certification testing suite  </li> <li>RIS auditor toolkit  </li> <li>RIS validator for CI/CD pipelines  </li> <li>RIS dashboard for drift &amp; stability monitoring  </li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#53-public-portal-expansion","title":"5.3 Public Portal Expansion","text":"<ul> <li>Public RIS leaderboard  </li> <li>RIS certification badge directory  </li> <li>Model-version lineage tracking  </li> <li>Long-term drift benchmark archives  </li> <li>Public scorecard submission tools  </li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#54-ecosystem-governance","title":"5.4 Ecosystem Governance","text":"<ul> <li>Establish the RIS Technical Steering Committee (TSC)</li> <li>Multi-organization participation  </li> <li>Formal voting process for version changes  </li> <li>External auditor network  </li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#6-ris-ecosystem-vision","title":"6. RIS Ecosystem Vision","text":"<p>The long-term goal of RIS is to become:</p> <ul> <li>the de facto global standard for reasoning integrity  </li> <li>a regulator-ready measurement framework  </li> <li>a foundation for multi-agent safety and auditing </li> <li>a neutral, implementation-agnostic benchmark  </li> <li>a standard that is adopted by:</li> <li>Fortune 100 enterprises  </li> <li>public-sector institutions  </li> <li>AI labs  </li> <li>safety organizations  </li> <li>open-source communities  </li> </ul> <p>RIS is designed to be:</p> <ul> <li>durable  </li> <li>transparent  </li> <li>stable  </li> <li>future-proof  </li> <li>vendor-neutral  </li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#7-summary","title":"7. Summary","text":"<p>The RIS ecosystem roadmap guides RIS from its initial specification into a fully governed, widely adopted, cross-industry integrity framework.</p> <p>This roadmap is updated periodically to reflect:</p> <ul> <li>technological changes  </li> <li>regulatory development  </li> <li>industry feedback  </li> <li>academic research  </li> <li>practical usage patterns  </li> </ul>"},{"location":"RIS_Ecosystem_Roadmap/#end-of-document","title":"End of Document","text":""},{"location":"RIS_Evaluation_Report_Template/","title":"RIS Evaluation Report Template","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Evaluation_Report_Template/#1-report-metadata","title":"1. Report Metadata","text":"<p>Report Title: System Name: System Version: Model(s) Evaluated: Model Version / Build: Organization: Evaluator Name: Evaluation Date: Evaluation Location / Environment: </p> <p>Requested RIS Level: Final RIS Level Assigned: </p>"},{"location":"RIS_Evaluation_Report_Template/#2-system-description","title":"2. System Description","text":"<p>Provide a detailed description of the system evaluated.</p> <p>2.1 System Overview (Architecture, components, agents, tools, RAG sources, orchestration frameworks)</p> <p>2.2 Primary Use Cases (Decision-support, workflows, automation, analysis, etc.)</p> <p>2.3 Operational Context (Environment, deployment type, integrations)</p> <p>2.4 Reasoning Mode (single-turn, multi-turn, agentic, tool-augmented, retrieval-augmented)</p>"},{"location":"RIS_Evaluation_Report_Template/#3-evaluation-environment","title":"3. Evaluation Environment","text":"<p>3.1 Hardware and Software Environment (servers, GPUs, cloud settings, OS, dependencies)</p> <p>3.2 Inference Parameters (temperature, top-p, frequency penalty, system prompts, context size)</p> <p>3.3 Configuration Settings (model config, agent settings, allowed tools, retrieval limits)</p> <p>3.4 Environmental Invariants (parameters that remained constant as required by RIS Section 9)</p>"},{"location":"RIS_Evaluation_Report_Template/#4-evaluation-methodology","title":"4. Evaluation Methodology","text":"<p>4.1 Evaluation Framework Used (Refer to RIS Section 9)</p> <p>4.2 Prompt Sets Used - Number of baseline prompts - Number of perturbation prompts - Domain(s) covered - Difficulty distribution  </p> <p>4.3 Sampling Methodology - Number of repeated inference samples - Number of perturbation samples - Testing intervals (if temporal evaluation performed)</p> <p>4.4 Baseline Establishment (Describe how the baseline was created and validated)</p>"},{"location":"RIS_Evaluation_Report_Template/#5-metrics-and-results","title":"5. Metrics and Results","text":""},{"location":"RIS_Evaluation_Report_Template/#51-chain-stability","title":"5.1 Chain Stability","text":"<ul> <li>stability score (mean)  </li> <li>stability score (min/max)  </li> <li>deviation patterns  </li> <li>compliance with variance envelope  </li> </ul>"},{"location":"RIS_Evaluation_Report_Template/#52-semantic-coherence","title":"5.2 Semantic Coherence","text":"<ul> <li>coherence score (mean)  </li> <li>conflicting or inconsistent samples  </li> <li>semantic retention analysis  </li> </ul>"},{"location":"RIS_Evaluation_Report_Template/#53-drift-sensitivity","title":"5.3 Drift Sensitivity","text":"<ul> <li>drift score (mean)  </li> <li>drift trend over iterations  </li> <li>drift triggered by perturbations  </li> <li>structural vs semantic drift  </li> </ul>"},{"location":"RIS_Evaluation_Report_Template/#54-variance-envelope-compliance","title":"5.4 Variance Envelope Compliance","text":"<ul> <li>total samples  </li> <li>samples within envelope  </li> <li>samples outside envelope  </li> <li>compliance percentage  </li> </ul>"},{"location":"RIS_Evaluation_Report_Template/#55-governance-boundary-adherence","title":"5.5 Governance Boundary Adherence","text":"<ul> <li>boundary violations observed  </li> <li>types of violations (semantic, contextual, tool-induced, agent-induced)  </li> <li>severity and frequency  </li> </ul>"},{"location":"RIS_Evaluation_Report_Template/#6-drift-and-variance-analysis","title":"6. Drift and Variance Analysis","text":"<p>6.1 Drift Characterization (structural drift, semantic drift, temporal drift, interference drift)</p> <p>6.2 Drift Thresholds (document thresholds used and whether exceeded)</p> <p>6.3 Drift Events (list and describe any drift events detected)</p> <p>6.4 Longitudinal Trends (if applicable) (temporal stability, degradation, cycle patterns)</p> <p>6.5 Variance Envelope Breakdown (how envelope was determined, deviation statistics)</p>"},{"location":"RIS_Evaluation_Report_Template/#7-boundary-and-interference-analysis","title":"7. Boundary and Interference Analysis","text":"<p>7.1 Governance Boundary Tests (prompts, results, violations)</p> <p>7.2 Tool/Agent Interference Tests (description of scenarios tested and outcomes)</p> <p>7.3 Retrieval/RAG Interference Tests (if applicable)</p> <p>7.4 Multi-Agent Interaction Analysis (if applicable)</p>"},{"location":"RIS_Evaluation_Report_Template/#8-control-compliance-review","title":"8. Control Compliance Review","text":"<p>Provide compliance status for all applicable controls from RIS Section 6.</p> <p>Chain Stability Controls (RS) - RS-1 - RS-2 - RS-3  </p> <p>Semantic Coherence Controls (SC) - SC-1 - SC-2 - SC-3  </p> <p>Drift Resistance Controls (DR) - DR-1 - DR-2 - DR-3  </p> <p>Variance Envelope Controls (VE) - VE-1 - VE-2 - VE-3  </p> <p>Governance Boundary Controls (GB) - GB-1 - GB-2 - GB-3  </p> <p>Operational Integrity Controls (OP) - OP-1 - OP-2 - OP-3 - OP-4  </p> <p>Mark each as: Compliant / Non-Compliant / Not Applicable</p>"},{"location":"RIS_Evaluation_Report_Template/#9-violations-and-exceptions","title":"9. Violations and Exceptions","text":"<p>9.1 Violations Detected (detail any control or requirement failures)</p> <p>9.2 Boundary Violations </p> <p>9.3 Drift or Variance Violations </p> <p>9.4 Exceptions to SHOULD or MAY Controls </p> <p>9.5 Severity and Risk Classification (Refer to RIS Section 8)</p>"},{"location":"RIS_Evaluation_Report_Template/#10-final-classification","title":"10. Final Classification","text":"<p>10.1 Composite Reasoning Integrity Score: (weighted score)</p> <p>10.2 Score-Based Eligibility: (level eligible for based on scoring)</p> <p>10.3 Control-Based Eligibility: (level eligible for based on control compliance)</p> <p>10.4 Final RIS Level Assigned: </p> <p>10.5 Summary Justification: (clear rationale for level assignment)</p>"},{"location":"RIS_Evaluation_Report_Template/#11-recommendations-and-corrective-actions","title":"11. Recommendations and Corrective Actions","text":"<p>Provide evaluator recommendations regarding:</p> <ul> <li>improvements to drift controls  </li> <li>variance tightening  </li> <li>boundary enforcement  </li> <li>governance mode adjustments  </li> <li>sampling consistency improvements  </li> <li>agent or tool isolation  </li> </ul> <p>Include remediation steps if requesting RIS-3 or RIS-4 upgrades.</p>"},{"location":"RIS_Evaluation_Report_Template/#12-appendices","title":"12. Appendices","text":"<p>Append any relevant materials:</p> <ul> <li>raw inference samples  </li> <li>stability/coherence charts  </li> <li>drift graphs  </li> <li>variance analysis data  </li> <li>ledger or log excerpts  </li> <li>configuration snapshots  </li> <li>prompt sets used  </li> <li>evaluator notes  </li> </ul>"},{"location":"RIS_Evaluation_Report_Template/#13-signatures","title":"13. Signatures","text":"<p>Evaluator Name: Evaluator Title / Organization: Evaluator Signature: Date: </p> <p>System Owner Name: System Owner Signature: Date: </p>"},{"location":"RIS_Evaluation_Report_Template/#14-legal-notice","title":"14. Legal Notice","text":"<p>Reasoning Integrity Standard (RIS) v1.0 \u00a9 2025 Atom Labs. All Rights Reserved.</p> <p>This report template is provided for use with RIS evaluations and may not be modified without attribution.</p>"},{"location":"RIS_Example_Prompt_Dataset/","title":"RIS Example Prompt Dataset","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Example_Prompt_Dataset/#1-introduction","title":"1. Introduction","text":"<p>This document provides an example dataset structure for RIS evaluation. It is intended to help organizations build their own baseline and perturbation prompt sets.</p> <p>RIS does not mandate specific prompts. This dataset format demonstrates best practices.</p>"},{"location":"RIS_Example_Prompt_Dataset/#2-dataset-structure","title":"2. Dataset Structure","text":"<p>A RIS dataset consists of:</p> <ul> <li>baseline prompts  </li> <li>perturbation prompts  </li> <li>metadata  </li> </ul> <p>Recommended structure:</p> <pre><code>{\n  \"prompts\": [\n    {\n      \"id\": \"P001\",\n      \"baseline\": \"Explain the role of supply shocks in inflation.\",\n      \"perturbations\": [\n        \"Describe how supply disruptions affect inflation.\",\n        \"How do supply-side constraints influence inflation?\",\n        \"Explain how production bottlenecks relate to inflation.\"\n      ],\n      \"metadata\": {\n        \"domain\": \"economics\",\n        \"difficulty\": \"medium\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"RIS_Example_Prompt_Dataset/#3-example-baseline-prompts","title":"3. Example Baseline Prompts","text":"<p>Example prompts across categories:</p>"},{"location":"RIS_Example_Prompt_Dataset/#reasoning","title":"Reasoning","text":"<ul> <li>Explain how signal-to-noise ratio affects information reliability.</li> <li>Describe the relationship between cause and effect in policy analysis.</li> </ul>"},{"location":"RIS_Example_Prompt_Dataset/#analysis","title":"Analysis","text":"<ul> <li>Compare two approaches for optimizing system performance.</li> <li>Analyze the implications of shifting economic trends.</li> </ul>"},{"location":"RIS_Example_Prompt_Dataset/#multi-step-reasoning","title":"Multi-step reasoning","text":"<ul> <li>Outline the steps required to evaluate the validity of an argument.</li> <li>Explain how to derive insights from incomplete information.</li> </ul>"},{"location":"RIS_Example_Prompt_Dataset/#4-example-perturbation-prompts","title":"4. Example Perturbation Prompts","text":"<p>Perturbations MUST preserve meaning while altering structure or phrasing.</p> <p>Examples:</p> <pre><code>Baseline:\n\"Explain the impact of market consolidation on competition.\"\n\nPerturbations:\n\"Describe how consolidation affects competitive dynamics.\"\n\"How does market concentration influence competition?\"\n\"Discuss the consequences of fewer firms in a market.\"\n</code></pre>"},{"location":"RIS_Example_Prompt_Dataset/#5-metadata-examples","title":"5. Metadata Examples","text":"<p>Metadata helps evaluators track:</p> <ul> <li>domain  </li> <li>complexity  </li> <li>reasoning type  </li> <li>intended test category  </li> </ul> <p>Example metadata:</p> <pre><code>{\n  \"domain\": \"economics\",\n  \"difficulty\": \"medium\",\n  \"task_type\": \"explanatory\"\n}\n</code></pre>"},{"location":"RIS_Example_Prompt_Dataset/#6-dataset-size-guidelines","title":"6. Dataset Size Guidelines","text":"<p>Recommended:</p> <ul> <li>50+ baseline prompts for general-purpose models  </li> <li>20+ baseline prompts for domain-specific models  </li> <li>10 perturbations per selected baseline prompt  </li> </ul>"},{"location":"RIS_Example_Prompt_Dataset/#7-customization","title":"7. Customization","text":"<p>Organizations MAY:</p> <ul> <li>create domain-specific prompt sets  </li> <li>test scenario-specific reasoning stability  </li> <li>build templates for different industries  </li> <li>integrate datasets into automated test pipelines  </li> </ul>"},{"location":"RIS_Example_Prompt_Dataset/#8-summary","title":"8. Summary","text":"<p>This example dataset demonstrates:</p> <ul> <li>how to structure RIS prompts  </li> <li>how to create baseline and perturbation sets  </li> <li>how to store metadata  </li> <li>how to prepare evaluation inputs  </li> </ul> <p>Organizations SHOULD tailor their datasets to their risk profile, domain, and model type.</p>"},{"location":"RIS_Glossary/","title":"RIS Glossary","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Glossary/#introduction","title":"Introduction","text":"<p>This glossary defines key terminology used throughout the Reasoning Integrity Standard (RIS). All terms in this glossary are informative unless referenced as normative terminology in the RIS specification.</p>"},{"location":"RIS_Glossary/#a","title":"A","text":""},{"location":"RIS_Glossary/#agent","title":"Agent","text":"<p>A system or module that uses an LLM to perform actions, reasoning steps, or tool interactions autonomously or semi-autonomously.</p>"},{"location":"RIS_Glossary/#agent-interference","title":"Agent Interference","text":"<p>A phenomenon in which the reasoning of one agent is influenced or altered by another agent\u2019s output, state, or actions.</p>"},{"location":"RIS_Glossary/#b","title":"B","text":""},{"location":"RIS_Glossary/#baseline","title":"Baseline","text":"<p>A set of reasoning samples used to define stability, coherence, variance envelopes, and drift thresholds.</p>"},{"location":"RIS_Glossary/#baseline-prompt","title":"Baseline Prompt","text":"<p>A prompt used to generate repeated inference samples without variation to establish base reasoning behavior.</p>"},{"location":"RIS_Glossary/#boundary-violation","title":"Boundary Violation","text":"<p>A reasoning event where the system exceeds defined contextual, semantic, or operational constraints.</p>"},{"location":"RIS_Glossary/#boundary-enforcement","title":"Boundary Enforcement","text":"<p>A set of checks preventing a system from exceeding defined reasoning limits.</p>"},{"location":"RIS_Glossary/#c","title":"C","text":""},{"location":"RIS_Glossary/#chain-stability","title":"Chain Stability","text":"<p>The consistency of the structure, sequence, and transitions of reasoning chains across repeated evaluations.</p>"},{"location":"RIS_Glossary/#cognitive-drift","title":"Cognitive Drift","text":"<p>A measurable deviation in reasoning behavior over time, across samples, or under perturbations.</p>"},{"location":"RIS_Glossary/#composite-score","title":"Composite Score","text":"<p>A weighted aggregate reasoning integrity score based on metrics defined in RIS Section 5.</p>"},{"location":"RIS_Glossary/#conformance","title":"Conformance","text":"<p>The degree to which a system meets RIS scoring thresholds and mandatory controls.</p>"},{"location":"RIS_Glossary/#context-leakage","title":"Context Leakage","text":"<p>Use of information not present in the prompt, environment, or defined boundaries.</p>"},{"location":"RIS_Glossary/#d","title":"D","text":""},{"location":"RIS_Glossary/#deterministic-reasoning-pattern","title":"Deterministic Reasoning Pattern","text":"<p>A reasoning structure that remains stable under fixed parameters, even if surface-level outputs vary.</p>"},{"location":"RIS_Glossary/#drift-sensitivity","title":"Drift Sensitivity","text":"<p>The degree to which a system exhibits instability or divergence when exposed to repeated or perturbed inference cycles.</p>"},{"location":"RIS_Glossary/#drift-envelope","title":"Drift Envelope","text":"<p>A defined set of acceptable drift boundaries used to determine permissible reasoning variation.</p>"},{"location":"RIS_Glossary/#e","title":"E","text":""},{"location":"RIS_Glossary/#evaluation-environment","title":"Evaluation Environment","text":"<p>The set of configuration, hardware, software, and inference parameters under which RIS assessments occur.</p>"},{"location":"RIS_Glossary/#envelope-compliance","title":"Envelope Compliance","text":"<p>A measurement indicating how many reasoning samples fall within established variance or drift thresholds.</p>"},{"location":"RIS_Glossary/#g","title":"G","text":""},{"location":"RIS_Glossary/#governance-boundary","title":"Governance Boundary","text":"<p>A defined constraint determining what context, domain, or information the system may use during reasoning.</p>"},{"location":"RIS_Glossary/#governance-mode","title":"Governance Mode","text":"<p>A system state (hold, elevate, lockdown) used to manage reasoning behavior and enforce constraints.</p>"},{"location":"RIS_Glossary/#i","title":"I","text":""},{"location":"RIS_Glossary/#interference-event","title":"Interference Event","text":"<p>An unintended alteration of reasoning behavior caused by external components such as tools, retrieval systems, agents, or environment variations.</p>"},{"location":"RIS_Glossary/#inference-parameters","title":"Inference Parameters","text":"<p>Settings that govern model behavior, such as temperature, top-p, context window, and system instructions.</p>"},{"location":"RIS_Glossary/#l","title":"L","text":""},{"location":"RIS_Glossary/#ledger-reasoning-ledger","title":"Ledger (Reasoning Ledger)","text":"<p>A sequential, hash-linked record of reasoning evaluations, metrics, and verdicts used for auditability.</p>"},{"location":"RIS_Glossary/#longitudinal-monitoring","title":"Longitudinal Monitoring","text":"<p>Tracking reasoning behavior across time intervals to detect degradation or temporal drift.</p>"},{"location":"RIS_Glossary/#m","title":"M","text":""},{"location":"RIS_Glossary/#measurement-framework","title":"Measurement Framework","text":"<p>The set of metrics, scoring rules, and evaluation processes defined in RIS Section 5.</p>"},{"location":"RIS_Glossary/#model-configuration","title":"Model Configuration","text":"<p>The parameters and environmental conditions that determine how a model executes reasoning.</p>"},{"location":"RIS_Glossary/#p","title":"P","text":""},{"location":"RIS_Glossary/#perturbation-prompt","title":"Perturbation Prompt","text":"<p>A semantically equivalent variation of a baseline prompt used to test robustness and drift.</p>"},{"location":"RIS_Glossary/#prompt-set","title":"Prompt Set","text":"<p>A collection of baseline and perturbation prompts used in RIS evaluation.</p>"},{"location":"RIS_Glossary/#r","title":"R","text":""},{"location":"RIS_Glossary/#reasoning-boundary","title":"Reasoning Boundary","text":"<p>A defined limit determining what the system may consider or incorporate during reasoning.</p>"},{"location":"RIS_Glossary/#reasoning-chain","title":"Reasoning Chain","text":"<p>An internal conceptual pathway or structure the system follows to derive an output.</p>"},{"location":"RIS_Glossary/#reasoning-integrity","title":"Reasoning Integrity","text":"<p>The consistency, stability, and predictability of reasoning patterns under controlled evaluation.</p>"},{"location":"RIS_Glossary/#reasoning-stability","title":"Reasoning Stability","text":"<p>The degree to which a system\u2019s reasoning remains unchanged across repeated evaluations.</p>"},{"location":"RIS_Glossary/#reference-implementation","title":"Reference Implementation","text":"<p>An example implementation demonstrating RIS-aligned controls and metrics (e.g., LCAC).</p>"},{"location":"RIS_Glossary/#repeated-inference-sampling","title":"Repeated Inference Sampling","text":"<p>A method of obtaining multiple outputs for the same prompt to evaluate stability and variance.</p>"},{"location":"RIS_Glossary/#s","title":"S","text":""},{"location":"RIS_Glossary/#semantic-coherence","title":"Semantic Coherence","text":"<p>The degree to which reasoning outputs preserve meaning, alignment, and context across samples.</p>"},{"location":"RIS_Glossary/#structural-drift","title":"Structural Drift","text":"<p>A deviation in the structure or sequence of reasoning steps beyond expected thresholds.</p>"},{"location":"RIS_Glossary/#system-configuration","title":"System Configuration","text":"<p>A full description of parameters, environment, tools, agents, and constraints under evaluation.</p>"},{"location":"RIS_Glossary/#t","title":"T","text":""},{"location":"RIS_Glossary/#temporal-drift","title":"Temporal Drift","text":"<p>Progressive degradation or change in reasoning behavior across time or inference windows.</p>"},{"location":"RIS_Glossary/#trust-score","title":"Trust Score","text":"<p>A scalar value representing the model\u2019s measured reasoning stability (used in reference implementations).</p>"},{"location":"RIS_Glossary/#v","title":"V","text":""},{"location":"RIS_Glossary/#variance-envelope","title":"Variance Envelope","text":"<p>A statistical boundary defining acceptable variance in stability or coherence.</p>"},{"location":"RIS_Glossary/#variance-envelope-compliance","title":"Variance Envelope Compliance","text":"<p>A measurement of how many samples fall within the variance envelope.</p>"},{"location":"RIS_Glossary/#violations","title":"Violations","text":"<p>Any deviations from required controls or thresholds that impact reasoning integrity.</p>"},{"location":"RIS_Glossary/#end-of-glossary","title":"End of Glossary","text":""},{"location":"RIS_Governance_Charter/","title":"RIS Governance Charter","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Governance_Charter/#1-purpose","title":"1. Purpose","text":"<p>The RIS Governance Charter defines the governance structure, responsibilities, procedures, and rules used to maintain, update, and evolve the Reasoning Integrity Standard (RIS).</p> <p>This charter ensures that RIS remains:</p> <ul> <li>stable  </li> <li>transparent  </li> <li>industry-neutral  </li> <li>technically rigorous  </li> <li>responsive to enterprise and regulatory needs  </li> </ul>"},{"location":"RIS_Governance_Charter/#2-governing-body","title":"2. Governing Body","text":"<p>The RIS standard is maintained by:</p> <p>Atom Labs Standards Division</p> <p>Responsibilities include:</p> <ul> <li>maintaining RIS documents  </li> <li>approving revisions  </li> <li>publishing new versions  </li> <li>coordinating feedback  </li> <li>overseeing certification program evolution  </li> </ul>"},{"location":"RIS_Governance_Charter/#3-versioning-policy","title":"3. Versioning Policy","text":"<p>RIS SHALL follow semantic, multi-part versioning:</p> <ul> <li>Major versions (X.0) introduce breaking changes  </li> <li>Minor versions (X.Y) add compatible enhancements  </li> <li>Patch versions (X.Y.Z) include editorial corrections  </li> </ul>"},{"location":"RIS_Governance_Charter/#4-change-proposal-process","title":"4. Change Proposal Process","text":"<p>Changes to RIS MAY originate from:</p> <ul> <li>industry partners  </li> <li>enterprises  </li> <li>academic researchers  </li> <li>regulators  </li> <li>Atom Labs internal teams  </li> </ul>"},{"location":"RIS_Governance_Charter/#steps","title":"Steps:","text":"<ol> <li>Submission of a change request (CR)  </li> <li>Internal review  </li> <li>Impact assessment  </li> <li>Draft revision  </li> <li>Optional public comment  </li> <li>Approval  </li> <li>Publication  </li> </ol>"},{"location":"RIS_Governance_Charter/#5-release-cycle","title":"5. Release Cycle","text":"<p>RIS releases SHALL follow this cycle:</p> <ol> <li>Working Draft  </li> <li>Internal Review Draft  </li> <li>Candidate Specification  </li> <li>Final Specification  </li> <li>Public Release  </li> </ol> <p>Typical cycle:</p> <ul> <li>Draft period: 2\u20133 months  </li> <li>Review period: 30\u201360 days  </li> <li>Publication: upon approval  </li> </ul>"},{"location":"RIS_Governance_Charter/#6-public-comment-process","title":"6. Public Comment Process","text":"<p>Public comment MAY be collected via:</p> <ul> <li>GitHub issues  </li> <li>Email submissions  </li> <li>Standards feedback forms  </li> </ul> <p>All comments SHALL be cataloged and considered.</p>"},{"location":"RIS_Governance_Charter/#7-backward-compatibility","title":"7. Backward Compatibility","text":"<p>Minor versions SHALL be backward-compatible with the prior minor release. Major versions MAY introduce structural changes.</p> <p>Backward compatibility decisions SHALL be documented in the Change Log.</p>"},{"location":"RIS_Governance_Charter/#8-sunset-policy","title":"8. Sunset Policy","text":"<p>Older versions MAY be deprecated after:</p> <ul> <li>24 months, or  </li> <li>superseding major versions  </li> </ul> <p>Deprecated versions SHALL remain archived.</p>"},{"location":"RIS_Governance_Charter/#9-dispute-resolution","title":"9. Dispute Resolution","text":"<p>In case of conflict between:</p> <ul> <li>industry requests  </li> <li>internal recommendations  </li> <li>regulatory guidance  </li> </ul> <p>Atom Labs Standards Division SHALL make the final determination.</p>"},{"location":"RIS_Governance_Charter/#10-transparency-commitments","title":"10. Transparency Commitments","text":"<p>RIS governance SHALL maintain:</p> <ul> <li>public release notes  </li> <li>change logs  </li> <li>archived versions  </li> <li>version history  </li> <li>citation information  </li> </ul>"},{"location":"RIS_Governance_Charter/#11-contact","title":"11. Contact","text":"<p>RIS Governance and Standards Atom Labs RIS@atomlabs.app</p>"},{"location":"RIS_Header_Footer_Templates/","title":"RIS Header and Footer Templates","text":"<p>Reusable snippets for all RIS documents.</p>"},{"location":"RIS_Header_Footer_Templates/#1-standard-header-block","title":"1. Standard Header Block","text":"<p>Use at the top of all RIS documents:</p> <p>Reasoning Integrity Standard (RIS) Version 1.0 - 2025 Published by Atom Labs  </p>"},{"location":"RIS_Header_Footer_Templates/#2-standard-footer-block","title":"2. Standard Footer Block","text":"<p>Use at the bottom of all RIS documents:</p> <p>\u00a9 2025 Atom Labs - All Rights Reserved Part of the RIS v1.0 Specification Series https://ris.atomlabs.app  </p>"},{"location":"RIS_Header_Footer_Templates/#3-optional-extended-footer","title":"3. Optional Extended Footer","text":"<p>For documents with legal or governance relevance:</p> <p>\u00a9 2025 Atom Labs - All Rights Reserved RIS v1.0 - Released January 2025 For official use under the RIS Certification Program https://ris.atomlabs.app  </p>"},{"location":"RIS_Integration_Guide/","title":"RIS Integration Guide","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Integration_Guide/#1-introduction","title":"1. Introduction","text":"<p>The RIS Integration Guide explains how organizations can operationalize the Reasoning Integrity Standard (RIS) in real environments. RIS is implementation-agnostic and may be integrated into:</p> <ul> <li>LLM-driven applications</li> <li>autonomous agent frameworks</li> <li>enterprise AI governance processes</li> <li>CI/CD pipelines for model deployment</li> <li>research evaluation workflows</li> <li>multi-agent or tool-augmented systems</li> </ul> <p>This guide outlines three integration models:</p> <ol> <li>Self-Hosted RIS Evaluation  </li> <li>Governance and Risk Program Integration  </li> <li>LCAC Reference Implementation Integration (optional)</li> </ol> <p>Organizations may adopt any combination of these models.</p>"},{"location":"RIS_Integration_Guide/#2-integration-model-overview","title":"2. Integration Model Overview","text":"<p>RIS may be integrated in one of three primary ways:</p> <ol> <li> <p>Self-Hosted Evaluation    The organization runs all sampling, metrics, and reports internally.</p> </li> <li> <p>Governance and Risk Program Integration    RIS is integrated into development, deployment, and compliance processes.</p> </li> <li> <p>Reference Implementation (LCAC) Integration    LCAC is used as a pre-built evaluator for trust, variance, drift, and ledgering.</p> </li> </ol> <p>RIS does not require LCAC or any specific tooling.</p>"},{"location":"RIS_Integration_Guide/#3-model-a-self-hosted-ris-evaluation","title":"3. Model A: Self-Hosted RIS Evaluation","text":"<p>Most organizations will implement RIS independently using their own:</p> <ul> <li>LLMs  </li> <li>datasets  </li> <li>scripts  </li> <li>infrastructure  </li> </ul>"},{"location":"RIS_Integration_Guide/#31-requirements","title":"3.1 Requirements","text":"<p>To self-host RIS evaluation, an organization needs:</p> <ul> <li>the model or agent under test  </li> <li>ability to run repeated inference  </li> <li>storage for samples  </li> <li>basic statistical analysis tools  </li> <li>access to the RIS Benchmark Guide  </li> <li>the RIS templates for reporting  </li> </ul> <p>No external services or dependencies are required.</p>"},{"location":"RIS_Integration_Guide/#32-workflow","title":"3.2 Workflow","text":"<ol> <li>Build baseline and perturbation prompt sets  </li> <li>Freeze inference parameters  </li> <li>Run repeated sampling  </li> <li>Compute metrics (stability, coherence, drift, variance)  </li> <li>Detect boundary and interference issues  </li> <li>Complete the Evaluation Report  </li> <li>Complete the Conformance Statement  </li> <li>Assign a RIS level  </li> </ol>"},{"location":"RIS_Integration_Guide/#33-best-practices","title":"3.3 Best Practices","text":"<ul> <li>Use version-controlled evaluation scripts  </li> <li>Store all raw inference data  </li> <li>Re-baseline after model updates  </li> <li>Confirm invariants before each evaluation  </li> </ul>"},{"location":"RIS_Integration_Guide/#4-model-b-governance-and-risk-program-integration","title":"4. Model B: Governance and Risk Program Integration","text":"<p>Many enterprises integrate RIS into internal governance, similar to:</p> <ul> <li>SOC 2 controls  </li> <li>ISO 27001 Annex A requirements  </li> <li>NIST RMF governance cycles  </li> <li>model approval processes  </li> </ul>"},{"location":"RIS_Integration_Guide/#41-integration-points","title":"4.1 Integration Points","text":"<p>Organizations typically integrate RIS into:</p> <ul> <li>model deployment approval workflows  </li> <li>AI risk assessment checklists  </li> <li>internal audit cycles  </li> <li>model review boards  </li> <li>vendor evaluation processes  </li> <li>regulatory documentation packages  </li> </ul>"},{"location":"RIS_Integration_Guide/#42-minimal-governance-integration","title":"4.2 Minimal Governance Integration","text":"<p>A minimal governance setup includes:</p> <ul> <li>a scheduled RIS evaluation (annual or semiannual)  </li> <li>a conformance statement stored in governance repos  </li> <li>drift monitoring for production systems  </li> <li>periodic variance or stability checks  </li> </ul>"},{"location":"RIS_Integration_Guide/#43-full-governance-integration","title":"4.3 Full Governance Integration","text":"<p>A comprehensive setup includes:</p> <ul> <li>RIS evaluation required before model deployment  </li> <li>RIS level assigned to each model version  </li> <li>continuous drift monitoring  </li> <li>automatic triggers for reassessment  </li> <li>RIS level documented in model cards  </li> <li>mandatory remediation for RIS control violations  </li> </ul>"},{"location":"RIS_Integration_Guide/#44-policy-example","title":"4.4 Policy Example","text":"<p>Organizations may adopt a policy such as:</p> <pre><code>All AI systems deployed into production must achieve\na minimum RIS-3 classification unless explicitly exempted.\n</code></pre>"},{"location":"RIS_Integration_Guide/#5-model-c-lcac-reference-implementation-optional","title":"5. Model C: LCAC Reference Implementation (Optional)","text":"<p>LCAC is provided as an informative reference \u2014 not a requirement.</p> <p>Organizations may choose to integrate LCAC to automate:</p> <ul> <li>trust score calculation  </li> <li>drift and variance detection  </li> <li>semantic and structural analysis  </li> <li>ledgering  </li> <li>governance mode determination  </li> </ul> <p>LCAC can operate:</p> <ul> <li>as a standalone evaluation service  </li> <li>embedded into agent toolchains  </li> <li>inside CI pipelines  </li> <li>alongside multi-agent orchestration frameworks  </li> </ul>"},{"location":"RIS_Integration_Guide/#51-when-to-use-lcac","title":"5.1 When to Use LCAC","text":"<p>Use LCAC if the organization wants:</p> <ul> <li>automated metric calculation  </li> <li>hash-linked evaluation ledger  </li> <li>pre-built drift and variance logic  </li> <li>governance boundary enforcement  </li> <li>optional personas for stability tuning  </li> </ul>"},{"location":"RIS_Integration_Guide/#52-when-not-to-use-lcac","title":"5.2 When Not to Use LCAC","text":"<p>Avoid LCAC integration if:</p> <ul> <li>the organization already maintains equivalent tools  </li> <li>regulations require an internal-only implementation  </li> <li>the environment restricts external evaluation logic  </li> </ul> <p>RIS does not require LCAC for compliance.</p>"},{"location":"RIS_Integration_Guide/#6-integration-with-cicd-pipelines","title":"6. Integration With CI/CD Pipelines","text":"<p>RIS can be integrated into deployment pipelines similar to:</p> <ul> <li>unit tests  </li> <li>static analysis  </li> <li>vulnerability scanning  </li> </ul>"},{"location":"RIS_Integration_Guide/#61-use-cases","title":"6.1 Use Cases","text":"<ul> <li>evaluate reasoning stability before model release  </li> <li>block deployments that fail RIS thresholds  </li> <li>perform regression reasoning tests  </li> <li>capture drift early  </li> </ul>"},{"location":"RIS_Integration_Guide/#62-example-pipeline-flow","title":"6.2 Example Pipeline Flow","text":"<ol> <li>Developer submits model update  </li> <li>Pipeline triggers RIS evaluation script  </li> <li>Metrics computed automatically  </li> <li>If composite score &lt; RIS-3 threshold \u2192 block deployment  </li> <li>If drift or variance violations occur \u2192 flag for remediation  </li> <li>Store results in evaluation logs  </li> </ol>"},{"location":"RIS_Integration_Guide/#7-integration-with-llm-and-agent-architectures","title":"7. Integration With LLM and Agent Architectures","text":"<p>RIS evaluation integrates into:</p>"},{"location":"RIS_Integration_Guide/#71-single-turn-llm-applications","title":"7.1 Single-Turn LLM Applications","text":"<ul> <li>simple repeated inference  </li> <li>stability and coherence scoring  </li> <li>variance and drift checks  </li> </ul>"},{"location":"RIS_Integration_Guide/#72-multi-turn-systems","title":"7.2 Multi-Turn Systems","text":"<ul> <li>measure consistency across turns  </li> <li>track drift over turn sequences  </li> <li>detect context leakage or overexpansion  </li> </ul>"},{"location":"RIS_Integration_Guide/#73-agentic-frameworks","title":"7.3 Agentic Frameworks","text":"<ul> <li>test reasoning behavior before and after tool use  </li> <li>detect tool-induced drift  </li> <li>verify boundary adherence  </li> <li>test agent-to-agent interactions  </li> </ul>"},{"location":"RIS_Integration_Guide/#74-multi-agent-systems","title":"7.4 Multi-Agent Systems","text":"<ul> <li>evaluate cross-agent interference  </li> <li>analyze emergent drift patterns  </li> <li>ensure bounded reasoning domains  </li> </ul>"},{"location":"RIS_Integration_Guide/#75-retrieval-augmented-systems-rag","title":"7.5 Retrieval-Augmented Systems (RAG)","text":"<ul> <li>test reasoning stability with varied retrieval sets  </li> <li>detect hallucination or drift caused by inconsistent retrieval results  </li> <li>ensure boundary compliance for external knowledge  </li> </ul>"},{"location":"RIS_Integration_Guide/#8-integration-in-regulated-environments","title":"8. Integration in Regulated Environments","text":"<p>RIS provides objective, audit-grade evidence for:</p> <ul> <li>financial services  </li> <li>healthcare  </li> <li>critical infrastructure  </li> <li>legal decision systems  </li> <li>insurance &amp; underwriting  </li> <li>public-sector or government applications  </li> </ul> <p>Organizations may include RIS evaluation reports in:</p> <ul> <li>regulatory filings  </li> <li>compliance documentation  </li> <li>internal audit cycles  </li> <li>model approval committees  </li> </ul>"},{"location":"RIS_Integration_Guide/#9-integration-with-vendor-management","title":"9. Integration With Vendor Management","text":"<p>Enterprises may require RIS compliance from vendors by requesting:</p> <ul> <li>RIS Evaluation Report  </li> <li>RIS Conformance Statement  </li> <li>supporting metrics  </li> <li>drift and variance profiles  </li> </ul> <p>Vendors MAY advertise RIS levels in procurement documentation.</p>"},{"location":"RIS_Integration_Guide/#10-integration-with-model-cards-and-documentation","title":"10. Integration With Model Cards and Documentation","text":"<p>RIS levels MAY be included in:</p> <ul> <li>model cards  </li> <li>AI service documentation  </li> <li>internal knowledge bases  </li> <li>risk registers  </li> <li>deployment approvals  </li> </ul> <p>Example field:</p> <pre><code>Reasoning Integrity Classification: RIS-3\nLast Evaluation: 2025-01\n</code></pre>"},{"location":"RIS_Integration_Guide/#11-integration-checklist","title":"11. Integration Checklist","text":"<p>Organizations SHOULD verify the following:</p> <ul> <li>prompt sets prepared  </li> <li>evaluation scripts versioned  </li> <li>drift and variance thresholds set  </li> <li>evaluation environment consistent  </li> <li>all required metrics computed  </li> <li>boundary and interference tests executed  </li> <li>Evaluation Report completed  </li> <li>Conformance Statement completed  </li> <li>RIS level documented  </li> <li>reassessment schedule defined  </li> </ul>"},{"location":"RIS_Integration_Guide/#12-closing-notes","title":"12. Closing Notes","text":"<p>Organizations may adopt RIS at any maturity level:</p> <ul> <li>minimal annual evaluation  </li> <li>integrated governance workflow  </li> <li>automated CI/CD integration  </li> <li>full LCAC-based implementation  </li> </ul> <p>RIS is model-agnostic, environment-agnostic, and vendor-neutral.</p> <p>This guide is intended to help organizations operationalize RIS efficiently and consistently.</p>"},{"location":"RIS_Model_Card_Integration/","title":"RIS Model Card Integration Template","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Model_Card_Integration/#1-model-identification","title":"1. Model Identification","text":"<p>Model Name: Version / Build: Release Date: Owner / Maintainer: Contact: </p>"},{"location":"RIS_Model_Card_Integration/#2-model-overview","title":"2. Model Overview","text":"<p>Provide a description of the model\u2019s purpose, architecture, and intended use cases.</p> <p>2.1 Description (General model capabilities)</p> <p>2.2 Intended Use Cases (supported domains, acceptable applications)</p> <p>2.3 Out-of-Scope Use Cases (applications explicitly not supported)</p> <p>2.4 System Dependencies (agents, tools, retrieval systems, memory systems, orchestration frameworks)</p>"},{"location":"RIS_Model_Card_Integration/#3-ris-classification-summary","title":"3. RIS Classification Summary","text":"<p>This section presents the model\u2019s current reasoning integrity classification.</p> <p>RIS Level Assigned: (RIS-0, RIS-1, RIS-2, RIS-3, or RIS-4)</p> <p>Date of Last RIS Evaluation: Evaluation Performed By: Evaluation Valid Until: </p> <p>Composite Reasoning Integrity Score: (weighted average based on RIS metrics)</p> <p>Conformance Statement Location: (link or reference to the completed RIS Conformance Statement)</p> <p>Evaluation Report Location: (link or reference to the RIS Evaluation Report)</p>"},{"location":"RIS_Model_Card_Integration/#4-ris-metrics-summary","title":"4. RIS Metrics Summary","text":"<p>Summaries of required RIS metrics:</p>"},{"location":"RIS_Model_Card_Integration/#41-chain-stability","title":"4.1 Chain Stability","text":"<p>(mean score, variance, lowest/highest stability values)</p>"},{"location":"RIS_Model_Card_Integration/#42-semantic-coherence","title":"4.2 Semantic Coherence","text":"<p>(mean score, outliers, contradiction detection notes)</p>"},{"location":"RIS_Model_Card_Integration/#43-drift-sensitivity","title":"4.3 Drift Sensitivity","text":"<p>(drift score, patterns over time)</p>"},{"location":"RIS_Model_Card_Integration/#44-variance-envelope-compliance","title":"4.4 Variance Envelope Compliance","text":"<p>(percentage of samples within envelope)</p>"},{"location":"RIS_Model_Card_Integration/#45-governance-boundary-adherence","title":"4.5 Governance Boundary Adherence","text":"<p>(boundary violations observed, if any)</p>"},{"location":"RIS_Model_Card_Integration/#5-ris-risk-profile","title":"5. RIS Risk Profile","text":"<p>A brief assessment aligned with RIS Section 8 risk models.</p> <p>5.1 Structural Drift Risk: (Low / Moderate / High)</p> <p>5.2 Semantic Drift Risk: (Low / Moderate / High)</p> <p>5.3 Temporal Stability Risk: (Low / Moderate / High)</p> <p>5.4 Interference Risk: (Low / Moderate / High)</p> <p>5.5 Boundary Violation Risk: (Low / Moderate / High)</p> <p>5.6 Degradation Risk: (Low / Moderate / High)</p>"},{"location":"RIS_Model_Card_Integration/#6-evaluation-environment","title":"6. Evaluation Environment","text":"<p>Document evaluation conditions used to compute RIS metrics.</p> <p>6.1 Inference Parameters: (temperature, top-p, penalties, context window, system prompts)</p> <p>6.2 Model Runtime Environment: (hardware, GPUs, cloud environment, framework versions)</p> <p>6.3 Operational State: (single-turn vs. multi-turn, memory usage, tools enabled/disabled)</p>"},{"location":"RIS_Model_Card_Integration/#7-governance-integration","title":"7. Governance Integration","text":"<p>Document how RIS fits into governance processes.</p> <p>7.1 Deployment Requirements: (e.g., \u201cModel must be RIS-3 or higher for production use\u201d)</p> <p>7.2 Monitoring Requirements: (drift monitoring, variance checks, reevaluation frequency)</p> <p>7.3 Approval Workflow: (model review board criteria, internal audit requirements)</p> <p>7.4 Retention of Evidence: (location of drift logs, baseline datasets, ledger records)</p>"},{"location":"RIS_Model_Card_Integration/#8-known-limitations","title":"8. Known Limitations","text":"<p>Document reasoning behavior limitations, such as:</p> <ul> <li>unstable reasoning under certain prompt patterns  </li> <li>sensitivity to specific perturbations  </li> <li>high drift in multi-turn workflows  </li> <li>boundary sensitivity when tool calls change  </li> <li>inconsistent RAG dependence  </li> </ul> <p>Include mitigation strategies if applicable.</p>"},{"location":"RIS_Model_Card_Integration/#9-reassessment-and-update-schedule","title":"9. Reassessment and Update Schedule","text":"<p>Specify the organization\u2019s planned evaluation cadence.</p> <p>Examples:</p> <ul> <li>general models: every 12 months  </li> <li>agentic models: every 6 months  </li> <li>safety-critical systems: every 3 months  </li> <li>or upon model retraining or version change  </li> </ul>"},{"location":"RIS_Model_Card_Integration/#10-version-history","title":"10. Version History","text":"<p>Document RIS classification changes over time.</p> <p>Example:</p> <ul> <li>v1.0.0 \u2014 RIS-3 (initial evaluation)  </li> <li>v1.1.0 \u2014 RIS-3 (no significant drift)  </li> <li>v2.0.0 \u2014 RIS-4 (after retraining)  </li> </ul>"},{"location":"RIS_Model_Card_Integration/#11-appendices-optional","title":"11. Appendices (Optional)","text":"<p>Include:</p> <ul> <li>drift charts  </li> <li>stability graphs  </li> <li>raw metrics samples  </li> <li>evaluation dataset examples  </li> <li>additional disclosures  </li> </ul>"},{"location":"RIS_Model_Card_Integration/#end-of-model-card-integration-template","title":"End of Model Card Integration Template","text":""},{"location":"RIS_Overview/","title":"Reasoning Integrity Standard (RIS) \u2014 Overview","text":"<p>Version 1.0 Published by Atom Labs \u00a9 2025 Atom Labs. All Rights Reserved.</p>"},{"location":"RIS_Overview/#1-purpose-of-ris","title":"1. Purpose of RIS","text":"<p>The Reasoning Integrity Standard (RIS) defines a formal, measurable framework for evaluating the stability, predictability, and structural reliability of reasoning performed by large language models (LLMs), autonomous agents, and multi-model cognitive systems.</p> <p>RIS does not measure correctness or factual accuracy. Instead, it measures the integrity and consistency of the reasoning process itself.</p> <p>The purpose of RIS is to:</p> <ul> <li>provide organizations with a validated method for assessing reasoning behavior  </li> <li>reduce operational risk associated with unstable or unpredictable AI systems  </li> <li>support governance programs, audits, and regulatory compliance  </li> <li>enable reproducible, evidence-based evaluation of reasoning  </li> <li>establish a shared technical language for reasoning integrity across the industry  </li> </ul>"},{"location":"RIS_Overview/#2-why-ris-matters","title":"2. Why RIS Matters","text":"<p>Modern AI systems rely heavily on internal reasoning behaviors that are:</p> <ul> <li>opaque  </li> <li>non-deterministic  </li> <li>sensitive to drift  </li> <li>dependent on context and tool interactions  </li> <li>variable across repeated evaluations  </li> </ul> <p>Without a standard for measuring reasoning stability, enterprises face risk in:</p> <ul> <li>safety-critical applications  </li> <li>financial or legal decision-making  </li> <li>automation systems  </li> <li>multi-agent environments  </li> <li>regulated industries  </li> <li>long-running or memory-dependent workflows  </li> </ul> <p>RIS provides the first formal standard addressing these risks.</p>"},{"location":"RIS_Overview/#3-what-ris-provides","title":"3. What RIS Provides","text":"<p>RIS v1.0 introduces:</p> <ul> <li>a structured, multi-level reasoning integrity classification (RIS-0 through RIS-4)  </li> <li>a measurement framework for chain stability, semantic coherence, drift, variance, and boundaries  </li> <li>a set of mandatory controls governing reasoning behavior  </li> <li>a standardized evaluation methodology  </li> <li>audit guidelines and evidence requirements  </li> <li>risk models specific to reasoning integrity  </li> <li>a reference implementation (LCAC)  </li> <li>templates for conformance and evaluation reporting  </li> </ul> <p>The result is a complete, operationally useful standard suitable for enterprise deployment.</p>"},{"location":"RIS_Overview/#4-who-ris-is-for","title":"4. Who RIS Is For","text":"<p>RIS is intended for:</p> <ul> <li>enterprises deploying LLM-driven systems  </li> <li>AI safety and governance teams  </li> <li>risk, audit, and compliance programs  </li> <li>developers building agentic and multi-model systems  </li> <li>researchers benchmarking reasoning behavior  </li> <li>organizations operating in regulated or safety-critical environments  </li> </ul> <p>Any system where reasoning integrity impacts safety, trust, or predictability benefits from RIS.</p>"},{"location":"RIS_Overview/#5-ris-levels","title":"5. RIS Levels","text":"<p>RIS defines five levels of reasoning maturity:</p> <ul> <li>RIS-0: Uncontrolled reasoning  </li> <li>RIS-1: Drift-sensitive reasoning  </li> <li>RIS-2: Semi-stable reasoning  </li> <li>RIS-3: Controlled reasoning (production-grade)  </li> <li>RIS-4: High-integrity reasoning (audit-ready, safety-critical)  </li> </ul> <p>Each level has defined:</p> <ul> <li>eligibility thresholds  </li> <li>mandatory controls  </li> <li>evidence requirements  </li> <li>risk profiles  </li> <li>evaluation expectations  </li> </ul>"},{"location":"RIS_Overview/#6-what-ris-evaluates","title":"6. What RIS Evaluates","text":"<p>RIS assesses five core categories:</p> <ol> <li>Chain Stability  </li> <li>Semantic Coherence  </li> <li>Drift Sensitivity  </li> <li>Variance Envelope Compliance  </li> <li>Governance Boundary Adherence  </li> </ol> <p>These measurements determine whether a system\u2019s reasoning behavior is stable enough for operational or regulated use.</p>"},{"location":"RIS_Overview/#7-relationship-to-other-standards","title":"7. Relationship to Other Standards","text":"<p>RIS complements but does not replace:</p> <ul> <li>NIST SP 800-53  </li> <li>ISO/IEC 27001  </li> <li>SOC 2  </li> <li>OWASP ASVS  </li> <li>NIST AI RMF  </li> <li>EU AI Act  </li> </ul> <p>Existing frameworks address security, privacy, and governance. RIS adds the missing layer: reasoning integrity.</p>"},{"location":"RIS_Overview/#8-reference-implementation-informative","title":"8. Reference Implementation (Informative)","text":"<p>The Least-Context Access Control (LCAC) framework is included as a reference implementation demonstrating how RIS controls and metrics may be operationalized.</p> <p>Use of LCAC is optional. RIS is implementation-agnostic and model-agnostic.</p>"},{"location":"RIS_Overview/#9-deliverables-included-in-ris-v10","title":"9. Deliverables Included in RIS v1.0","text":"<p>The RIS Standard Package consists of:</p> <ul> <li>Reasoning Integrity Standard (full specification, sections 0\u201313)  </li> <li>RIS Overview (this document)  </li> <li>RIS Conformance Statement Template  </li> <li>RIS Evaluation Report Template  </li> <li>RIS Benchmark Guide  </li> <li>RIS Glossary  </li> <li>RIS Change Log  </li> <li>Reference implementation notes (LCAC)  </li> <li>Website-ready structure (mkdocs)  </li> </ul>"},{"location":"RIS_Overview/#10-using-the-ris-standard","title":"10. Using the RIS Standard","text":"<p>Organizations may use RIS to:</p> <ul> <li>classify AI systems by reasoning integrity level  </li> <li>evaluate systems before deployment  </li> <li>monitor reasoning behavior in production  </li> <li>support audit and compliance programs  </li> <li>meet regulatory expectations for transparency  </li> <li>compare models or vendors using standardized metrics  </li> </ul> <p>RIS classification may be referenced in internal documentation, audit reports, vendor assessments, risk registries, and regulatory submissions.</p>"},{"location":"RIS_Overview/#11-citation","title":"11. Citation","text":"<p>Organizations citing RIS should use:</p> <p>Reasoning Integrity Standard (RIS) v1.0 Atom Labs, 2025 https://github.com/"},{"location":"RIS_Overview/#12-contact","title":"12. Contact","text":"<p>For feedback or contributions, please contact: Atom Labs Standards Division Email: qstackfield@seedcore.io</p>"},{"location":"RIS_Portal_API_Blueprint/","title":"RIS Public Portal API Blueprint","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Portal_API_Blueprint/#1-purpose","title":"1. Purpose","text":"<p>This document defines a reference API blueprint for a RIS public portal.</p> <p>The portal API is responsible for:</p> <ul> <li>listing RIS-certified systems</li> <li>returning certification entries</li> <li>returning scorecards</li> <li>exposing metadata for UI components</li> </ul> <p>This blueprint is implementation-agnostic. Organizations MAY implement any subset of these endpoints.</p>"},{"location":"RIS_Portal_API_Blueprint/#2-base-url","title":"2. Base URL","text":"<p>Examples:</p> <ul> <li>https://ris.atomlabs.app/api</li> <li>https://ris.example.com/api</li> </ul>"},{"location":"RIS_Portal_API_Blueprint/#3-endpoints-overview","title":"3. Endpoints Overview","text":"<p>Recommended endpoints:</p> <ul> <li>GET /directory</li> <li>GET /certifications/{id}</li> <li>GET /scorecards/{id}</li> <li>GET /organizations</li> <li>GET /stats</li> </ul> <p>Optional:</p> <ul> <li>GET /levels</li> <li>GET /tags</li> <li>GET /search</li> </ul>"},{"location":"RIS_Portal_API_Blueprint/#4-get-directory","title":"4. GET /directory","text":"<p>Returns a paginated RIS directory.</p> <p>Query parameters:</p> <ul> <li>page (integer, default 1)</li> <li>page_size (integer, default 50)</li> <li>organization</li> <li>category</li> <li>ris_level_min</li> <li>ris_level_max</li> <li>status</li> <li>tag</li> <li>search</li> </ul> <p>Response body follows the RIS Directory Format.</p>"},{"location":"RIS_Portal_API_Blueprint/#5-get-certificationsid","title":"5. GET /certifications/{id}","text":"<p>Returns a RIS Certification Entry.</p> <p>Path parameter:</p> <ul> <li>id: certification entry ID (e.g., \u201cCERT-2025-0001\u201d)</li> </ul> <p>Response:</p> <ul> <li>200: certification entry JSON</li> <li>404: not found</li> </ul>"},{"location":"RIS_Portal_API_Blueprint/#6-get-scorecardsid","title":"6. GET /scorecards/{id}","text":"<p>Returns the full RIS Scorecard for a given certification.</p> <p>Path parameter:</p> <ul> <li>id: certification entry ID or evaluation ID</li> </ul> <p>Response:</p> <ul> <li>200: RIS Scorecard JSON</li> <li>404: not found</li> </ul>"},{"location":"RIS_Portal_API_Blueprint/#7-get-organizations","title":"7. GET /organizations","text":"<p>Returns a list of organizations participating in RIS.</p> <p>Response example:</p> <pre><code>{\n  \"organizations\": [\n    {\n      \"name\": \"ExampleCorp\",\n      \"slug\": \"examplecorp\",\n      \"total_certifications\": 5,\n      \"highest_ris_level\": \"RIS-4\"\n    }\n  ]\n}\n</code></pre>"},{"location":"RIS_Portal_API_Blueprint/#8-get-stats","title":"8. GET /stats","text":"<p>Returns high-level portal statistics.</p> <p>Example:</p> <pre><code>{\n  \"total_certifications\": 245,\n  \"by_ris_level\": {\n    \"RIS-0\": 4,\n    \"RIS-1\": 21,\n    \"RIS-2\": 80,\n    \"RIS-3\": 110,\n    \"RIS-4\": 30\n  },\n  \"by_category\": {\n    \"LLM\": 90,\n    \"agent\": 100,\n    \"RAG\": 40,\n    \"multi-agent\": 15\n  }\n}\n</code></pre>"},{"location":"RIS_Portal_API_Blueprint/#9-optional-get-search","title":"9. Optional: GET /search","text":"<p>Search endpoint using free-text query parameter.</p> <ul> <li>q: search string</li> </ul> <p>Returns lightweight certification entries or directory-like responses.</p>"},{"location":"RIS_Portal_API_Blueprint/#10-security-considerations","title":"10. Security Considerations","text":"<p>Public portal APIs SHOULD:</p> <ul> <li>be read-only</li> <li>avoid exposing sensitive details</li> <li>rate-limit abusive usage</li> <li>support HTTPS only</li> </ul> <p>Write operations (e.g., submitting evaluations) SHOULD be handled by separate, authenticated APIs.</p>"},{"location":"RIS_Portal_Layout_Spec/","title":"RIS Dynamic Portal Layout Specification","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Portal_Layout_Spec/#1-purpose","title":"1. Purpose","text":"<p>This document defines a recommended layout for a dynamic RIS portal hosted at:</p> <pre><code>https://ris.atomlabs.app\n</code></pre> <p>The portal is intended to:</p> <ul> <li>present the Reasoning Integrity Standard</li> <li>showcase certified systems</li> <li>act as an authoritative reference for RIS levels and scorecards</li> </ul>"},{"location":"RIS_Portal_Layout_Spec/#2-high-level-pages","title":"2. High-Level Pages","text":"<p>Suggested primary pages:</p> <ul> <li>Home</li> <li>Directory</li> <li>Certification Detail</li> <li>Scorecard View</li> <li>About / Standard</li> <li>Resources (Guides, Templates)</li> <li>Governance / Program</li> </ul>"},{"location":"RIS_Portal_Layout_Spec/#3-home-page","title":"3. Home Page","text":"<p>Elements:</p> <ul> <li>brief description of RIS</li> <li>current version (v1.0)</li> <li>call-to-action buttons:</li> <li>View Standard</li> <li>Browse Certified Systems</li> <li>Download Templates</li> <li>high-level stats:</li> <li>total certifications</li> <li>distribution by RIS level</li> <li>selected featured systems (optional)</li> </ul>"},{"location":"RIS_Portal_Layout_Spec/#4-directory-page","title":"4. Directory Page","text":"<p>Purpose:</p> <ul> <li>display a searchable, filterable table of RIS Certification Entries</li> </ul> <p>Key components:</p> <ul> <li>filters:</li> <li>organization</li> <li>RIS level</li> <li>category</li> <li>status</li> <li>tags</li> <li>search bar:</li> <li>free-text across system name, organization, tags</li> <li>table columns:</li> <li>system name</li> <li>organization</li> <li>category</li> <li>RIS level</li> <li>composite score</li> <li>status</li> <li>valid until</li> </ul> <p>Rows link to Certification Detail page.</p>"},{"location":"RIS_Portal_Layout_Spec/#5-certification-detail-page","title":"5. Certification Detail Page","text":"<p>Purpose:</p> <ul> <li>human-readable view of a single certification entry</li> </ul> <p>Sections:</p> <ul> <li>system overview</li> <li>RIS classification</li> <li>risk profile</li> <li>validity period</li> <li>links:</li> <li>model card</li> <li>scorecard JSON</li> <li>conformance statement</li> <li>evaluation report</li> </ul> <p>Optional:</p> <ul> <li>mini-charts for metrics (stability, coherence, drift, variance)</li> </ul>"},{"location":"RIS_Portal_Layout_Spec/#6-scorecard-view","title":"6. Scorecard View","text":"<p>Purpose:</p> <ul> <li>present RIS Scorecard in structured, readable format</li> </ul> <p>Sections:</p> <ul> <li>system details</li> <li>evaluation details</li> <li>metrics</li> <li>RIS outcomes</li> <li>risk profile</li> <li>evidence references</li> </ul> <p>Offer:</p> <ul> <li>\u201cDownload JSON\u201d</li> <li>\u201cDownload PDF\u201d (optional)</li> </ul>"},{"location":"RIS_Portal_Layout_Spec/#7-about-standard-page","title":"7. About / Standard Page","text":"<p>Sections:</p> <ul> <li>what RIS is</li> <li>why it exists</li> <li>who maintains it</li> <li>link to:</li> <li>full spec</li> <li>overview</li> <li>change log</li> <li>glossary</li> </ul> <p>This page SHOULD be static and updated only when RIS versions change.</p>"},{"location":"RIS_Portal_Layout_Spec/#8-resources-page","title":"8. Resources Page","text":"<p>Content:</p> <ul> <li>links to guides:</li> <li>Quickstart</li> <li>Benchmark Guide</li> <li>Integration Guide</li> <li>SDK Guide</li> <li>Evaluation Report Template</li> <li>Conformance Statement Template</li> <li>links to example datasets</li> </ul>"},{"location":"RIS_Portal_Layout_Spec/#9-governance-program-page","title":"9. Governance / Program Page","text":"<p>Content:</p> <ul> <li>description of RIS certification program</li> <li>levels and requirements</li> <li>assessment and reassessment cycles</li> <li>how to participate</li> <li>contact information</li> </ul> <p>Over time, this may evolve into a full program description for enterprises and regulators.</p>"},{"location":"RIS_Portal_Layout_Spec/#10-ux-principles","title":"10. UX Principles","text":"<p>Recommended design principles:</p> <ul> <li>clarity over marketing</li> <li>emphasize auditability and transparency</li> <li>minimize branding noise</li> <li>highlight RIS level and risk information</li> <li>accessible and responsive design</li> </ul>"},{"location":"RIS_Portal_Layout_Spec/#11-integration-with-apis","title":"11. Integration With APIs","text":"<p>Dynamic content SHOULD be driven by:</p> <ul> <li>the RIS Directory API</li> <li>certification entry API</li> <li>scorecard retrieval API</li> <li>stats API</li> </ul> <p>This separates presentation (UI) from data (backend).</p>"},{"location":"RIS_Quickstart/","title":"RIS Quickstart Guide","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Quickstart/#1-introduction","title":"1. Introduction","text":"<p>This Quickstart Guide explains how to begin using the Reasoning Integrity Standard (RIS) in under 10 minutes. It is intended for organizations, researchers, and engineering teams who need a simple, high-level process for applying RIS to an LLM, agent, or AI system.</p> <p>This guide does not replace the full standard. It provides a fast-path workflow for initial adoption.</p>"},{"location":"RIS_Quickstart/#2-basic-requirements","title":"2. Basic Requirements","text":"<p>To perform a RIS evaluation you will need:</p> <ul> <li>access to the LLM or agent system under evaluation  </li> <li>ability to run repeated inference with fixed parameters  </li> <li>ability to save output samples  </li> <li>basic scripting or data analysis tools (Python recommended)  </li> <li>the RIS evaluation templates (included in this repository)  </li> </ul> <p>No special infrastructure is required.</p>"},{"location":"RIS_Quickstart/#3-quickstart-workflow-summary","title":"3. Quickstart Workflow Summary","text":"<p>RIS evaluation involves the following steps:</p> <ol> <li>Prepare evaluation environment  </li> <li>Build prompt sets  </li> <li>Run repeated sampling  </li> <li>Run perturbation sampling  </li> <li>Compute metrics  </li> <li>Identify drift, variance, and boundary results  </li> <li>Complete the Evaluation Report  </li> <li>Complete the Conformance Statement  </li> <li>Assign a RIS level  </li> </ol> <p>Each step is described below.</p>"},{"location":"RIS_Quickstart/#4-step-by-step-instructions","title":"4. Step-by-Step Instructions","text":""},{"location":"RIS_Quickstart/#step-1-prepare-evaluation-environment","title":"Step 1: Prepare Evaluation Environment","text":"<ul> <li>Freeze inference parameters (temperature, top-p, etc.).  </li> <li>Disable adaptive memory or persistent state.  </li> <li>Fix system instructions or agent configuration.  </li> <li>Document all environment details.</li> </ul> <p>All evaluations MUST be performed with identical settings.</p>"},{"location":"RIS_Quickstart/#step-2-build-baseline-prompt-set","title":"Step 2: Build Baseline Prompt Set","text":"<p>Select:</p> <ul> <li>at least 50 baseline prompts (general-purpose models)  </li> <li>at least 20 prompts (domain-specific models)</li> </ul> <p>Prompts should cover:</p> <ul> <li>reasoning  </li> <li>multi-step tasks  </li> <li>analytical queries  </li> <li>comparative questions  </li> <li>conceptual understanding  </li> </ul> <p>Example:</p> <pre><code>\"Explain the impact of market consolidation on competition.\"\n</code></pre>"},{"location":"RIS_Quickstart/#step-3-build-perturbation-prompt-set","title":"Step 3: Build Perturbation Prompt Set","text":"<p>For each baseline prompt selected for perturbation:</p> <ul> <li>create at least 10 semantically equivalent variations  </li> </ul> <p>Variations may include:</p> <ul> <li>synonym substitutions  </li> <li>reordering clauses  </li> <li>minor phrasal changes  </li> </ul> <p>Example:</p> <pre><code>Baseline: \"Explain how supply shocks affect inflation.\"\nPerturbation: \"Describe how supply disruptions influence inflation.\"\n</code></pre>"},{"location":"RIS_Quickstart/#step-4-run-repeated-inference-sampling","title":"Step 4: Run Repeated Inference Sampling","text":"<p>For each baseline prompt:</p> <ul> <li>generate at least 25 repeated samples  </li> <li>keep all parameters identical  </li> <li>store outputs in structured format (JSON, CSV, etc.)</li> </ul> <p>These samples support:</p> <ul> <li>chain stability measurement  </li> <li>semantic coherence  </li> <li>drift sensitivity  </li> <li>variance compliance  </li> </ul>"},{"location":"RIS_Quickstart/#step-5-run-perturbation-sampling","title":"Step 5: Run Perturbation Sampling","text":"<p>For each perturbation prompt:</p> <ul> <li>generate at least 10 samples  </li> <li>use the same parameters as baseline sampling  </li> </ul> <p>This evaluates robustness and drift under controlled variation.</p>"},{"location":"RIS_Quickstart/#step-6-compute-ris-metrics","title":"Step 6: Compute RIS Metrics","text":"<p>The following metrics MUST be calculated:</p> <ul> <li>Chain Stability  </li> <li>Semantic Coherence  </li> <li>Drift Sensitivity  </li> <li>Variance Envelope Compliance  </li> <li>Governance Boundary Adherence  </li> </ul> <p>Metric definitions are located in RIS Section 5. You may compute metrics using:</p> <ul> <li>Python statistical analysis  </li> <li>embedding-based similarity  </li> <li>JSON-based analysis scripts  </li> <li>internal or external evaluation tools  </li> </ul>"},{"location":"RIS_Quickstart/#step-7-analyze-drift-variance-and-boundary-behavior","title":"Step 7: Analyze Drift, Variance, and Boundary Behavior","text":"<p>Inspect results for:</p> <ul> <li>structural or semantic divergence  </li> <li>temporal degradation  </li> <li>boundary violations (use of unauthorized context or domain)  </li> <li>interference-based instability (tool, RAG, or agent effects)</li> </ul> <p>Document all findings for the final report.</p>"},{"location":"RIS_Quickstart/#step-8-complete-the-ris-evaluation-report","title":"Step 8: Complete the RIS Evaluation Report","text":"<p>Use:</p> <p><code>RIS_Evaluation_Report_Template.md</code></p> <p>Document:</p> <ul> <li>metrics  </li> <li>drift and variance results  </li> <li>boundary adherence  </li> <li>violations  </li> <li>environment details  </li> <li>sampling methodology  </li> </ul> <p>This becomes the official audit record.</p>"},{"location":"RIS_Quickstart/#step-9-complete-the-ris-conformance-statement","title":"Step 9: Complete the RIS Conformance Statement","text":"<p>Use:</p> <p><code>RIS_Conformance_Statement.md</code></p> <p>Document:</p> <ul> <li>composite score  </li> <li>control compliance  </li> <li>evidence summary  </li> <li>final RIS level assigned  </li> </ul> <p>This becomes the organization\u2019s self-attestation or audit deliverable.</p>"},{"location":"RIS_Quickstart/#5-assigning-a-ris-level","title":"5. Assigning a RIS Level","text":"<p>Use the rules in RIS Section 7:</p> <ul> <li>composite score threshold  </li> <li>mandatory control compliance  </li> <li>absence of high-severity risks  </li> <li>documented evidence  </li> </ul> <p>The final RIS level MUST be the lower of:</p> <ul> <li>level qualified by score  </li> <li>level qualified by control compliance  </li> </ul>"},{"location":"RIS_Quickstart/#6-next-steps","title":"6. Next Steps","text":"<p>After initial evaluation:</p> <ul> <li>schedule periodic reassessment  </li> <li>integrate RIS controls into development workflows  </li> <li>build internal governance procedures  </li> <li>optionally integrate LCAC or other tooling to simplify evaluation  </li> </ul>"},{"location":"RIS_Quickstart/#7-additional-resources","title":"7. Additional Resources","text":"<p>Included in this repository:</p> <ul> <li>RIS Specification (full)  </li> <li>Benchmark Guide  </li> <li>Evaluation Report Template  </li> <li>Conformance Statement Template  </li> <li>Glossary  </li> <li>Change Log  </li> </ul> <p>For questions or contributions: Atom Labs Standards Division RIS@atomlabs.app</p>"},{"location":"RIS_Reference_Evaluator_Guide/","title":"RIS Reference Evaluator Guide","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Reference_Evaluator_Guide/#1-introduction","title":"1. Introduction","text":"<p>This guide explains how to structure a RIS Reference Evaluator. A RIS Reference Evaluator is a system that:</p> <ul> <li>accepts inference samples  </li> <li>computes RIS metrics  </li> <li>determines RIS level  </li> <li>logs results  </li> <li>returns a standardized output format</li> </ul> <p>LCAC is an example of a reference evaluator, but RIS is implementation-agnostic.</p>"},{"location":"RIS_Reference_Evaluator_Guide/#2-evaluator-architecture","title":"2. Evaluator Architecture","text":"<p>A reference evaluator typically includes:</p> <ul> <li>input ingestion (samples)</li> <li>metric computation engine</li> <li>variance envelope logic</li> <li>drift analysis module</li> <li>boundary violation checks</li> <li>composite scoring module</li> <li>evaluator output formatter</li> <li>optional ledger storage</li> </ul> <p>These components MAY be implemented as:</p> <ul> <li>a Python library  </li> <li>a REST API  </li> <li>a CLI tool  </li> <li>a local script  </li> <li>a microservice  </li> </ul>"},{"location":"RIS_Reference_Evaluator_Guide/#3-inputs","title":"3. Inputs","text":"<p>The evaluator MUST accept a list of samples:</p> <pre><code>[\n  { \"prompt\": \"...\", \"output\": \"...\" },\n  { \"prompt\": \"...\", \"output\": \"...\" }\n]\n</code></pre> <p>Optional fields:</p> <ul> <li>intermediate steps (if available)</li> <li>tool outputs (for agent systems)</li> <li>context metadata</li> </ul>"},{"location":"RIS_Reference_Evaluator_Guide/#4-outputs","title":"4. Outputs","text":"<p>A RIS evaluator SHOULD return:</p> <pre><code>{\n  \"metrics\": {},\n  \"composite_score\": &lt;float&gt;,\n  \"ris_level\": \"RIS-x\",\n  \"violations\": [],\n  \"summary\": \"text summary\"\n}\n</code></pre>"},{"location":"RIS_Reference_Evaluator_Guide/#5-evaluator-execution-steps","title":"5. Evaluator Execution Steps","text":"<p>A RIS evaluator follows this workflow:</p>"},{"location":"RIS_Reference_Evaluator_Guide/#step-1-load-samples","title":"Step 1: Load samples","text":""},{"location":"RIS_Reference_Evaluator_Guide/#step-2-compute-metrics","title":"Step 2: Compute metrics","text":""},{"location":"RIS_Reference_Evaluator_Guide/#step-3-compute-composite-score","title":"Step 3: Compute composite score","text":""},{"location":"RIS_Reference_Evaluator_Guide/#step-4-determine-ris-level","title":"Step 4: Determine RIS level","text":""},{"location":"RIS_Reference_Evaluator_Guide/#step-5-generate-summary","title":"Step 5: Generate summary","text":""},{"location":"RIS_Reference_Evaluator_Guide/#step-6-log-results-optional","title":"Step 6: Log results (optional)","text":""},{"location":"RIS_Reference_Evaluator_Guide/#step-7-return-response","title":"Step 7: Return response","text":""},{"location":"RIS_Reference_Evaluator_Guide/#6-logging-optional-but-recommended","title":"6. Logging (Optional but Recommended)","text":"<p>Evaluators MAY log:</p> <ul> <li>metric values  </li> <li>drift events  </li> <li>variance envelope breaches  </li> <li>boundary violations  </li> <li>scores over time  </li> </ul> <p>Logging improves:</p> <ul> <li>auditability  </li> <li>reproducibility  </li> <li>long-term monitoring  </li> </ul>"},{"location":"RIS_Reference_Evaluator_Guide/#7-integration-with-lcac-optional","title":"7. Integration With LCAC (Optional)","text":"<p>LCAC provides:</p> <ul> <li>trust score  </li> <li>drift model  </li> <li>ledger  </li> <li>variance tracking  </li> <li>governance modes</li> </ul> <p>Organizations may choose to use LCAC as:</p> <ul> <li>a full evaluator  </li> <li>a partial evaluator  </li> <li>an evaluation engine plugin  </li> </ul> <p>RIS does not require LCAC.</p>"},{"location":"RIS_Reference_Evaluator_Guide/#8-extensibility","title":"8. Extensibility","text":"<p>Evaluators MAY support:</p> <ul> <li>plugin metric modules  </li> <li>domain-specific RIS profiles  </li> <li>multi-agent scoring  </li> <li>scenario-based testing  </li> <li>RAG stability analysis  </li> </ul> <p>These features are optional but powerful.</p>"},{"location":"RIS_Reference_Evaluator_Guide/#9-summary","title":"9. Summary","text":"<p>A RIS Reference Evaluator enables:</p> <ul> <li>reproducible reasoning integrity assessments  </li> <li>consistent RIS scoring  </li> <li>integration with CI/CD pipelines  </li> <li>integration with governance frameworks  </li> </ul> <p>LCAC is one example, but any evaluator that follows RIS rules is compliant.</p>"},{"location":"RIS_Release_Checklist/","title":"RIS Release Checklist","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Release_Checklist/#1-purpose","title":"1. Purpose","text":"<p>This checklist ensures that all required steps have been completed before publishing a new version of the Reasoning Integrity Standard (RIS). It applies to:</p> <ul> <li>initial releases  </li> <li>minor revisions  </li> <li>patch releases  </li> <li>major revisions  </li> </ul>"},{"location":"RIS_Release_Checklist/#2-pre-release-verification","title":"2. Pre-Release Verification","text":""},{"location":"RIS_Release_Checklist/#21-specification-completeness","title":"2.1 Specification Completeness","text":"<p>Confirm the following files are present and finalized:</p> <ul> <li>All 14 specification sections (00\u201313)  </li> <li>Supporting documents  </li> <li>Schemas  </li> <li>Guides  </li> <li>Change Log  </li> </ul> <p>Checklist:</p> <ul> <li>[ ] All spec sections validated  </li> <li>[ ] Glossary updated  </li> <li>[ ] Change Log updated  </li> <li>[ ] Version identifiers updated across all docs  </li> </ul>"},{"location":"RIS_Release_Checklist/#3-technical-validation-documentation-only","title":"3. Technical Validation (Documentation Only)","text":"<p>(no server required)</p> <ul> <li>[ ] mkdocs.yml validated  </li> <li>[ ] mkdocs <code>nav:</code> references correct  </li> <li>[ ] Local build test: <code>mkdocs serve</code> </li> <li>[ ] All docs render correctly  </li> <li>[ ] No dead internal links  </li> <li>[ ] No formatting breaks (lists, headers, code blocks)  </li> <li>[ ] All fenced code blocks compile correctly  </li> <li>[ ] No smart quotes or illegal Unicode characters  </li> </ul>"},{"location":"RIS_Release_Checklist/#4-content-review","title":"4. Content Review","text":"<ul> <li>[ ] Consistent terminology (matches glossary)  </li> <li>[ ] Control language aligns with normative definitions (MUST/SHALL/SHOULD/MAY)  </li> <li>[ ] Risk model descriptions consistent  </li> <li>[ ] Metrics definitions consistent  </li> <li>[ ] Evaluation methodology verified  </li> <li>[ ] Reference implementation notes updated  </li> <li>[ ] No section conflicts or circular references  </li> <li>[ ] All diagrams (if included later) match spec text  </li> </ul>"},{"location":"RIS_Release_Checklist/#5-structural-integrity","title":"5. Structural Integrity","text":"<ul> <li>[ ] All files in <code>/docs</code> root are top-level supporting documents  </li> <li>[ ] All spec documents live in <code>/docs/sections/</code> </li> <li>[ ] Directory format matches RIS Standard Index  </li> <li>[ ] Repository organized cleanly for public readers  </li> </ul>"},{"location":"RIS_Release_Checklist/#6-versioning","title":"6. Versioning","text":"<ul> <li>[ ] Change Log updated  </li> <li>[ ] Version string updated in all docs  </li> <li>[ ] Release manifest generated (RIS_Release_Manifest.md)  </li> <li>[ ] Tag created for release (e.g., <code>v1.0</code>)  </li> </ul>"},{"location":"RIS_Release_Checklist/#7-publishing-github-pages-or-custom-domain","title":"7. Publishing (GitHub Pages or custom domain)","text":"<p>(no server access required yet)</p> <ul> <li>[ ] Repo set to Public  </li> <li>[ ] GitHub Pages enabled  </li> <li>[ ] Build from <code>gh-pages</code> or <code>docs/</code> </li> <li>[ ] Custom domain <code>ris.atomlabs.app</code> added  </li> <li>[ ] HTTPS enforced  </li> <li>[ ] Final site verified  </li> </ul>"},{"location":"RIS_Release_Checklist/#8-post-release-documentation","title":"8. Post-Release Documentation","text":"<ul> <li>[ ] RIS citation format added to homepage  </li> <li>[ ] RIS v1.0 PDF prepared (optional)  </li> <li>[ ] Announcement draft prepared (optional)  </li> <li>[ ] v1.1 roadmap created  </li> </ul>"},{"location":"RIS_Release_Checklist/#9-approval","title":"9. Approval","text":"<p>Sign-off steps before release:</p> <ul> <li>[ ] Technical review complete  </li> <li>[ ] Editorial review complete  </li> <li>[ ] Governance approval granted  </li> <li>[ ] Final publish authorization  </li> </ul>"},{"location":"RIS_Release_Checklist/#10-version","title":"10. Version","text":"<p>Checklist applies to:</p> <p>RIS v1.0 - January 2025</p>"},{"location":"RIS_Release_Manifest/","title":"RIS v1.0 Release Manifest","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs Release Date: 2025-01-01</p>"},{"location":"RIS_Release_Manifest/#1-overview","title":"1. Overview","text":"<p>This manifest provides a complete list of all documents included in the RIS v1.0 release. It acts as the authoritative reference for:</p> <ul> <li>release validation  </li> <li>change verification  </li> <li>archival integrity  </li> <li>distribution tracking  </li> <li>future version comparisons  </li> </ul> <p>This manifest SHALL accompany all official RIS releases.</p>"},{"location":"RIS_Release_Manifest/#2-specification-documents-core-standard","title":"2. Specification Documents (Core Standard)","text":"<p>Located in: docs/sections/</p> <ul> <li>00-foreword.md  </li> <li>01-scope.md  </li> <li>02-normative-references.md  </li> <li>03-fundamental-concepts.md  </li> <li>04-ris-levels.md  </li> <li>05-measurement-framework.md  </li> <li>06-controls.md  </li> <li>07-scoring-and-conformance.md  </li> <li>08-risk-models.md  </li> <li>09-evaluation-methodology.md  </li> <li>10-audit-guidelines.md  </li> <li>11-reference-implementation.md  </li> <li>12-standard-mapping.md  </li> <li>13-appendices.md  </li> </ul>"},{"location":"RIS_Release_Manifest/#3-supporting-documents","title":"3. Supporting Documents","text":"<p>Located in: docs/</p> <ul> <li>RIS_Overview.md  </li> <li>RIS_Conformance_Statement.md  </li> <li>RIS_Evaluation_Report_Template.md  </li> <li>RIS_Benchmark_Guide.md  </li> <li>RIS_Integration_Guide.md  </li> <li>RIS_SDK_Guide.md  </li> <li>RIS_Reference_Evaluator_Guide.md  </li> <li>RIS_Example_Prompt_Dataset.md  </li> <li>RIS_API_Integration_Guide.md  </li> <li>RIS_Portal_API_Blueprint.md  </li> <li>RIS_Portal_Layout_Spec.md  </li> <li>RIS_Certification_Program.md  </li> </ul>"},{"location":"RIS_Release_Manifest/#4-schema-directory-documents","title":"4. Schema &amp; Directory Documents","text":"<ul> <li>RIS_Scorecard_Schema.md  </li> <li>RIS_Certification_Schema.md  </li> <li>RIS_Directory_Format.md  </li> </ul>"},{"location":"RIS_Release_Manifest/#5-metadata-supplementary-files","title":"5. Metadata &amp; Supplementary Files","text":"<ul> <li>RIS_Glossary.md  </li> <li>RIS_Change_Log.md  </li> <li>RIS_Standard_Index.md  </li> </ul>"},{"location":"RIS_Release_Manifest/#6-repository-structure-expected","title":"6. Repository Structure (Expected)","text":"<pre><code>docs/\n    index.md\n    RIS_Overview.md\n    ...\n    RIS_Standard_Index.md\n    sections/\n        00-foreword.md\n        ...\n        13-appendices.md\nmkdocs.yml\n</code></pre>"},{"location":"RIS_Release_Manifest/#7-version-information","title":"7. Version Information","text":"<p>Version: 1.0 Status: Initial Release Publisher: Atom Labs Release Date: 2025-01-01  </p> <p>All future versions SHALL include an updated manifest.</p>"},{"location":"RIS_SDK_Guide/","title":"RIS SDK Guide","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_SDK_Guide/#1-introduction","title":"1. Introduction","text":"<p>This guide describes how organizations can build a simple Software Development Kit (SDK) for RIS evaluation. The SDK provides utilities for:</p> <ul> <li>loading samples  </li> <li>computing RIS metrics  </li> <li>evaluating composite scores  </li> <li>determining RIS levels  </li> <li>generating summary reports  </li> </ul> <p>RIS does not mandate a specific SDK. The examples here represent a reference design that any enterprise can implement.</p>"},{"location":"RIS_SDK_Guide/#2-sdk-capabilities","title":"2. SDK Capabilities","text":"<p>A RIS SDK SHOULD provide:</p> <ul> <li>helper functions for computing metrics</li> <li>a composite scoring function</li> <li>a RIS level classification method</li> <li>IO helpers for loading/saving JSON samples</li> <li>optional integration with LCAC or an internal evaluator</li> </ul>"},{"location":"RIS_SDK_Guide/#3-suggested-sdk-functions","title":"3. Suggested SDK Functions","text":""},{"location":"RIS_SDK_Guide/#31-compute_metricssamples","title":"3.1 compute_metrics(samples)","text":"<p>Computes:</p> <ul> <li>chain stability  </li> <li>semantic coherence  </li> <li>drift sensitivity  </li> <li>variance compliance  </li> <li>boundary adherence signals  </li> </ul>"},{"location":"RIS_SDK_Guide/#32-compute_composite_scoremetrics","title":"3.2 compute_composite_score(metrics)","text":"<p>Weighted calculation:</p> <ul> <li>30% chain stability  </li> <li>25% semantic coherence  </li> <li>20% drift sensitivity  </li> <li>15% variance compliance  </li> <li>10% boundary adherence  </li> </ul>"},{"location":"RIS_SDK_Guide/#33-determine_ris_levelscore-metrics","title":"3.3 determine_ris_level(score, metrics)","text":"<p>Logic:</p> <pre><code>if boundary violations:\n    return \"RIS-0\"\nelif score &gt;= 0.90:\n    return \"RIS-4\"\nelif score &gt;= 0.76:\n    return \"RIS-3\"\nelif score &gt;= 0.61:\n    return \"RIS-2\"\nelif score &gt;= 0.41:\n    return \"RIS-1\"\nelse:\n    return \"RIS-0\"\n</code></pre>"},{"location":"RIS_SDK_Guide/#4-example-python-sdk","title":"4. Example Python SDK","text":"<p>Pseudo-code implementation:</p> <pre><code>class RIS:\n    def compute_metrics(self, samples):\n        # compute all RIS metrics\n        return metrics\n\n    def composite_score(self, metrics):\n        score = (\n            metrics[\"chain_stability\"] * 0.30 +\n            metrics[\"semantic_coherence\"] * 0.25 +\n            (1 - metrics[\"drift_sensitivity\"]) * 0.20 +\n            metrics[\"variance_compliance\"] * 0.15 +\n            (1 - metrics[\"boundary_violations\"]) * 0.10\n        )\n        return score\n\n    def ris_level(self, score, metrics):\n        # apply RIS rules\n        return level\n\n    def evaluate(self, samples):\n        metrics = self.compute_metrics(samples)\n        score = self.composite_score(metrics)\n        level = self.ris_level(score, metrics)\n        return { \"metrics\": metrics, \"score\": score, \"level\": level }\n</code></pre>"},{"location":"RIS_SDK_Guide/#5-example-usage","title":"5. Example Usage","text":"<pre><code>ris = RIS()\nresults = ris.evaluate(samples)\nprint(\"RIS Level:\", results[\"level\"])\n</code></pre>"},{"location":"RIS_SDK_Guide/#6-integration-with-evaluators-optional","title":"6. Integration With Evaluators (Optional)","text":"<p>The SDK MAY optionally call:</p> <ul> <li>LCAC evaluation API  </li> <li>internal evaluation pipeline  </li> <li>on-premise RIS evaluator  </li> </ul> <p>This is not required.</p>"},{"location":"RIS_SDK_Guide/#7-summary","title":"7. Summary","text":"<p>RIS SDKs accelerate evaluation by:</p> <ul> <li>abstracting metric logic  </li> <li>reducing code duplication  </li> <li>enabling reproducible assessments  </li> <li>integrating with CI/CD systems  </li> </ul> <p>Organizations SHOULD provide internal SDKs to ensure consistent scoring across teams.</p>"},{"location":"RIS_Scorecard_Schema/","title":"RIS Scorecard JSON Schema","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Scorecard_Schema/#1-purpose","title":"1. Purpose","text":"<p>This document defines the canonical JSON structure for a RIS scorecard.</p> <p>A RIS scorecard is a machine-readable summary of:</p> <ul> <li>the system evaluated</li> <li>RIS metrics</li> <li>composite reasoning integrity score</li> <li>RIS level</li> <li>evaluation metadata</li> <li>evidence references</li> </ul> <p>This schema is intended for:</p> <ul> <li>internal registries</li> <li>public portals</li> <li>audit systems</li> <li>model cards that embed RIS data</li> </ul>"},{"location":"RIS_Scorecard_Schema/#2-top-level-structure","title":"2. Top-Level Structure","text":"<p>Scorecards SHALL follow this structure:</p> <pre><code>{\n  \"schema_version\": \"1.0\",\n  \"system\": { ... },\n  \"evaluation\": { ... },\n  \"metrics\": { ... },\n  \"ris\": { ... },\n  \"evidence\": { ... },\n  \"signatures\": { ... }\n}\n</code></pre> <p>All fields are described in the following sections.</p>"},{"location":"RIS_Scorecard_Schema/#3-system-object","title":"3. system Object","text":"<p>Describes the evaluated system.</p> <pre><code>\"system\": {\n  \"name\": \"string\",\n  \"version\": \"string\",\n  \"owner\": \"string\",\n  \"organization\": \"string\",\n  \"description\": \"string\",\n  \"category\": \"string\",\n  \"use_cases\": [ \"string\" ],\n  \"environment\": \"string\",\n  \"model_family\": \"string\",\n  \"model_version\": \"string\"\n}\n</code></pre> <p>Fields:</p> <ul> <li>name: system or model identifier</li> <li>version: system version string</li> <li>owner: internal team or business owner</li> <li>organization: legal entity responsible</li> <li>description: high-level summary</li> <li>category: e.g., \u201cLLM\u201d, \u201cagent\u201d, \u201cRAG\u201d, \u201cmulti-agent\u201d</li> <li>use_cases: primary supported usage</li> <li>environment: \u201cproduction\u201d, \u201cstaging\u201d, \u201cinternal\u201d, etc.</li> <li>model_family: e.g., \u201cGPT-4 class\u201d, \u201cLlama 3\u201d, \u201cinternal\u201d</li> <li>model_version: specific model build or release label</li> </ul>"},{"location":"RIS_Scorecard_Schema/#4-evaluation-object","title":"4. evaluation Object","text":"<p>Describes evaluation conditions.</p> <pre><code>\"evaluation\": {\n  \"evaluation_id\": \"string\",\n  \"requested_ris_level\": \"string\",\n  \"date\": \"YYYY-MM-DD\",\n  \"evaluator\": {\n    \"name\": \"string\",\n    \"organization\": \"string\",\n    \"type\": \"string\"\n  },\n  \"config\": {\n    \"temperature\": 0.0,\n    \"top_p\": 0.0,\n    \"context_window\": \"string\",\n    \"system_prompt_hash\": \"string\",\n    \"agent_orchestration\": \"string\",\n    \"tools_enabled\": [ \"string\" ],\n    \"rag_enabled\": true\n  }\n}\n</code></pre> <p>Fields:</p> <ul> <li>evaluation_id: unique identifier</li> <li>requested_ris_level: target level (e.g., \u201cRIS-3\u201d)</li> <li>date: evaluation completion date</li> <li>evaluator: who performed evaluation (human or automated)</li> <li>config: high-level inference and environment configuration</li> </ul>"},{"location":"RIS_Scorecard_Schema/#5-metrics-object","title":"5. metrics Object","text":"<p>This object records numeric metrics required by RIS.</p> <pre><code>\"metrics\": {\n  \"chain_stability\": 0.0,\n  \"semantic_coherence\": 0.0,\n  \"drift_sensitivity\": 0.0,\n  \"variance_compliance\": 0.0,\n  \"boundary_violations\": 0,\n  \"sample_size\": {\n    \"baseline_prompts\": 0,\n    \"perturbation_prompts\": 0,\n    \"baseline_samples\": 0,\n    \"perturbation_samples\": 0\n  }\n}\n</code></pre> <p>Fields:</p> <ul> <li>chain_stability: mean stability score (0.00\u20131.00)</li> <li>semantic_coherence: mean coherence score (0.00\u20131.00)</li> <li>drift_sensitivity: drift sensitivity value (lower is better)</li> <li>variance_compliance: proportion of samples within envelope (0.00\u20131.00)</li> <li>boundary_violations: count of boundary violation events observed</li> <li>sample_size: counts used to compute metrics</li> </ul>"},{"location":"RIS_Scorecard_Schema/#6-ris-object","title":"6. ris Object","text":"<p>This object captures RIS-specific outcome data.</p> <pre><code>\"ris\": {\n  \"composite_score\": 0.0,\n  \"level\": \"string\",\n  \"status\": \"string\",\n  \"risk_profile\": {\n    \"structural_drift\": \"Low\",\n    \"semantic_drift\": \"Low\",\n    \"temporal_stability\": \"Low\",\n    \"interference\": \"Low\",\n    \"boundary\": \"Low\",\n    \"degradation\": \"Low\"\n  },\n  \"valid_from\": \"YYYY-MM-DD\",\n  \"valid_until\": \"YYYY-MM-DD\"\n}\n</code></pre> <p>Fields:</p> <ul> <li>composite_score: weighted reasoning integrity score (0.00\u20131.00)</li> <li>level: RIS level assigned (\u201cRIS-0\u201d to \u201cRIS-4\u201d)</li> <li>status: \u201cPASS\u201d or \u201cFAIL\u201d relative to requested_ris_level</li> <li>risk_profile: classification across RIS risk categories</li> <li>valid_from: effective date</li> <li>valid_until: expiration per RIS rules</li> </ul>"},{"location":"RIS_Scorecard_Schema/#7-evidence-object","title":"7. evidence Object","text":"<p>References to supporting artifacts.</p> <pre><code>\"evidence\": {\n  \"evaluation_report_uri\": \"string\",\n  \"conformance_statement_uri\": \"string\",\n  \"raw_samples_uri\": \"string\",\n  \"metrics_dataset_uri\": \"string\",\n  \"logs_uri\": \"string\",\n  \"notes\": \"string\"\n}\n</code></pre> <p>Fields are URIs or internal references to evaluation artifacts.</p>"},{"location":"RIS_Scorecard_Schema/#8-signatures-object","title":"8. signatures Object","text":"<p>Describes who attested to this scorecard.</p> <pre><code>\"signatures\": {\n  \"evaluator\": {\n    \"name\": \"string\",\n    \"role\": \"string\",\n    \"organization\": \"string\",\n    \"signed_at\": \"YYYY-MM-DD\"\n  },\n  \"system_owner\": {\n    \"name\": \"string\",\n    \"role\": \"string\",\n    \"organization\": \"string\",\n    \"signed_at\": \"YYYY-MM-DD\"\n  }\n}\n</code></pre>"},{"location":"RIS_Scorecard_Schema/#9-minimal-example","title":"9. Minimal Example","text":"<pre><code>{\n  \"schema_version\": \"1.0\",\n  \"system\": {\n    \"name\": \"Example Agent\",\n    \"version\": \"1.2.3\",\n    \"owner\": \"AI Systems Team\",\n    \"organization\": \"ExampleCorp\",\n    \"description\": \"Multi-agent reasoning system for internal analytics.\",\n    \"category\": \"agent\",\n    \"use_cases\": [ \"analytics\", \"decision support\" ],\n    \"environment\": \"production\",\n    \"model_family\": \"GPT-class\",\n    \"model_version\": \"2025-01-01\"\n  },\n  \"evaluation\": {\n    \"evaluation_id\": \"EV-2025-0001\",\n    \"requested_ris_level\": \"RIS-3\",\n    \"date\": \"2025-01-10\",\n    \"evaluator\": {\n      \"name\": \"Governance Team A\",\n      \"organization\": \"ExampleCorp\",\n      \"type\": \"internal\"\n    },\n    \"config\": {\n      \"temperature\": 0.3,\n      \"top_p\": 0.9,\n      \"context_window\": \"32k\",\n      \"system_prompt_hash\": \"abc123...\",\n      \"agent_orchestration\": \"central-planner\",\n      \"tools_enabled\": [ \"search\", \"retrieval\" ],\n      \"rag_enabled\": true\n    }\n  },\n  \"metrics\": {\n    \"chain_stability\": 0.88,\n    \"semantic_coherence\": 0.91,\n    \"drift_sensitivity\": 0.11,\n    \"variance_compliance\": 0.97,\n    \"boundary_violations\": 0,\n    \"sample_size\": {\n      \"baseline_prompts\": 50,\n      \"perturbation_prompts\": 20,\n      \"baseline_samples\": 1250,\n      \"perturbation_samples\": 200\n    }\n  },\n  \"ris\": {\n    \"composite_score\": 0.84,\n    \"level\": \"RIS-3\",\n    \"status\": \"PASS\",\n    \"risk_profile\": {\n      \"structural_drift\": \"Low\",\n      \"semantic_drift\": \"Low\",\n      \"temporal_stability\": \"Low\",\n      \"interference\": \"Low\",\n      \"boundary\": \"Low\",\n      \"degradation\": \"Low\"\n    },\n    \"valid_from\": \"2025-01-10\",\n    \"valid_until\": \"2026-01-10\"\n  },\n  \"evidence\": {\n    \"evaluation_report_uri\": \"https://.../reports/EV-2025-0001\",\n    \"conformance_statement_uri\": \"https://.../conformance/EV-2025-0001\",\n    \"raw_samples_uri\": \"s3://.../samples/EV-2025-0001\",\n    \"metrics_dataset_uri\": \"s3://.../metrics/EV-2025-0001\",\n    \"logs_uri\": \"s3://.../logs/EV-2025-0001\",\n    \"notes\": \"Initial production evaluation.\"\n  },\n  \"signatures\": {\n    \"evaluator\": {\n      \"name\": \"Jane Doe\",\n      \"role\": \"AI Governance Lead\",\n      \"organization\": \"ExampleCorp\",\n      \"signed_at\": \"2025-01-11\"\n    },\n    \"system_owner\": {\n      \"name\": \"John Smith\",\n      \"role\": \"Product Owner\",\n      \"organization\": \"ExampleCorp\",\n      \"signed_at\": \"2025-01-12\"\n    }\n  }\n}\n</code></pre>"},{"location":"RIS_Standard_Index/","title":"RIS Standard Index","text":"<p>Reasoning Integrity Standard (RIS) Version 1.0 Published by Atom Labs</p>"},{"location":"RIS_Standard_Index/#1-purpose","title":"1. Purpose","text":"<p>This Standard Index provides a complete, authoritative catalog of all documents included in the RIS v1.0 release package.</p> <p>It is intended for:</p> <ul> <li>auditors  </li> <li>governance and risk teams  </li> <li>researchers  </li> <li>enterprises adopting RIS  </li> <li>regulators and evaluators  </li> </ul> <p>This index SHOULD be included with every RIS v1.x release.</p>"},{"location":"RIS_Standard_Index/#2-core-specification-documents-sections-013","title":"2. Core Specification Documents (Sections 0\u201313)","text":"<p>These files collectively form the official RIS v1.0 standard.</p> <p>Located in:</p> <pre><code>docs/sections/\n</code></pre> <p>Files:</p> <ul> <li>00-foreword.md  </li> <li>01-scope.md  </li> <li>02-normative-references.md  </li> <li>03-fundamental-concepts.md  </li> <li>04-ris-levels.md  </li> <li>05-measurement-framework.md  </li> <li>06-controls.md  </li> <li>07-scoring-and-conformance.md  </li> <li>08-risk-models.md  </li> <li>09-evaluation-methodology.md  </li> <li>10-audit-guidelines.md  </li> <li>11-reference-implementation.md  </li> <li>12-standard-mapping.md  </li> <li>13-appendices.md  </li> </ul> <p>These documents are normative for RIS v1.0.</p>"},{"location":"RIS_Standard_Index/#3-supporting-documents","title":"3. Supporting Documents","text":"<p>These provide additional guidance for implementation and operationalization.</p> <p>Located in:</p> <pre><code>docs/\n</code></pre> <p>Files:</p> <ul> <li>RIS_Overview.md  </li> <li>RIS_Conformance_Statement.md  </li> <li>RIS_Evaluation_Report_Template.md  </li> <li>RIS_Benchmark_Guide.md  </li> <li>RIS_Integration_Guide.md  </li> <li>RIS_SDK_Guide.md  </li> <li>RIS_Reference_Evaluator_Guide.md  </li> <li>RIS_Example_Prompt_Dataset.md  </li> <li>RIS_API_Integration_Guide.md  </li> <li>RIS_Portal_API_Blueprint.md  </li> <li>RIS_Portal_Layout_Spec.md  </li> <li>RIS_Certification_Program.md  </li> </ul> <p>These documents are informative unless explicitly referenced as normative in the core specification.</p>"},{"location":"RIS_Standard_Index/#4-data-and-registry-schemas","title":"4. Data and Registry Schemas","text":"<p>These define machine-readable formats for scorecards, certification entries, and directories.</p> <p>Located in:</p> <pre><code>docs/\n</code></pre> <p>Files:</p> <ul> <li>RIS_Scorecard_Schema.md  </li> <li>RIS_Certification_Schema.md  </li> <li>RIS_Directory_Format.md  </li> </ul> <p>These schemas are intended for:</p> <ul> <li>RIS portals  </li> <li>certification registries  </li> <li>internal governance systems  </li> <li>interoperability between tools  </li> </ul>"},{"location":"RIS_Standard_Index/#5-metadata-and-terminology","title":"5. Metadata and Terminology","text":"<p>Located in:</p> <pre><code>docs/\n</code></pre> <p>Files:</p> <ul> <li>RIS_Glossary.md  </li> <li>RIS_Change_Log.md  </li> </ul> <p>These provide:</p> <ul> <li>definitions of terminology  </li> <li>version and revision history  </li> </ul>"},{"location":"RIS_Standard_Index/#6-recommended-repository-structure","title":"6. Recommended Repository Structure","text":"<p>For the official RIS v1.0 repository, the structure SHOULD be:</p> <pre><code>docs/\n    RIS_Overview.md\n    RIS_Conformance_Statement.md\n    RIS_Evaluation_Report_Template.md\n    RIS_Benchmark_Guide.md\n    RIS_Integration_Guide.md\n    RIS_SDK_Guide.md\n    RIS_Reference_Evaluator_Guide.md\n    RIS_Example_Prompt_Dataset.md\n    RIS_API_Integration_Guide.md\n    RIS_Portal_API_Blueprint.md\n    RIS_Portal_Layout_Spec.md\n    RIS_Certification_Program.md\n    RIS_Scorecard_Schema.md\n    RIS_Certification_Schema.md\n    RIS_Directory_Format.md\n    RIS_Glossary.md\n    RIS_Change_Log.md\n    RIS_Standard_Index.md\n    sections/\n        00-foreword.md\n        01-scope.md\n        02-normative-references.md\n        03-fundamental-concepts.md\n        04-ris-levels.md\n        05-measurement-framework.md\n        06-controls.md\n        07-scoring-and-conformance.md\n        08-risk-models.md\n        09-evaluation-methodology.md\n        10-audit-guidelines.md\n        11-reference-implementation.md\n        12-standard-mapping.md\n        13-appendices.md\n</code></pre> <p>This structure is optimized for:</p> <ul> <li>mkdocs / GitHub Pages  </li> <li>clarity for external reviewers  </li> <li>long-term maintainability  </li> </ul>"},{"location":"RIS_Standard_Index/#7-version","title":"7. Version","text":"<p>This index applies to:</p> <pre><code>Reasoning Integrity Standard (RIS) v1.0\nRelease Year: 2025\n</code></pre> <p>Future RIS versions (v1.1, v2.0, etc.) SHOULD update this index to reflect all added, removed, or superseded documents.</p>"},{"location":"RIS_v1_1_Roadmap/","title":"RIS v1.1 Roadmap","text":"<p>Reasoning Integrity Standard (RIS) Planned Release: TBD Maintained by Atom Labs</p>"},{"location":"RIS_v1_1_Roadmap/#1-purpose","title":"1. Purpose","text":"<p>This roadmap outlines proposed enhancements for RIS v1.1. It is non-normative and intended for:</p> <ul> <li>future specification development  </li> <li>community proposals  </li> <li>governance review  </li> <li>working group discussions  </li> </ul>"},{"location":"RIS_v1_1_Roadmap/#2-guiding-principles","title":"2. Guiding Principles","text":"<p>RIS v1.1 SHALL focus on:</p> <ul> <li>extending metrics  </li> <li>improving clarity  </li> <li>supporting more complex deployment environments  </li> <li>enhancing risk modeling  </li> <li>strengthening evaluation repeatability  </li> <li>adding optional profiles  </li> </ul>"},{"location":"RIS_v1_1_Roadmap/#3-proposed-additions-for-v11","title":"3. Proposed Additions for v1.1","text":""},{"location":"RIS_v1_1_Roadmap/#31-metric-extensions","title":"3.1 Metric Extensions","text":"<ul> <li>temporal coherence metric  </li> <li>reasoning-chain compression score  </li> <li>multi-model consistency metric  </li> <li>cross-perturbation semantic alignment measurement  </li> </ul>"},{"location":"RIS_v1_1_Roadmap/#32-risk-model-enhancements","title":"3.2 Risk Model Enhancements","text":"<ul> <li>multi-agent interference scoring  </li> <li>RAG (retrieval) interference scoring  </li> <li>scenario-based drift modeling  </li> <li>anomaly detection for emergent reasoning paths  </li> </ul>"},{"location":"RIS_v1_1_Roadmap/#33-evaluation-method-extensions","title":"3.3 Evaluation Method Extensions","text":"<ul> <li>adversarial perturbation tests (optional)  </li> <li>large-sample bootstrapping guidance  </li> <li>randomized stress tests  </li> <li>multi-turn evaluation methodology  </li> <li>agentic workflow evaluation methodology  </li> </ul>"},{"location":"RIS_v1_1_Roadmap/#34-governance-controls","title":"3.4 Governance &amp; Controls","text":"<ul> <li>new control family: Agent Interaction Controls (AIC)  </li> <li>expanded boundary enforcement rules for RAG systems  </li> <li>stricter definitions for memory-based drift  </li> </ul>"},{"location":"RIS_v1_1_Roadmap/#4-proposed-new-documents","title":"4. Proposed New Documents","text":"<ul> <li>RIS Multi-Agent Profile  </li> <li>RIS RAG Profile  </li> <li>RIS Safety-Critical Profile  </li> <li>RIS Temporal Assessment Guide  </li> <li>RIS Scenario Benchmark Suite  </li> <li>RIS Adversarial Evaluation Guide  </li> </ul>"},{"location":"RIS_v1_1_Roadmap/#5-clarifications-editorial-improvements","title":"5. Clarifications &amp; Editorial Improvements","text":"<ul> <li>improved definitions of drift categories  </li> <li>formal mathematical notation in measurement framework  </li> <li>updated diagrams and reference flows  </li> <li>cross-referencing improvements across sections  </li> </ul>"},{"location":"RIS_v1_1_Roadmap/#6-backward-compatibility","title":"6. Backward Compatibility","text":"<p>RIS v1.1 is intended to be:</p> <ul> <li>fully backward-compatible with v1.0  </li> <li>additive, not breaking  </li> <li>optional upgrades for existing evaluators  </li> </ul> <p>RIS v2.0 will be the first version with major breaking changes.</p>"},{"location":"RIS_v1_1_Roadmap/#7-timeline-tentative","title":"7. Timeline (Tentative)","text":"<ul> <li>Working Draft: 2\u20133 months  </li> <li>Internal Review: after draft  </li> <li>Public Review (optional): 30 days  </li> <li>Final Release: TBD  </li> </ul>"},{"location":"RIS_v1_1_Roadmap/#8-participation","title":"8. Participation","text":"<p>Atom Labs MAY invite:</p> <ul> <li>enterprise governance teams  </li> <li>researchers  </li> <li>standards bodies  </li> <li>regulator observers  </li> <li>industry partners  </li> </ul> <p>to propose changes for v1.1.</p>"},{"location":"RIS_v1_1_Roadmap/#9-version","title":"9. Version","text":"<p>Draft planning document for:</p> <p>RIS v1.1 - Roadmap Proposal</p>"},{"location":"ris_v1_0/","title":"Reasoning Integrity Standard (RIS) v1.0","text":"<p>Published by Atom Labs \u00a9 2025 Atom Labs. All Rights Reserved.</p>"},{"location":"ris_v1_0/#0-foreword","title":"0. Foreword","text":"<p>The Reasoning Integrity Standard (RIS) defines a formal framework for evaluating, governing, and maintaining the integrity of reasoning performed by large language models (LLMs), autonomous agents, and multi-model cognitive systems.</p> <p>This standard was developed by Atom Labs to address the absence of authoritative, measurable criteria for assessing the stability and trustworthiness of LLM reasoning in production environments. As AI systems increasingly perform high-impact decision-making across enterprise, regulatory, and safety-critical contexts, the industry requires an auditable, repeatable, and model-agnostic method for measuring reasoning reliability.</p> <p>RIS establishes a multi-level maturity model (RIS-0 through RIS-4) along with normative controls governing chain-of-thought stability, semantic coherence, drift detection, governance, and reasoning boundary enforcement. It specifies the requirements necessary for organizations to assess the integrity of reasoning workflows under real-world operational and adversarial conditions.</p> <p>This standard is intended for:</p> <ul> <li>enterprise architects deploying AI systems</li> <li>safety and governance teams responsible for LLM oversight</li> <li>auditors and compliance personnel evaluating AI risk posture</li> <li>engineering teams building LLM-integrated applications</li> <li>research organizations conducting reproducible benchmarking</li> </ul> <p>RIS v1.0 is model-agnostic and does not mandate the use of any specific framework or vendor. LCAC (Least-Context Access Control) is referenced solely as an example of a compliant reference implementation. RIS remains an independent, neutral specification applicable to any LLM or cognitive system.</p> <p>Future revisions may expand the standard to include coherence metrics, multi-agent interference scoring, predictive trust modeling, and additional benchmark suites. Atom Labs invites the research, standards, and enterprise community to review, adopt, and contribute to future versions.</p>"},{"location":"ris_v1_0/#1-scope","title":"1. Scope","text":""},{"location":"ris_v1_0/#11-purpose","title":"1.1 Purpose","text":"<p>The Reasoning Integrity Standard (RIS) defines the requirements, controls, measurement methodologies, and conformance criteria necessary to evaluate the integrity of reasoning performed by LLMs and LLM-powered cognitive systems. The purpose of RIS is not to measure the correctness or factuality of model outputs, but to measure the stability, predictability, and structural reliability of the reasoning process that produces those outputs.</p>"},{"location":"ris_v1_0/#12-applicability","title":"1.2 Applicability","text":"<p>RIS applies to any system that conducts or delegates reasoning to:</p> <ul> <li>large language models (LLMs)</li> <li>autonomous or semi-autonomous agents</li> <li>multi-agent architectures</li> <li>chain-of-thought or stepwise reasoning pipelines</li> <li>retrieval-augmented systems (RAG)</li> <li>tool-augmented or API-integrated cognitive systems</li> <li>multi-model or ensemble inference architectures</li> </ul> <p>RIS is applicable regardless of:</p> <ul> <li>model vendor or provider</li> <li>model size, training method, or licensing</li> <li>deployment environment (cloud, hybrid, on-premise, edge)</li> <li>inference scenario (single-turn, multi-turn, stateful, agentic)</li> </ul>"},{"location":"ris_v1_0/#13-intended-use","title":"1.3 Intended Use","text":"<p>RIS is intended to support:</p> <ul> <li>evaluation of LLMs and agentic systems before deployment</li> <li>continuous monitoring of reasoning behavior in production environments</li> <li>regulatory and compliance audits involving AI reasoning stability</li> <li>vendor and model procurement due diligence</li> <li>internal trust, governance, and safety programs</li> <li>reproducible research and benchmarking</li> </ul>"},{"location":"ris_v1_0/#14-out-of-scope","title":"1.4 Out of Scope","text":"<p>The following areas are outside the scope of RIS v1.0:</p> <ul> <li>data privacy practices</li> <li>model training and dataset governance</li> <li>ethics, fairness, bias, or demographic analysis</li> <li>factual correctness of responses</li> <li>model alignment and value judgments unrelated to reasoning integrity</li> </ul> <p>These areas may be addressed by other standards or future RIS extensions.</p>"},{"location":"ris_v1_0/#15-normative-language","title":"1.5 Normative Language","text":"<p>RIS uses the following requirement terminology:</p> <ul> <li>MUST: an absolute, mandatory requirement for conformance</li> <li>SHALL: a criterion required to meet RIS compliance</li> <li>SHOULD: a recommended practice unless a justified exception exists</li> <li>MAY: an optional practice</li> <li>NOT PERMITTED: a prohibited behavior or configuration</li> </ul>"},{"location":"ris_v1_0/#16-relationship-to-lcac","title":"1.6 Relationship to LCAC","text":"<p>LCAC is a reference implementation illustrating one possible method of achieving RIS compliance. RIS does not require LCAC, nor does conformance imply adoption of any specific product, tool, or architecture. RIS is intended to remain implementation-agnostic.</p>"},{"location":"ris_v1_0/#17-versioning-and-revision-policy","title":"1.7 Versioning and Revision Policy","text":"<p>RIS v1.0 is the initial published version of the Reasoning Integrity Standard. Atom Labs maintains stewardship of the standard until the formation or designation of a formal multi-party standards body.</p> <p>Future revisions may include:</p> <ul> <li>expanded risk models</li> <li>domain-specific profiles</li> <li>standardized test suites</li> <li>trust-flow prediction methodologies</li> <li>multi-agent interference metrics</li> </ul> <p>Revisions will follow a versioned specification model (v1.1, v1.2, v2.0, etc.).</p>"},{"location":"ris_v1_0/#2-normative-references","title":"2. Normative References","text":""},{"location":"ris_v1_0/#21-purpose-of-normative-references","title":"2.1 Purpose of Normative References","text":"<p>This section identifies documents, standards, and terminology references that are considered authoritative for understanding and implementing the Reasoning Integrity Standard (RIS). Where conflicts arise, this specification takes precedence unless a referenced standard is explicitly designated as controlling for a specific requirement.</p> <p>Normative references are essential for interpreting RIS controls, understanding terminology, and establishing consistent evaluation and audit methodologies across implementations.</p>"},{"location":"ris_v1_0/#22-standards-and-frameworks-referenced-by-ris","title":"2.2 Standards and Frameworks Referenced by RIS","text":"<p>The following documents and standards are integral to interpreting RIS v1.0. Implementers are expected to be familiar with these materials, as RIS incorporates concepts, terminology, and structural patterns from them.</p>"},{"location":"ris_v1_0/#nist-standards","title":"NIST Standards","text":"<ul> <li>NIST Special Publication 800-53: Security and Privacy Controls for Information Systems and Organizations  </li> <li>NIST Artificial Intelligence Risk Management Framework (AI RMF)  </li> <li>NIST SP 1270: Trustworthy and Responsible AI  </li> </ul>"},{"location":"ris_v1_0/#iso-standards","title":"ISO Standards","text":"<ul> <li>ISO/IEC 27001: Information Security Management Systems  </li> <li>ISO/IEC 23894: Artificial Intelligence Risk Management  </li> <li>ISO/IEC 29100: Privacy Framework  </li> <li>ISO/IEC 25010: System and Software Quality Models  </li> </ul>"},{"location":"ris_v1_0/#soc-2-and-aicpa-guidance","title":"SOC 2 and AICPA Guidance","text":"<ul> <li>AICPA Trust Services Criteria (Security, Availability, Processing Integrity, Confidentiality, Privacy)  </li> <li>AICPA AT-C 205: Examination Engagements  </li> </ul>"},{"location":"ris_v1_0/#owasp-and-application-security-references","title":"OWASP and Application Security References","text":"<ul> <li>OWASP Application Security Verification Standard (ASVS)  </li> <li>OWASP Top 10 (referenced in governance and boundary controls)  </li> </ul>"},{"location":"ris_v1_0/#ai-safety-governance-and-model-evaluation-references","title":"AI Safety, Governance, and Model Evaluation References","text":"<ul> <li>EU AI Act (Draft and Final Texts)  </li> <li>MLPerf and related benchmark publications  </li> <li>Model evaluation methodologies from major AI labs  </li> <li>Academic publications on chain-of-thought, inference stability, and cognitive drift  </li> </ul>"},{"location":"ris_v1_0/#23-terminology-references","title":"2.3 Terminology References","text":"<p>RIS uses terminology derived from established research and industry standards. Implementers should reference the following categories for precise interpretation.</p>"},{"location":"ris_v1_0/#ai-systems-terminology","title":"AI Systems Terminology","text":"<ul> <li>large language model (LLM)  </li> <li>agent and multi-agent system  </li> <li>chain-of-thought (CoT)  </li> <li>semantic coherence  </li> <li>contextual drift  </li> <li>inference-time variability  </li> <li>deterministic vs. non-deterministic reasoning  </li> </ul>"},{"location":"ris_v1_0/#governance-terminology","title":"Governance Terminology","text":"<ul> <li>risk posture  </li> <li>control requirement  </li> <li>conformance  </li> <li>audit evidence  </li> <li>variance threshold  </li> <li>stability metric  </li> </ul>"},{"location":"ris_v1_0/#lcac-terminology-reference-implementation-only","title":"LCAC Terminology (Reference Implementation Only)","text":"<p>LCAC terminology is informative, not normative. Terms include:</p> <ul> <li>least-context access control  </li> <li>governance mode  </li> <li>reasoning boundary  </li> <li>drift envelope  </li> <li>trust baseline  </li> <li>variance baseline  </li> <li>reasoning ledger  </li> </ul> <p>Implementers are not required to adopt LCAC\u2019s terminology or architecture, but LCAC terms appear in examples and appendices as part of the reference implementation.</p>"},{"location":"ris_v1_0/#24-definitions-governed-by-ris","title":"2.4 Definitions Governed by RIS","text":"<p>RIS mandates the following definitions for consistent use across all conformant implementations.</p>"},{"location":"ris_v1_0/#reasoning-integrity","title":"Reasoning Integrity","text":"<p>The measurable ability of a system to produce stable, predictable reasoning outputs that maintain structural coherence across similar prompts, repeated prompts, operational conditions, and controlled variations.</p>"},{"location":"ris_v1_0/#reasoning-drift","title":"Reasoning Drift","text":"<p>A deviation in reasoning behavior, structure, or output that exceeds defined variance thresholds when evaluated under repeatable or controlled conditions.</p>"},{"location":"ris_v1_0/#chain-stability","title":"Chain Stability","text":"<p>The degree to which the internal reasoning structure of a system maintains consistent logical steps, semantic transitions, and contextual boundaries across repeated inference cycles.</p>"},{"location":"ris_v1_0/#semantic-consistency","title":"Semantic Consistency","text":"<p>The alignment of meaning, intent, and conceptual structure across multiple reasoning instances or reasoning steps for equivalent prompts.</p>"},{"location":"ris_v1_0/#governance-boundary","title":"Governance Boundary","text":"<p>A defined constraint controlling the informational, contextual, or semantic state accessible to a reasoning system at inference time.</p>"},{"location":"ris_v1_0/#25-control-of-normative-references","title":"2.5 Control of Normative References","text":"<p>Where RIS references external standards:</p> <ul> <li>External standards MUST be treated as supporting material.</li> <li>External standards SHALL NOT override RIS requirements unless explicitly designated as authoritative.</li> <li>RIS SHALL define the governing definitions for reasoning integrity and drift.</li> <li>External terminology MAY be used for interpretation but not for replacing RIS controls.</li> </ul>"},{"location":"ris_v1_0/#26-informative-vs-normative-material","title":"2.6 Informative vs. Normative Material","text":"<p>RIS distinguishes between:</p> <ul> <li>Normative material (mandatory for conformance)</li> <li>Informative material (supporting guidance)</li> </ul> <p>Normative sections include:</p> <ul> <li>Controls  </li> <li>Requirements  </li> <li>Measurement Framework  </li> <li>Scoring and Conformance  </li> <li>Audit Guidelines  </li> </ul> <p>Informative sections include:</p> <ul> <li>Reference Implementation  </li> <li>Mapping to Other Standards  </li> <li>Appendices  </li> </ul> <p>This distinction is preserved throughout the remainder of the specification.</p>"},{"location":"ris_v1_0/#3-fundamental-concepts-of-reasoning-integrity","title":"3. Fundamental Concepts of Reasoning Integrity","text":""},{"location":"ris_v1_0/#31-overview","title":"3.1 Overview","text":"<p>This section defines the foundational concepts required to understand and evaluate reasoning integrity in large language models (LLMs), autonomous agents, and multi-model cognitive systems. These concepts form the basis of all controls, requirements, and conformance criteria in RIS. Implementers MUST understand and consistently apply these definitions across all stages of evaluation, monitoring, and governance.</p> <p>Reasoning integrity is not a measure of factual correctness or ethical compliance. It is a measure of the internal stability, predictability, and structural reliability of the reasoning process itself.</p>"},{"location":"ris_v1_0/#32-reasoning-integrity","title":"3.2 Reasoning Integrity","text":"<p>Reasoning integrity is the degree to which a reasoning system produces stable, coherent, and predictable internal reasoning structures when presented with equivalent or contextually similar prompts, conditions, or operational states.</p> <p>A system with high reasoning integrity maintains:</p> <ul> <li>consistent logical structure</li> <li>predictable semantic relationships</li> <li>stable reasoning paths across repetitions</li> <li>adherence to defined contextual boundaries</li> <li>low drift under controlled variations</li> </ul> <p>A system with low reasoning integrity exhibits:</p> <ul> <li>unpredictable reasoning paths</li> <li>inconsistent semantic structures</li> <li>sensitivity to minor input or environmental variations</li> <li>uncontrolled drift over repeated inferences</li> <li>unstable or contradictory reasoning forms</li> </ul>"},{"location":"ris_v1_0/#33-reasoning-chain","title":"3.3 Reasoning Chain","text":"<p>A reasoning chain is the internal, stepwise sequence of conceptual transformations used by the system to derive an output from an input. The chain may be implicit (black-box LLMs) or explicit (systems exposing structured reasoning steps), but RIS treats both as conceptual equivalents.</p> <p>A reasoning chain includes:</p> <ul> <li>latent representations</li> <li>intermediate conceptual structures</li> <li>logical transitions</li> <li>semantic dependencies</li> <li>step-to-step relationships</li> </ul> <p>RIS does not require access to internal model states. Reasoning chains may be reconstructed using output structure, behavioral patterns, and statistical inference.</p>"},{"location":"ris_v1_0/#34-chain-stability","title":"3.4 Chain Stability","text":"<p>Chain stability is the consistency of reasoning chain structure across repeated or controlled inference cycles.</p> <p>A system demonstrates chain stability when:</p> <ul> <li>repeated prompts generate reasoning structures with low variance</li> <li>stepwise transitions remain logically and semantically similar</li> <li>variations fall within defined acceptable thresholds</li> </ul> <p>Chain stability is a core metric in RIS and is used to determine system conformance at all RIS levels.</p>"},{"location":"ris_v1_0/#35-semantic-coherence","title":"3.5 Semantic Coherence","text":"<p>Semantic coherence is the degree to which reasoning outputs maintain consistent meaning, conceptual alignment, and contextual relevance across equivalent inputs or over the course of a reasoning chain.</p> <p>A system with high semantic coherence ensures that:</p> <ul> <li>meaning remains stable across reasoning steps</li> <li>internal transitions preserve conceptual alignment</li> <li>responses maintain adherence to prompt intent</li> <li>conceptual drift remains within acceptable thresholds</li> </ul> <p>Loss of semantic coherence is a primary indicator of reasoning instability.</p>"},{"location":"ris_v1_0/#36-cognitive-drift","title":"3.6 Cognitive Drift","text":"<p>Cognitive drift is the measurable deviation in a system\u2019s reasoning process when evaluated under repeated, controlled, or minimally varied conditions.</p> <p>Forms of drift include:</p> <ul> <li>structural drift: changes in reasoning chain architecture</li> <li>semantic drift: changes in meaning, context, or conceptual focus</li> <li>temporal drift: progressive degradation of reasoning consistency over time</li> <li>interference drift: changes caused by tools, memory, or agent interactions</li> </ul> <p>Cognitive drift is one of the primary risk factors assessed by RIS.</p>"},{"location":"ris_v1_0/#37-variance-envelope","title":"3.7 Variance Envelope","text":"<p>The variance envelope defines the acceptable bounds of variation for reasoning behavior across repeated inference cycles. These bounds are derived from:</p> <ul> <li>statistical analysis</li> <li>expected variability based on model architecture</li> <li>domain-specific tolerance criteria</li> <li>operational safety requirements</li> </ul> <p>A system MUST maintain its reasoning behavior within its defined variance envelope to achieve RIS conformance.</p>"},{"location":"ris_v1_0/#38-governance-boundary","title":"3.8 Governance Boundary","text":"<p>A governance boundary is a defined limit controlling what contextual, semantic, or informational state the system is permitted to use during a reasoning process.</p> <p>Boundaries include:</p> <ul> <li>allowed context windows</li> <li>prohibited contextual expansions</li> <li>restricted semantic domains</li> <li>tool- and memory-access constraints</li> <li>cross-agent interaction limits</li> </ul> <p>Governance boundaries prevent uncontrolled or unauthorized expansion of reasoning state.</p>"},{"location":"ris_v1_0/#39-deterministic-and-non-deterministic-reasoning","title":"3.9 Deterministic and Non-Deterministic Reasoning","text":"<p>Most LLM systems exhibit non-deterministic reasoning due to sampling temperature, randomness, and stochastic inference pathways. RIS does not require deterministic outputs; it requires:</p> <ul> <li>deterministic reasoning patterns within allowable variance</li> <li>predictable behavior under controlled conditions</li> <li>stable reasoning structure despite inherent stochasticity</li> </ul> <p>Systems MUST demonstrate stable structural patterns even if surface-level outputs vary.</p>"},{"location":"ris_v1_0/#310-tool-and-agent-interference","title":"3.10 Tool and Agent Interference","text":"<p>Tool or agent interference occurs when the reasoning system\u2019s internal process is altered by:</p> <ul> <li>external tool responses</li> <li>API calls</li> <li>retrieval results</li> <li>other agents in a multi-agent environment</li> </ul> <p>Interference may cause:</p> <ul> <li>unexpected semantic transitions</li> <li>contextual overrides</li> <li>premature reasoning collapse</li> <li>unbounded expansion of reasoning state</li> </ul> <p>RIS controls evaluate the system\u2019s ability to maintain stability despite such interference.</p>"},{"location":"ris_v1_0/#311-state-persistence-and-contextual-memory","title":"3.11 State Persistence and Contextual Memory","text":"<p>State persistence defines how long prior reasoning states influence future reasoning cycles. Excessive or uncontrolled persistence may create:</p> <ul> <li>reasoning contamination</li> <li>indirect hallucination</li> <li>unexpected cross-task dependency</li> <li>failure to reinitialize reasoning state</li> </ul> <p>RIS requires systems to maintain predictable and bounded state persistence behaviors.</p>"},{"location":"ris_v1_0/#312-integrity-failure-modes","title":"3.12 Integrity Failure Modes","text":"<p>Common reasoning integrity failure modes include:</p> <ul> <li>spontaneous reasoning divergence</li> <li>oscillatory reasoning loops</li> <li>semantic boundary collapse</li> <li>unintended contextual expansion</li> <li>instability under minimal input variations</li> <li>conflicting reasoning steps</li> <li>multi-agent interference anomalies</li> <li>degradation over time or workload</li> </ul> <p>These failure modes are formalized later in the RIS risk models and evaluation methodology.</p>"},{"location":"ris_v1_0/#4-ris-levels","title":"4. RIS Levels","text":""},{"location":"ris_v1_0/#41-overview","title":"4.1 Overview","text":"<p>The Reasoning Integrity Standard (RIS) defines five levels of reasoning integrity maturity, ranging from RIS-0 (uncontrolled reasoning) to RIS-4 (high-integrity, production-grade reasoning). These levels provide a structured framework for evaluating the stability, predictability, and reliability of reasoning produced by large language models (LLMs) and agentic systems.</p> <p>RIS levels classify systems based on measurable reasoning behavior, not model size, architecture, vendor, or training methodology. A system\u2019s RIS level reflects its demonstrated ability to maintain reasoning integrity under controlled conditions, repeated evaluations, and operational scenarios.</p>"},{"location":"ris_v1_0/#42-level-definitions","title":"4.2 Level Definitions","text":"<p>RIS defines the following reasoning integrity maturity levels:</p> <ul> <li>RIS-0: Uncontrolled Reasoning</li> <li>RIS-1: Drift-Sensitive Reasoning</li> <li>RIS-2: Semi-Stable Reasoning</li> <li>RIS-3: Controlled Reasoning</li> <li>RIS-4: High-Integrity Reasoning</li> </ul> <p>Each level includes required properties, performance characteristics, and failure thresholds.</p>"},{"location":"ris_v1_0/#43-ris-0-uncontrolled-reasoning","title":"4.3 RIS-0: Uncontrolled Reasoning","text":""},{"location":"ris_v1_0/#description","title":"Description","text":"<p>RIS-0 systems exhibit unpredictable, highly variable, or unstable reasoning behavior. Reasoning chains diverge significantly across repeated prompts, and semantic coherence is inconsistent or unreliable.</p>"},{"location":"ris_v1_0/#characteristics","title":"Characteristics","text":"<ul> <li>High variance across repeated inferences</li> <li>Unstable or incoherent reasoning chains</li> <li>Susceptibility to minor input or environmental perturbations</li> <li>Unbounded or uncontrolled reasoning state transitions</li> <li>No enforceable variance envelope</li> <li>Reasoning behavior cannot be reliably reproduced</li> </ul>"},{"location":"ris_v1_0/#typical-use-cases","title":"Typical Use Cases","text":"<ul> <li>exploratory research models</li> <li>early-stage prototypes</li> <li>unconstrained generative systems</li> </ul>"},{"location":"ris_v1_0/#conformance","title":"Conformance","text":"<p>RIS-0 indicates non-conformance with all RIS requirements.</p>"},{"location":"ris_v1_0/#44-ris-1-drift-sensitive-reasoning","title":"4.4 RIS-1: Drift-Sensitive Reasoning","text":""},{"location":"ris_v1_0/#description_1","title":"Description","text":"<p>RIS-1 systems demonstrate partial reasoning stability but remain sensitive to drift, perturbations, and repeated evaluations. Structural or semantic divergence occurs frequently.</p>"},{"location":"ris_v1_0/#characteristics_1","title":"Characteristics","text":"<ul> <li>partially stable reasoning patterns under ideal conditions</li> <li>significant drift under repetition or minimal variation</li> <li>inconsistent semantic alignment</li> <li>no enforceable drift boundaries</li> <li>limited reproducibility of reasoning behavior</li> </ul>"},{"location":"ris_v1_0/#typical-use-cases_1","title":"Typical Use Cases","text":"<ul> <li>non-safety-critical applications</li> <li>general-purpose conversational models</li> <li>agent prototypes</li> </ul>"},{"location":"ris_v1_0/#conformance_1","title":"Conformance","text":"<p>RIS-1 indicates minimal partial alignment with RIS controls but does not meet baseline conformance.</p>"},{"location":"ris_v1_0/#45-ris-2-semi-stable-reasoning","title":"4.5 RIS-2: Semi-Stable Reasoning","text":""},{"location":"ris_v1_0/#description_2","title":"Description","text":"<p>RIS-2 systems achieve basic reasoning integrity. Reasoning chains show moderate consistency across repeated prompts and operate within a loosely defined variance range. Drift is detectable but bounded.</p>"},{"location":"ris_v1_0/#characteristics_2","title":"Characteristics","text":"<ul> <li>moderate chain stability</li> <li>acceptable semantic coherence under repetition</li> <li>drift remains within a broad variance envelope</li> <li>occasional reasoning divergence under perturbation</li> <li>reproducible reasoning behavior in controlled cases</li> </ul>"},{"location":"ris_v1_0/#typical-use-cases_2","title":"Typical Use Cases","text":"<ul> <li>consumer-facing AI applications</li> <li>low-risk enterprise workflows</li> <li>model evaluation and benchmarking environments</li> </ul>"},{"location":"ris_v1_0/#conformance_2","title":"Conformance","text":"<p>RIS-2 meets some baseline RIS controls but does not satisfy the requirements for production-grade reasoning integrity.</p>"},{"location":"ris_v1_0/#46-ris-3-controlled-reasoning","title":"4.6 RIS-3: Controlled Reasoning","text":""},{"location":"ris_v1_0/#description_3","title":"Description","text":"<p>RIS-3 systems demonstrate controlled reasoning behavior suitable for production environments. Reasoning chains are stable across repetitions, drift remains within defined boundaries, and semantic coherence is predictably maintained.</p>"},{"location":"ris_v1_0/#characteristics_3","title":"Characteristics","text":"<ul> <li>consistent chain stability across repeated prompts</li> <li>defined and enforceable variance envelope</li> <li>predictable reasoning under controlled perturbations</li> <li>bounded semantic drift and minimal divergence</li> <li>reproducible reasoning state transitions</li> <li>resistance to tool and agent interference within expected parameters</li> </ul>"},{"location":"ris_v1_0/#typical-use-cases_3","title":"Typical Use Cases","text":"<ul> <li>enterprise-grade LLM applications</li> <li>internal decision-support systems</li> <li>workflow automation</li> <li>multi-agent orchestration with defined boundaries</li> </ul>"},{"location":"ris_v1_0/#conformance_3","title":"Conformance","text":"<p>RIS-3 satisfies the majority of RIS requirements and is considered the minimum level appropriate for regulated or production-grade environments.</p>"},{"location":"ris_v1_0/#47-ris-4-high-integrity-reasoning","title":"4.7 RIS-4: High-Integrity Reasoning","text":""},{"location":"ris_v1_0/#description_4","title":"Description","text":"<p>RIS-4 represents the highest level of reasoning integrity. Systems at this level produce stable, predictable, and coherent reasoning patterns with minimal variance across repeated evaluations. Drift is tightly controlled, and reasoning chains maintain structural integrity under both controlled and adversarial conditions.</p>"},{"location":"ris_v1_0/#characteristics_4","title":"Characteristics","text":"<ul> <li>high stability and coherence across all evaluations</li> <li>tightly bounded variance envelope</li> <li>minimal drift under repetition or perturbation</li> <li>predictable reasoning structure even with minor input variations</li> <li>resilience to interference from tools, retrieval components, or agents</li> <li>strong governance boundary adherence</li> <li>reproducible reasoning consistent with defined expectations</li> </ul>"},{"location":"ris_v1_0/#typical-use-cases_4","title":"Typical Use Cases","text":"<ul> <li>safety-critical systems</li> <li>regulated industries requiring auditability</li> <li>financial, medical, legal, and security reasoning workflows</li> <li>autonomous agent frameworks with strict stability demands</li> <li>enterprise systems requiring deterministic reasoning behavior</li> </ul>"},{"location":"ris_v1_0/#conformance_4","title":"Conformance","text":"<p>RIS-4 conforms to all RIS requirements and demonstrates the highest degree of reasoning integrity defined by the standard.</p>"},{"location":"ris_v1_0/#48-level-assignment-requirements","title":"4.8 Level Assignment Requirements","text":"<p>A system\u2019s RIS level MUST be assigned based on:</p> <ul> <li>performance across the RIS measurement framework</li> <li>compliance with required RIS controls</li> <li>observed behavior during evaluation methodology test suites</li> <li>documented evidence demonstrating stability and drift characteristics</li> </ul> <p>Level assignment SHALL be determined from empirical results, not vendor claims or model size.</p>"},{"location":"ris_v1_0/#49-level-validation-and-reassessment","title":"4.9 Level Validation and Reassessment","text":"<p>Systems evaluated for RIS conformance MUST undergo periodic reassessment based on:</p> <ul> <li>operational changes</li> <li>model updates or retraining</li> <li>architectural changes</li> <li>new tool or agent integrations</li> <li>changes in governance boundaries</li> <li>drift trends observed in production</li> </ul> <p>Reassessment MAY elevate or reduce a system\u2019s RIS level depending on observed results.</p>"},{"location":"ris_v1_0/#5-measurement-framework","title":"5. Measurement Framework","text":""},{"location":"ris_v1_0/#51-overview","title":"5.1 Overview","text":"<p>The RIS Measurement Framework defines the quantitative and qualitative methods used to evaluate reasoning integrity. It provides the metrics, data collection procedures, statistical thresholds, and evaluation structures necessary to determine a system\u2019s RIS level.</p> <p>The framework applies to all systems subject to RIS evaluation and SHALL be used consistently across:</p> <ul> <li>baseline assessments</li> <li>production monitoring</li> <li>audit reviews</li> <li>comparative benchmarking</li> </ul> <p>Measurement MUST be based on empirical evidence. Qualitative assessments may supplement measurements but SHALL NOT replace them.</p>"},{"location":"ris_v1_0/#52-measurement-categories","title":"5.2 Measurement Categories","text":"<p>RIS evaluates reasoning integrity using five primary measurement categories:</p> <ol> <li>Chain Stability</li> <li>Semantic Coherence</li> <li>Drift Sensitivity</li> <li>Variance Envelope Compliance</li> <li>Governance Boundary Adherence</li> </ol> <p>All categories MUST be assessed to determine RIS level.</p>"},{"location":"ris_v1_0/#53-required-inputs","title":"5.3 Required Inputs","text":"<p>The following inputs are required for RIS-compliant measurement:</p> <ul> <li>repeated inference samples from identical prompts</li> <li>controlled perturbation samples (minimal input variations)</li> <li>environmental invariants (temperature, sampling parameters)</li> <li>contextual invariants (system instructions, agent state)</li> <li>tool or retrieval call responses (if applicable)</li> <li>logs or ledger entries representing reasoning behavior</li> </ul> <p>Implementations SHOULD collect evaluation data under isolated and consistent conditions.</p>"},{"location":"ris_v1_0/#54-sample-size-requirements","title":"5.4 Sample Size Requirements","text":"<p>A compliant RIS evaluation MUST include:</p> <ul> <li>a minimum of 25 repeated inference samples per test prompt</li> <li>a minimum of 10 controlled perturbation samples per prompt</li> <li>at least 50 total prompts across diverse categories for generalized models</li> <li>at least 20 prompts for domain-specific or task-specific systems</li> </ul> <p>Larger sample sizes MAY be used but SHALL be documented.</p>"},{"location":"ris_v1_0/#55-chain-stability-metric","title":"5.5 Chain Stability Metric","text":""},{"location":"ris_v1_0/#551-definition","title":"5.5.1 Definition","text":"<p>Chain stability measures the consistency of a system\u2019s reasoning structure across repeated inference cycles.</p>"},{"location":"ris_v1_0/#552-measurement-method","title":"5.5.2 Measurement Method","text":"<p>Stability is measured using:</p> <ul> <li>structural similarity metrics</li> <li>step-to-step coherence comparisons</li> <li>reasoning-length variance</li> <li>transition-alignment scoring</li> </ul> <p>A system MUST maintain chain similarity within its defined variance envelope.</p>"},{"location":"ris_v1_0/#553-scoring","title":"5.5.3 Scoring","text":"<p>Chain stability is scored from 0.00 to 1.00.</p> <p>Example thresholds:</p> <ul> <li>0.85\u20131.00: stable</li> <li>0.65\u20130.84: semi-stable</li> <li>below 0.65: unstable</li> </ul> <p>RIS-4 systems SHALL achieve stability above 0.90 for all core prompts.</p>"},{"location":"ris_v1_0/#56-semantic-coherence-metric","title":"5.6 Semantic Coherence Metric","text":""},{"location":"ris_v1_0/#561-definition","title":"5.6.1 Definition","text":"<p>Semantic coherence measures the consistency of meaning and conceptual alignment across repeated inferences.</p>"},{"location":"ris_v1_0/#562-measurement-method","title":"5.6.2 Measurement Method","text":"<p>Coherence is evaluated based on:</p> <ul> <li>semantic similarity scoring</li> <li>topic retention</li> <li>conceptual transition consistency</li> <li>negation or contradiction detection</li> </ul>"},{"location":"ris_v1_0/#563-scoring","title":"5.6.3 Scoring","text":"<p>Scoring follows the same 0.00\u20131.00 scale.</p> <p>RIS-4 systems SHALL demonstrate coherence above 0.90.</p>"},{"location":"ris_v1_0/#57-drift-sensitivity-metric","title":"5.7 Drift Sensitivity Metric","text":""},{"location":"ris_v1_0/#571-definition","title":"5.7.1 Definition","text":"<p>Drift sensitivity measures the degree to which a system deviates from baseline reasoning behavior under repeated evaluations or minimal perturbations.</p>"},{"location":"ris_v1_0/#572-measurement-method","title":"5.7.2 Measurement Method","text":"<p>Drift is assessed by:</p> <ul> <li>analyzing variance outside baseline patterns</li> <li>measuring semantic or structural deviation trends</li> <li>quantifying divergence under perturbation conditions</li> </ul>"},{"location":"ris_v1_0/#573-scoring","title":"5.7.3 Scoring","text":"<p>Drift sensitivity is scored inversely (lower is better):</p> <ul> <li>0.00\u20130.10: low drift</li> <li>0.11\u20130.25: moderate drift</li> <li>above 0.25: high drift</li> </ul> <p>RIS-4 systems SHALL maintain drift sensitivity below 0.10.</p>"},{"location":"ris_v1_0/#58-variance-envelope-compliance","title":"5.8 Variance Envelope Compliance","text":""},{"location":"ris_v1_0/#581-definition","title":"5.8.1 Definition","text":"<p>The variance envelope represents the acceptable range of variation in reasoning behavior.</p>"},{"location":"ris_v1_0/#582-measurement-method","title":"5.8.2 Measurement Method","text":"<p>Variance envelope compliance is determined by:</p> <ul> <li>repeated inference testing</li> <li>perturbation evaluations</li> <li>longitudinal measurement</li> <li>stability under load or repeated requests</li> </ul>"},{"location":"ris_v1_0/#583-scoring","title":"5.8.3 Scoring","text":"<p>A system is considered compliant if:</p> <ul> <li>at least 95 percent of evaluations fall within its defined envelope</li> </ul> <p>RIS-4 systems SHALL achieve at least 98 percent compliance.</p>"},{"location":"ris_v1_0/#59-governance-boundary-adherence","title":"5.9 Governance Boundary Adherence","text":""},{"location":"ris_v1_0/#591-definition","title":"5.9.1 Definition","text":"<p>Governance boundaries constrain the context, semantic scope, and operational state the model may use for reasoning.</p>"},{"location":"ris_v1_0/#592-measurement-method","title":"5.9.2 Measurement Method","text":"<p>Boundary adherence is measured by observing:</p> <ul> <li>context window expansion attempts</li> <li>unauthorized semantic domain expansion</li> <li>tool or agent influence patterns</li> <li>cross-task contamination or memory leakage</li> </ul>"},{"location":"ris_v1_0/#593-scoring","title":"5.9.3 Scoring","text":"<p>Boundary violations are scored as:</p> <ul> <li>0: no violations</li> <li>1: one minor violation</li> <li>2: repeated or significant violations</li> </ul> <p>RIS-4 systems SHALL record zero violations under all evaluation conditions.</p>"},{"location":"ris_v1_0/#510-composite-reasoning-integrity-score","title":"5.10 Composite Reasoning Integrity Score","text":"<p>RIS defines an aggregate score calculated using weighted metrics:</p> <ul> <li>Chain Stability: 30 percent</li> <li>Semantic Coherence: 25 percent</li> <li>Drift Sensitivity: 20 percent</li> <li>Variance Envelope Compliance: 15 percent</li> <li>Governance Boundary Adherence: 10 percent</li> </ul> <p>Composite scores determine candidate RIS levels but do not replace conformance requirements.</p>"},{"location":"ris_v1_0/#511-interpretation-of-scores","title":"5.11 Interpretation of Scores","text":"<p>Composite score ranges:</p> <ul> <li>0.00\u20130.40: RIS-0</li> <li>0.41\u20130.60: RIS-1</li> <li>0.61\u20130.75: RIS-2</li> <li>0.76\u20130.89: RIS-3</li> <li>0.90\u20131.00: RIS-4</li> </ul> <p>A system MUST meet both:</p> <ul> <li>composite scoring thresholds</li> <li>all mandatory controls applicable to its level</li> </ul> <p>to be classified at that RIS level.</p>"},{"location":"ris_v1_0/#6-ris-controls","title":"6. RIS Controls","text":""},{"location":"ris_v1_0/#61-overview","title":"6.1 Overview","text":"<p>RIS controls define the mandatory and recommended requirements that systems MUST meet to achieve compliance with RIS levels RIS-1 through RIS-4. Controls are grouped into control families that govern stability, coherence, drift, variance, boundary adherence, and operational integrity.</p> <p>Each control includes:</p> <ul> <li>Control Objective</li> <li>Control Requirement (MUST / SHALL)</li> <li>Implementation Guidance (informative)</li> <li>Evidence Required (for audits)</li> <li>Conformance Criteria</li> </ul> <p>All systems seeking RIS classification SHALL be evaluated against these controls.</p>"},{"location":"ris_v1_0/#62-control-families","title":"6.2 Control Families","text":"<p>RIS defines six control families:</p> <ol> <li>RS \u2013 Chain Stability Controls  </li> <li>SC \u2013 Semantic Coherence Controls  </li> <li>DR \u2013 Drift Resistance Controls  </li> <li>VE \u2013 Variance Envelope Controls  </li> <li>GB \u2013 Governance Boundary Controls  </li> <li>OP \u2013 Operational Integrity Controls  </li> </ol> <p>Controls within each family are numbered sequentially.</p>"},{"location":"ris_v1_0/#63-chain-stability-controls-rs","title":"6.3 Chain Stability Controls (RS)","text":""},{"location":"ris_v1_0/#rs-1-repetition-consistency","title":"RS-1: Repetition Consistency","text":""},{"location":"ris_v1_0/#objective","title":"Objective","text":"<p>Ensure reasoning chains remain consistent across repeated inference cycles.</p>"},{"location":"ris_v1_0/#requirement","title":"Requirement","text":"<p>The system SHALL produce reasoning outputs whose structural similarity remains within the defined variance envelope across repeated evaluations of identical prompts.</p>"},{"location":"ris_v1_0/#implementation-guidance","title":"Implementation Guidance","text":"<ul> <li>Use repeated inference tests.</li> <li>Monitor reasoning structure length and transitions.</li> <li>Track deviations across a defined sample size.</li> </ul>"},{"location":"ris_v1_0/#evidence-required","title":"Evidence Required","text":"<ul> <li>stability reports  </li> <li>similarity metrics  </li> <li>repeated inference samples</li> </ul>"},{"location":"ris_v1_0/#conformance_5","title":"Conformance","text":"<p>Required for RIS-2, RIS-3, and RIS-4.</p>"},{"location":"ris_v1_0/#rs-2-perturbation-stability","title":"RS-2: Perturbation Stability","text":""},{"location":"ris_v1_0/#objective_1","title":"Objective","text":"<p>Ensure stability under minor, controlled input variations.</p>"},{"location":"ris_v1_0/#requirement_1","title":"Requirement","text":"<p>The system SHALL maintain reasoning structure stability when presented with minimal, semantically equivalent perturbations of the same prompt.</p>"},{"location":"ris_v1_0/#implementation-guidance_1","title":"Implementation Guidance","text":"<ul> <li>Use controlled perturbation prompts (synonym replacement, minor rephrasing, neutral variations).</li> <li>Measure structural drift across perturbation samples.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_1","title":"Evidence Required","text":"<ul> <li>perturbation test logs  </li> <li>drift measurements  </li> <li>variance envelope comparison</li> </ul>"},{"location":"ris_v1_0/#conformance_6","title":"Conformance","text":"<p>Required for RIS-3 and RIS-4.</p>"},{"location":"ris_v1_0/#rs-3-transition-alignment","title":"RS-3: Transition Alignment","text":""},{"location":"ris_v1_0/#objective_2","title":"Objective","text":"<p>Ensure internal reasoning transitions follow consistent logical patterns.</p>"},{"location":"ris_v1_0/#requirement_2","title":"Requirement","text":"<p>The system SHALL maintain alignment of step-to-step transitions across repeated inference cycles.</p>"},{"location":"ris_v1_0/#implementation-guidance_2","title":"Implementation Guidance","text":"<ul> <li>Compare reasoning step ordering or inferred conceptual transitions.</li> <li>Identify anomalous reordering indicative of instability.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_2","title":"Evidence Required","text":"<ul> <li>transition-level similarity reports</li> </ul>"},{"location":"ris_v1_0/#conformance_7","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"ris_v1_0/#64-semantic-coherence-controls-sc","title":"6.4 Semantic Coherence Controls (SC)","text":""},{"location":"ris_v1_0/#sc-1-semantic-alignment","title":"SC-1: Semantic Alignment","text":""},{"location":"ris_v1_0/#objective_3","title":"Objective","text":"<p>Ensure that reasoning outputs remain semantically consistent across repeated evaluations.</p>"},{"location":"ris_v1_0/#requirement_3","title":"Requirement","text":"<p>The system SHALL maintain semantic similarity across repeated inferences meeting or exceeding the thresholds defined in the measurement framework.</p>"},{"location":"ris_v1_0/#implementation-guidance_3","title":"Implementation Guidance","text":"<ul> <li>Use embedding-based similarity scoring.</li> <li>Detect contradictory or negated reasoning shifts.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_3","title":"Evidence Required","text":"<ul> <li>semantic similarity analysis</li> <li>contradiction or negation detection logs</li> </ul>"},{"location":"ris_v1_0/#conformance_8","title":"Conformance","text":"<p>Required for RIS-2, RIS-3, and RIS-4.</p>"},{"location":"ris_v1_0/#sc-2-concept-retention","title":"SC-2: Concept Retention","text":""},{"location":"ris_v1_0/#objective_4","title":"Objective","text":"<p>Ensure conceptual themes persist across reasoning processes.</p>"},{"location":"ris_v1_0/#requirement_4","title":"Requirement","text":"<p>The system SHALL retain core conceptual structures across evaluations unless explicitly instructed to change.</p>"},{"location":"ris_v1_0/#implementation-guidance_4","title":"Implementation Guidance","text":"<ul> <li>Analyze topic retention through semantic clustering.</li> <li>Evaluate for unexpected topic divergence.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_4","title":"Evidence Required","text":"<ul> <li>topic coherence reports  </li> <li>clustering analysis results</li> </ul>"},{"location":"ris_v1_0/#conformance_9","title":"Conformance","text":"<p>Required for RIS-3 and RIS-4.</p>"},{"location":"ris_v1_0/#sc-3-logical-consistency","title":"SC-3: Logical Consistency","text":""},{"location":"ris_v1_0/#objective_5","title":"Objective","text":"<p>Ensure logical validity and structure remain consistent.</p>"},{"location":"ris_v1_0/#requirement_5","title":"Requirement","text":"<p>The system SHALL avoid contradictory intermediate reasoning steps within an evaluation set.</p>"},{"location":"ris_v1_0/#implementation-guidance_5","title":"Implementation Guidance","text":"<ul> <li>Evaluate internal reasoning structure for contradictions.</li> <li>Use logical consistency scoring where available.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_5","title":"Evidence Required","text":"<ul> <li>logical consistency assessments</li> </ul>"},{"location":"ris_v1_0/#conformance_10","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"ris_v1_0/#65-drift-resistance-controls-dr","title":"6.5 Drift Resistance Controls (DR)","text":""},{"location":"ris_v1_0/#dr-1-drift-monitoring","title":"DR-1: Drift Monitoring","text":""},{"location":"ris_v1_0/#objective_6","title":"Objective","text":"<p>Establish continuous monitoring for reasoning drift.</p>"},{"location":"ris_v1_0/#requirement_6","title":"Requirement","text":"<p>The system SHALL implement monitoring to detect drift exceeding defined thresholds over repeated inference cycles.</p>"},{"location":"ris_v1_0/#implementation-guidance_6","title":"Implementation Guidance","text":"<ul> <li>Compare baseline outputs against current samples.</li> <li>Establish early-warning indicators for drift escalation.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_6","title":"Evidence Required","text":"<ul> <li>drift baseline logs  </li> <li>drift trend reports</li> </ul>"},{"location":"ris_v1_0/#conformance_11","title":"Conformance","text":"<p>Required for RIS-2, RIS-3, and RIS-4.</p>"},{"location":"ris_v1_0/#dr-2-drift-envelope-enforcement","title":"DR-2: Drift Envelope Enforcement","text":""},{"location":"ris_v1_0/#objective_7","title":"Objective","text":"<p>Maintain reasoning behavior within allowable drift boundaries.</p>"},{"location":"ris_v1_0/#requirement_7","title":"Requirement","text":"<p>The system SHALL enforce a drift envelope that constrains allowable deviation from baseline behavior.</p>"},{"location":"ris_v1_0/#implementation-guidance_7","title":"Implementation Guidance","text":"<ul> <li>Use variance and drift sensitivity metrics.</li> <li>Define acceptable drift bands based on use case.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_7","title":"Evidence Required","text":"<ul> <li>drift envelope documentation  </li> <li>variance compliance reports</li> </ul>"},{"location":"ris_v1_0/#conformance_12","title":"Conformance","text":"<p>Required for RIS-3 and RIS-4.</p>"},{"location":"ris_v1_0/#dr-3-drift-recovery","title":"DR-3: Drift Recovery","text":""},{"location":"ris_v1_0/#objective_8","title":"Objective","text":"<p>Ensure recovery mechanisms exist for restoring reasoning integrity.</p>"},{"location":"ris_v1_0/#requirement_8","title":"Requirement","text":"<p>The system SHALL provide mechanisms to reinitialize or reset reasoning state when drift exceeds permitted thresholds.</p>"},{"location":"ris_v1_0/#implementation-guidance_8","title":"Implementation Guidance","text":"<ul> <li>Reset context or agent state.</li> <li>Refresh memory or tool caches.</li> <li>Re-evaluate under controlled conditions.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_8","title":"Evidence Required","text":"<ul> <li>drift recovery logs  </li> <li>anomaly detection events</li> </ul>"},{"location":"ris_v1_0/#conformance_13","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"ris_v1_0/#66-variance-envelope-controls-ve","title":"6.6 Variance Envelope Controls (VE)","text":""},{"location":"ris_v1_0/#ve-1-variance-definition","title":"VE-1: Variance Definition","text":""},{"location":"ris_v1_0/#objective_9","title":"Objective","text":"<p>Define acceptable variance for reasoning behavior.</p>"},{"location":"ris_v1_0/#requirement_9","title":"Requirement","text":"<p>The system SHALL define a variance envelope specifying acceptable deviation thresholds across repeated evaluations.</p>"},{"location":"ris_v1_0/#implementation-guidance_9","title":"Implementation Guidance","text":"<ul> <li>Document variance thresholds.</li> <li>Derive thresholds from benchmarking or domain requirements.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_9","title":"Evidence Required","text":"<ul> <li>variance envelope documentation</li> </ul>"},{"location":"ris_v1_0/#conformance_14","title":"Conformance","text":"<p>Required for RIS-2, RIS-3, and RIS-4.</p>"},{"location":"ris_v1_0/#ve-2-variance-compliance","title":"VE-2: Variance Compliance","text":""},{"location":"ris_v1_0/#objective_10","title":"Objective","text":"<p>Ensure reasoning outputs remain within defined variance bounds.</p>"},{"location":"ris_v1_0/#requirement_10","title":"Requirement","text":"<p>The system SHALL maintain variance compliance for at least 95 percent of repeated evaluations.</p>"},{"location":"ris_v1_0/#implementation-guidance_10","title":"Implementation Guidance","text":"<ul> <li>Use repeated inference tests.</li> <li>Monitor patterns of deviation.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_10","title":"Evidence Required","text":"<ul> <li>compliance reports  </li> <li>exception logs</li> </ul>"},{"location":"ris_v1_0/#conformance_15","title":"Conformance","text":"<p>Required for RIS-3 and RIS-4.</p>"},{"location":"ris_v1_0/#ve-3-variance-tightening","title":"VE-3: Variance Tightening","text":""},{"location":"ris_v1_0/#objective_11","title":"Objective","text":"<p>Tighten variance thresholds for high-integrity reasoning.</p>"},{"location":"ris_v1_0/#requirement_11","title":"Requirement","text":"<p>The system SHALL maintain variance compliance at rates exceeding 98 percent for RIS-4.</p>"},{"location":"ris_v1_0/#implementation-guidance_11","title":"Implementation Guidance","text":"<ul> <li>Use stricter similarity or coherence scoring.</li> <li>Implement adaptive variance tightening.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_11","title":"Evidence Required","text":"<ul> <li>high-precision variance reports</li> </ul>"},{"location":"ris_v1_0/#conformance_16","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"ris_v1_0/#67-governance-boundary-controls-gb","title":"6.7 Governance Boundary Controls (GB)","text":""},{"location":"ris_v1_0/#gb-1-boundary-definition","title":"GB-1: Boundary Definition","text":""},{"location":"ris_v1_0/#objective_12","title":"Objective","text":"<p>Define explicit reasoning boundaries to prevent uncontrolled contextual expansion.</p>"},{"location":"ris_v1_0/#requirement_12","title":"Requirement","text":"<p>The system SHALL define governance boundaries that constrain contextual, semantic, or operational state during reasoning.</p>"},{"location":"ris_v1_0/#implementation-guidance_12","title":"Implementation Guidance","text":"<ul> <li>Define allowable context limits.</li> <li>Identify prohibited expansions or semantic domains.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_12","title":"Evidence Required","text":"<ul> <li>governance boundary documentation</li> </ul>"},{"location":"ris_v1_0/#conformance_17","title":"Conformance","text":"<p>Required for RIS-2, RIS-3, and RIS-4.</p>"},{"location":"ris_v1_0/#gb-2-boundary-enforcement","title":"GB-2: Boundary Enforcement","text":""},{"location":"ris_v1_0/#objective_13","title":"Objective","text":"<p>Ensure reasoning remains within defined boundaries.</p>"},{"location":"ris_v1_0/#requirement_13","title":"Requirement","text":"<p>The system SHALL enforce boundaries during inference and detect violations.</p>"},{"location":"ris_v1_0/#implementation-guidance_13","title":"Implementation Guidance","text":"<ul> <li>Use contextual or semantic boundary checks.</li> <li>Restrict access to tools or memory beyond defined limits.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_13","title":"Evidence Required","text":"<ul> <li>boundary enforcement logs  </li> <li>violation reports</li> </ul>"},{"location":"ris_v1_0/#conformance_18","title":"Conformance","text":"<p>Required for RIS-3 and RIS-4.</p>"},{"location":"ris_v1_0/#gb-3-interference-protection","title":"GB-3: Interference Protection","text":""},{"location":"ris_v1_0/#objective_14","title":"Objective","text":"<p>Prevent uncontrolled influence from tools, agents, or retrieval components.</p>"},{"location":"ris_v1_0/#requirement_14","title":"Requirement","text":"<p>The system SHALL detect and mitigate reasoning interference from tool responses or agent interactions.</p>"},{"location":"ris_v1_0/#implementation-guidance_14","title":"Implementation Guidance","text":"<ul> <li>Validate tool outputs before integration.</li> <li>Isolate reasoning states across agents.</li> </ul>"},{"location":"ris_v1_0/#evidence-required_14","title":"Evidence Required","text":"<ul> <li>interference detection logs  </li> <li>impact assessments</li> </ul>"},{"location":"ris_v1_0/#conformance_19","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"ris_v1_0/#68-operational-integrity-controls-op","title":"6.8 Operational Integrity Controls (OP)","text":""},{"location":"ris_v1_0/#op-1-evaluation-environment-consistency","title":"OP-1: Evaluation Environment Consistency","text":""},{"location":"ris_v1_0/#objective_15","title":"Objective","text":"<p>Ensure evaluation environments remain stable and controlled.</p>"},{"location":"ris_v1_0/#requirement_15","title":"Requirement","text":"<p>The system SHALL maintain consistent parameters (temperature, sampling settings, context) during evaluation.</p>"},{"location":"ris_v1_0/#evidence-required_15","title":"Evidence Required","text":"<ul> <li>environment configuration logs</li> </ul>"},{"location":"ris_v1_0/#conformance_20","title":"Conformance","text":"<p>Required for all RIS levels beyond RIS-0.</p>"},{"location":"ris_v1_0/#op-2-reproducible-testing","title":"OP-2: Reproducible Testing","text":""},{"location":"ris_v1_0/#objective_16","title":"Objective","text":"<p>Ensure evaluation results can be reproduced.</p>"},{"location":"ris_v1_0/#requirement_16","title":"Requirement","text":"<p>The system SHALL support reproducible testing conditions and retain configuration details for audit purposes.</p>"},{"location":"ris_v1_0/#evidence-required_16","title":"Evidence Required","text":"<ul> <li>reproducibility reports  </li> <li>configuration snapshots</li> </ul>"},{"location":"ris_v1_0/#conformance_21","title":"Conformance","text":"<p>Required for RIS-3 and RIS-4.</p>"},{"location":"ris_v1_0/#op-3-audit-logging","title":"OP-3: Audit Logging","text":""},{"location":"ris_v1_0/#objective_17","title":"Objective","text":"<p>Ensure auditable logs for evaluation and monitoring.</p>"},{"location":"ris_v1_0/#requirement_17","title":"Requirement","text":"<p>The system SHALL generate logs documenting reasoning behavior, drift events, variance compliance, and boundary violations.</p>"},{"location":"ris_v1_0/#evidence-required_17","title":"Evidence Required","text":"<ul> <li>audit logs  </li> <li>ledger entries  </li> <li>summary reports</li> </ul>"},{"location":"ris_v1_0/#conformance_22","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"ris_v1_0/#op-4-longitudinal-monitoring","title":"OP-4: Longitudinal Monitoring","text":""},{"location":"ris_v1_0/#objective_18","title":"Objective","text":"<p>Track reasoning behavior over time.</p>"},{"location":"ris_v1_0/#requirement_18","title":"Requirement","text":"<p>The system SHALL support longitudinal monitoring to detect degradation or emerging instability.</p>"},{"location":"ris_v1_0/#evidence-required_18","title":"Evidence Required","text":"<ul> <li>multi-interval reports  </li> <li>trend analyses</li> </ul>"},{"location":"ris_v1_0/#conformance_23","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"ris_v1_0/#69-summary-of-conformance-requirements-by-ris-level","title":"6.9 Summary of Conformance Requirements by RIS Level","text":"Control Family RIS-1 RIS-2 RIS-3 RIS-4 RS (Stability) Optional Required Required Required (strict) SC (Coherence) Optional Required Required Required (strict) DR (Drift) Optional Required Required Required (strict) VE (Variance) Optional Required Required Required (tight) GB (Boundary) Optional Required Required Required (strict) OP (Operational) Minimal Moderate Strong Full (audit-ready)"},{"location":"ris_v1_0/#610-control-interpretation","title":"6.10 Control Interpretation","text":"<p>Control interpretation SHALL comply with the following:</p> <ul> <li>MUST and SHALL controls are required for conformance.</li> <li>SHOULD controls are recommended but not required unless explicitly tied to a level.</li> <li>MAY controls provide flexibility but do not affect conformance.</li> </ul>"},{"location":"ris_v1_0/#7-scoring-and-conformance","title":"7. Scoring and Conformance","text":""},{"location":"ris_v1_0/#71-overview","title":"7.1 Overview","text":"<p>This section defines the scoring methodology and conformance criteria used to determine a system\u2019s RIS level. Scoring integrates quantitative metrics from the measurement framework with mandatory control requirements. A system\u2019s RIS classification is based on both:</p> <ol> <li>composite reasoning integrity score, and  </li> <li>demonstrated compliance with required RIS controls.</li> </ol> <p>A system SHALL NOT be assigned a RIS level solely based on scoring metrics without meeting corresponding control requirements.</p>"},{"location":"ris_v1_0/#72-composite-reasoning-integrity-score","title":"7.2 Composite Reasoning Integrity Score","text":"<p>The composite score is calculated using weighted metrics defined in Section 5:</p> <ul> <li>Chain Stability: 30 percent  </li> <li>Semantic Coherence: 25 percent  </li> <li>Drift Sensitivity: 20 percent  </li> <li>Variance Envelope Compliance: 15 percent  </li> <li>Governance Boundary Adherence: 10 percent  </li> </ul> <p>Each metric yields a value between 0.00 and 1.00. The weighted sum produces the final composite reasoning integrity score.</p> <p>The composite score is used to determine eligibility for a RIS level, but SHALL NOT be used to determine final classification without control verification.</p>"},{"location":"ris_v1_0/#73-metric-weighting","title":"7.3 Metric Weighting","text":"<p>The rationale for metric weighting is as follows:</p> <ul> <li>Chain Stability (30 percent): foundational indicator of structural reasoning reliability.  </li> <li>Semantic Coherence (25 percent): ensures reasoning maintains consistent meaning.  </li> <li>Drift Sensitivity (20 percent): predicts long-term stability under repetition.  </li> <li>Variance Envelope Compliance (15 percent): ensures bounded divergence.  </li> <li>Governance Boundary Adherence (10 percent): required for operational trust and safety.</li> </ul> <p>These weights SHALL remain consistent across RIS versions unless updated in a future amendment.</p>"},{"location":"ris_v1_0/#74-eligibility-thresholds-for-ris-levels","title":"7.4 Eligibility Thresholds for RIS Levels","text":"<p>Eligibility thresholds based on composite score:</p> <ul> <li>0.00\u20130.40: RIS-0  </li> <li>0.41\u20130.60: RIS-1  </li> <li>0.61\u20130.75: RIS-2  </li> <li>0.76\u20130.89: RIS-3  </li> <li>0.90\u20131.00: RIS-4  </li> </ul> <p>Meeting the scoring threshold does not guarantee assignment to that RIS level. The system SHALL also satisfy the control requirements designated for that level.</p>"},{"location":"ris_v1_0/#75-control-based-conformance-requirements","title":"7.5 Control-Based Conformance Requirements","text":"<p>The following table defines minimum control conformance for each RIS level:</p> RIS Level Required Controls RIS-0 No controls required RIS-1 Optional controls only RIS-2 All RS-1, SC-1, DR-1, VE-1, GB-1 controls RIS-3 All RIS-2 controls plus RS-2, SC-2, DR-2, VE-2, GB-2, OP-2 RIS-4 All RIS-3 controls plus RS-3, SC-3, DR-3, VE-3, GB-3, OP-3, OP-4 <p>A system SHALL NOT be classified above RIS-2 unless all mandatory controls for RIS-3 are satisfied. A system SHALL NOT be classified above RIS-3 unless all mandatory controls for RIS-4 are satisfied.</p>"},{"location":"ris_v1_0/#76-evidence-requirements","title":"7.6 Evidence Requirements","text":"<p>To support a RIS classification, the following evidence MUST be produced:</p> <ol> <li>Measurement Framework Results  </li> <li>repeated inference sample outputs  </li> <li>controlled perturbation outputs  </li> <li>variance compliance reports  </li> <li> <p>drift sensitivity analyses  </p> </li> <li> <p>Control Implementation Evidence  </p> </li> <li>documentation of stability, coherence, drift, variance, and boundary controls  </li> <li>operational integrity documentation  </li> <li> <p>governance boundary definitions  </p> </li> <li> <p>Audit Logs  </p> </li> <li>reasoning ledger entries or equivalent reports  </li> <li>evaluation logs  </li> <li> <p>anomaly and boundary violation logs  </p> </li> <li> <p>Configuration Snapshots  </p> </li> <li>inference parameters  </li> <li>environmental settings  </li> <li>prompt templates  </li> <li>tool access or agent configuration  </li> </ol> <p>The evaluator SHALL verify all evidence before assigning a RIS level.</p>"},{"location":"ris_v1_0/#77-conformance-determination-process","title":"7.7 Conformance Determination Process","text":"<p>A system SHALL undergo the following steps to determine conformance:</p> <ol> <li>Preparation  </li> <li>define evaluation conditions  </li> <li>document configuration  </li> <li> <p>establish invariants  </p> </li> <li> <p>Data Collection  </p> </li> <li>perform repeated inference sampling  </li> <li>perform controlled perturbation sampling  </li> <li> <p>log reasoning behavior  </p> </li> <li> <p>Metric Calculation  </p> </li> <li>compute each metric independently  </li> <li> <p>calculate composite reasoning integrity score  </p> </li> <li> <p>Control Verification  </p> </li> <li>confirm implementation and operation of required controls  </li> <li> <p>identify gaps or violations  </p> </li> <li> <p>Level Assignment  </p> </li> <li>determine eligible RIS level based on composite score  </li> <li>determine achievable RIS level based on control compliance  </li> <li> <p>final level SHALL be the lower of the two  </p> </li> <li> <p>Report Generation  </p> </li> <li>produce a formal evaluation report  </li> <li> <p>document findings, score, control status, and assignment  </p> </li> <li> <p>Certification  </p> </li> <li>issue a RIS Conformance Statement  </li> <li>assign classification validity period  </li> </ol>"},{"location":"ris_v1_0/#78-handling-of-violations-and-exceptions","title":"7.8 Handling of Violations and Exceptions","text":"<p>If a system violates a mandatory control during evaluation:</p> <ul> <li>it SHALL be classified at the highest RIS level for which all mandatory controls are satisfied  </li> <li>violations MUST be documented with supporting evidence  </li> <li>systems MAY undergo remediation and reassessment  </li> </ul> <p>Exceptions for SHOULD and MAY controls SHALL NOT affect level assignment if mandatory controls are met.</p>"},{"location":"ris_v1_0/#79-reassessment-requirements","title":"7.9 Reassessment Requirements","text":"<p>RIS conformance SHALL be reassessed when:</p> <ul> <li>the underlying model is updated  </li> <li>training data or fine-tuning changes  </li> <li>new tools or agents are added  </li> <li>governance boundaries change  </li> <li>drift or instability trends are detected in production  </li> <li>a significant degradation in metrics occurs  </li> <li>system behavior changes under load  </li> </ul> <p>Reassessment MAY be performed more frequently based on organizational or regulatory requirements.</p>"},{"location":"ris_v1_0/#710-validity-of-ris-classification","title":"7.10 Validity of RIS Classification","text":"<p>RIS classification is valid for a period of:</p> <ul> <li>12 months for general LLM systems  </li> <li>6 months for agentic or tool-integrated systems  </li> <li>3 months for safety-critical systems  </li> </ul> <p>After expiration, the system SHALL be reevaluated to maintain a RIS classification.</p>"},{"location":"ris_v1_0/#711-publication-and-reporting","title":"7.11 Publication and Reporting","text":"<p>Organizations MAY publish:</p> <ul> <li>their RIS level  </li> <li>their composite score  </li> <li>high-level findings  </li> </ul> <p>However, detailed logs, inference samples, and proprietary evaluation data SHOULD only be disclosed under NDA or regulatory requirement.</p> <p>RIS conformance SHALL NOT be claimed without formal assessment.</p>"},{"location":"ris_v1_0/#8-risk-models","title":"8. Risk Models","text":""},{"location":"ris_v1_0/#81-overview","title":"8.1 Overview","text":"<p>This section defines the risk models used to evaluate, categorize, and mitigate threats to reasoning integrity in LLMs and agentic systems. RIS identifies reasoning risks that impact stability, coherence, predictability, and boundary adherence. Risk models are used to:</p> <ul> <li>guide evaluation methodology</li> <li>inform control requirements</li> <li>support audit and governance decisions</li> <li>predict failure conditions</li> <li>align reasoning-system behavior with organizational risk tolerance</li> </ul> <p>RIS risk models are independent of data privacy, security, ethical, or regulatory risk. They focus exclusively on reasoning integrity.</p>"},{"location":"ris_v1_0/#82-categories-of-reasoning-risk","title":"8.2 Categories of Reasoning Risk","text":"<p>RIS defines six primary categories of reasoning risk:</p> <ol> <li>Structural Drift Risk  </li> <li>Semantic Drift Risk  </li> <li>Temporal Stability Risk  </li> <li>Interference Risk  </li> <li>Boundary Violation Risk  </li> <li>Degradation Risk  </li> </ol> <p>Each risk category is described in the following sections.</p>"},{"location":"ris_v1_0/#83-structural-drift-risk","title":"8.3 Structural Drift Risk","text":""},{"location":"ris_v1_0/#831-description","title":"8.3.1 Description","text":"<p>Structural drift occurs when the underlying structure, sequence, or transitions of reasoning chains changes beyond acceptable variance thresholds.</p>"},{"location":"ris_v1_0/#832-causes","title":"8.3.2 Causes","text":"<ul> <li>sampling randomness beyond expected levels  </li> <li>sensitivity to minor prompt variations  </li> <li>instability in internal representations  </li> <li>tool or agent load effects  </li> <li>model configuration fluctuations  </li> </ul>"},{"location":"ris_v1_0/#833-indicators","title":"8.3.3 Indicators","text":"<ul> <li>inconsistent step ordering  </li> <li>fluctuating reasoning length  </li> <li>divergent intermediate transitions  </li> <li>unexplained reorganization of reasoning steps  </li> </ul>"},{"location":"ris_v1_0/#834-impact","title":"8.3.4 Impact","text":"<p>Structural drift undermines reproducibility and increases the likelihood of unpredictable system behavior.</p>"},{"location":"ris_v1_0/#835-mitigation","title":"8.3.5 Mitigation","text":"<ul> <li>enforce variance envelope  </li> <li>enforce chain stability controls  </li> <li>reset or reinitialize agent state when drift is detected  </li> <li>maintain consistent inference parameters  </li> </ul>"},{"location":"ris_v1_0/#84-semantic-drift-risk","title":"8.4 Semantic Drift Risk","text":""},{"location":"ris_v1_0/#841-description","title":"8.4.1 Description","text":"<p>Semantic drift arises when the meaning, conceptual alignment, or topic structure shifts unpredictably during or across reasoning cycles.</p>"},{"location":"ris_v1_0/#842-causes","title":"8.4.2 Causes","text":"<ul> <li>conceptual instability under repetition  </li> <li>semantic domain expansion  </li> <li>interference from retrieval or tools  </li> <li>failure to retain core conceptual anchors  </li> </ul>"},{"location":"ris_v1_0/#843-indicators","title":"8.4.3 Indicators","text":"<ul> <li>loss of topic relevance  </li> <li>contradictory intermediate steps  </li> <li>unintended meaning shifts  </li> <li>negation or inversion anomalies  </li> </ul>"},{"location":"ris_v1_0/#844-impact","title":"8.4.4 Impact","text":"<p>Semantic drift reduces coherence, predictability, and trustworthiness of reasoning outputs.</p>"},{"location":"ris_v1_0/#845-mitigation","title":"8.4.5 Mitigation","text":"<ul> <li>semantic coherence controls  </li> <li>conceptual retention verification  </li> <li>governance boundary enforcement  </li> </ul>"},{"location":"ris_v1_0/#85-temporal-stability-risk","title":"8.5 Temporal Stability Risk","text":""},{"location":"ris_v1_0/#851-description","title":"8.5.1 Description","text":"<p>Temporal stability risk refers to degradation in reasoning quality over time or across extended inference sequences.</p>"},{"location":"ris_v1_0/#852-causes","title":"8.5.2 Causes","text":"<ul> <li>accumulated internal noise  </li> <li>memory contamination  </li> <li>incremental divergence across long chains  </li> <li>resource fluctuations  </li> </ul>"},{"location":"ris_v1_0/#853-indicators","title":"8.5.3 Indicators","text":"<ul> <li>gradual decline in coherence  </li> <li>increasing drift across intervals  </li> <li>failure to maintain reasoning state consistency  </li> </ul>"},{"location":"ris_v1_0/#854-impact","title":"8.5.4 Impact","text":"<p>Long-duration applications may become unreliable, particularly in agentic or multi-step workflows.</p>"},{"location":"ris_v1_0/#855-mitigation","title":"8.5.5 Mitigation","text":"<ul> <li>longitudinal monitoring  </li> <li>periodic state resets  </li> <li>consistency checkpoints  </li> </ul>"},{"location":"ris_v1_0/#86-interference-risk","title":"8.6 Interference Risk","text":""},{"location":"ris_v1_0/#861-description","title":"8.6.1 Description","text":"<p>Interference risk arises when external components affect reasoning integrity.</p> <p>Sources of interference include:</p> <ul> <li>tools  </li> <li>retrieval components  </li> <li>other agents  </li> <li>environment APIs  </li> <li>context or memory systems  </li> </ul>"},{"location":"ris_v1_0/#862-causes","title":"8.6.2 Causes","text":"<ul> <li>misaligned tool responses  </li> <li>unbounded agent interactions  </li> <li>unpredictable RAG data injection  </li> <li>multi-agent reasoning collisions  </li> </ul>"},{"location":"ris_v1_0/#863-indicators","title":"8.6.3 Indicators","text":"<ul> <li>sudden reasoning divergence  </li> <li>tool-dependent instability  </li> <li>inconsistent decisions after tool use  </li> <li>agent cross-contamination  </li> </ul>"},{"location":"ris_v1_0/#864-impact","title":"8.6.4 Impact","text":"<p>Interference may cause unpredictable reasoning patterns, loss of boundary control, and uncontrolled drift.</p>"},{"location":"ris_v1_0/#865-mitigation","title":"8.6.5 Mitigation","text":"<ul> <li>interference detection  </li> <li>boundary enforcement controls  </li> <li>isolation of reasoning state  </li> <li>validation of tool outputs  </li> </ul>"},{"location":"ris_v1_0/#87-boundary-violation-risk","title":"8.7 Boundary Violation Risk","text":""},{"location":"ris_v1_0/#871-description","title":"8.7.1 Description","text":"<p>Boundary violation risk occurs when the system exceeds allowed contextual, semantic, or operational boundaries.</p>"},{"location":"ris_v1_0/#872-causes","title":"8.7.2 Causes","text":"<ul> <li>context window expansion  </li> <li>unauthorized semantic domain exploration  </li> <li>cross-task memory leakage  </li> <li>overreliance on external tools  </li> <li>multi-agent interference  </li> </ul>"},{"location":"ris_v1_0/#873-indicators","title":"8.7.3 Indicators","text":"<ul> <li>references to disallowed information  </li> <li>use of context not present in the input  </li> <li>reasoning dependent on external unintended state  </li> <li>creeping expansion of reasoning scope  </li> </ul>"},{"location":"ris_v1_0/#874-impact","title":"8.7.4 Impact","text":"<p>Boundary violations compromise predictability, auditability, and operational safety.</p>"},{"location":"ris_v1_0/#875-mitigation","title":"8.7.5 Mitigation","text":"<ul> <li>boundary definition and enforcement controls  </li> <li>monitoring for out-of-bound semantic transitions  </li> <li>strict tool and agent constraints  </li> </ul>"},{"location":"ris_v1_0/#88-degradation-risk","title":"8.8 Degradation Risk","text":""},{"location":"ris_v1_0/#881-description","title":"8.8.1 Description","text":"<p>Degradation risk refers to progressive reasoning quality decline over time due to repeated use, extended agent loops, environmental factors, or model instability.</p>"},{"location":"ris_v1_0/#882-causes","title":"8.8.2 Causes","text":"<ul> <li>prolonged agentic sessions  </li> <li>heavy tool usage  </li> <li>accumulated contextual noise  </li> <li>fluctuating infrastructure conditions  </li> </ul>"},{"location":"ris_v1_0/#883-indicators","title":"8.8.3 Indicators","text":"<ul> <li>increasing drift trends  </li> <li>decreasing coherence metrics  </li> <li>longer or shorter reasoning chains without cause  </li> <li>gradual divergence across evaluation windows  </li> </ul>"},{"location":"ris_v1_0/#884-impact","title":"8.8.4 Impact","text":"<p>Degradation affects reliability in long-running systems, especially in automation and orchestration applications.</p>"},{"location":"ris_v1_0/#885-mitigation","title":"8.8.5 Mitigation","text":"<ul> <li>longitudinal stability tracking  </li> <li>state resets  </li> <li>performance baselining  </li> <li>proactive drift sensitivity checks  </li> </ul>"},{"location":"ris_v1_0/#89-risk-levels","title":"8.9 Risk Levels","text":"<p>RIS classifies reasoning risks into three levels:</p> <ul> <li>Low: Within established variance and stability thresholds  </li> <li>Moderate: Deviations observed but within safe operational limits  </li> <li>High: Significant deviations requiring remediation  </li> </ul> <p>Risk levels SHALL be documented in the conformance report.</p>"},{"location":"ris_v1_0/#810-relationship-between-risks-and-ris-levels","title":"8.10 Relationship Between Risks and RIS Levels","text":"<p>Each RIS level carries a corresponding risk tolerance:</p> <ul> <li>RIS-0: All risks high  </li> <li>RIS-1: Most risks moderate to high  </li> <li>RIS-2: Risks moderate with limited high-risk events  </li> <li>RIS-3: All risks low to moderate; no high-risk events allowed  </li> <li>RIS-4: All risks low; strict controls required  </li> </ul> <p>A system SHALL NOT qualify for RIS-3 or RIS-4 if any high-risk category is identified.</p>"},{"location":"ris_v1_0/#811-use-of-risk-models-in-evaluation","title":"8.11 Use of Risk Models in Evaluation","text":"<p>During evaluation, assessors SHALL:</p> <ul> <li>identify risk categories triggered  </li> <li>document causes and indicators  </li> <li>evaluate drift and stability trends  </li> <li>map observed behavior to risk severity  </li> <li>determine required mitigation for higher RIS levels  </li> </ul> <p>Risk models inform conformance decisions but do not replace mandatory controls or scoring.</p>"},{"location":"ris_v1_0/#812-use-of-risk-models-in-production-monitoring","title":"8.12 Use of Risk Models in Production Monitoring","text":"<p>Organizations SHOULD use risk models to:</p> <ul> <li>detect early warning signals  </li> <li>anticipate degradation trends  </li> <li>identify tool-induced instability  </li> <li>evaluate multi-agent interference  </li> <li>maintain operational safety  </li> </ul> <p>Risk-based monitoring SHALL supplement RIS control compliance.</p>"},{"location":"ris_v1_0/#9-evaluation-methodology","title":"9. Evaluation Methodology","text":""},{"location":"ris_v1_0/#91-overview","title":"9.1 Overview","text":"<p>The RIS evaluation methodology defines the standardized procedures for performing a reasoning integrity assessment. These procedures ensure that measurements are:</p> <ul> <li>repeatable  </li> <li>objective  </li> <li>statistically meaningful  </li> <li>comparable across systems  </li> <li>suitable for audit and certification  </li> </ul> <p>All evaluations MUST follow the methodology in this section to be considered valid for RIS classification.</p>"},{"location":"ris_v1_0/#92-evaluation-objectives","title":"9.2 Evaluation Objectives","text":"<p>The objectives of RIS evaluation are to:</p> <ol> <li>quantify reasoning stability and coherence  </li> <li>measure drift and variance across repeated evaluations  </li> <li>detect boundary violations  </li> <li>assess interference sensitivity  </li> <li>validate compliance with required controls  </li> <li>classify the system into an appropriate RIS level  </li> </ol> <p>Evaluation SHALL NOT assess factual correctness or ethical outcomes. Only reasoning integrity is measured.</p>"},{"location":"ris_v1_0/#93-evaluation-conditions","title":"9.3 Evaluation Conditions","text":""},{"location":"ris_v1_0/#931-environmental-consistency","title":"9.3.1 Environmental consistency","text":"<p>Evaluations SHALL be conducted under consistent conditions, including:</p> <ul> <li>model version  </li> <li>inference parameters (temperature, top-p, settings)  </li> <li>context and prompt format  </li> <li>agent or tool configurations  </li> <li>hardware or infrastructure environment  </li> </ul> <p>Changes to any parameter invalidate comparisons.</p>"},{"location":"ris_v1_0/#932-isolation","title":"9.3.2 Isolation","text":"<p>The evaluation environment SHOULD minimize:</p> <ul> <li>interference from concurrent workloads  </li> <li>environmental noise  </li> <li>model adaptation or memory building  </li> <li>caching effects  </li> </ul>"},{"location":"ris_v1_0/#933-documentation","title":"9.3.3 Documentation","text":"<p>The evaluator SHALL document:</p> <ul> <li>all inference parameters  </li> <li>all environment variables  </li> <li>system settings used for assessment  </li> <li>test prompt sets  </li> </ul>"},{"location":"ris_v1_0/#94-prompt-set-requirements","title":"9.4 Prompt Set Requirements","text":"<p>RIS evaluation uses two types of prompts:</p> <ol> <li>Baseline prompts  </li> <li>Perturbation prompts  </li> </ol>"},{"location":"ris_v1_0/#941-baseline-prompts","title":"9.4.1 Baseline prompts","text":"<p>Baseline prompts SHOULD include:</p> <ul> <li>informational queries  </li> <li>reasoning tasks  </li> <li>multi-step tasks  </li> <li>conceptual analysis tasks  </li> <li>domain-neutral questions  </li> </ul> <p>At least 50 prompts SHOULD be used for general-purpose systems. At least 20 prompts SHOULD be used for specialized systems.</p>"},{"location":"ris_v1_0/#942-perturbation-prompts","title":"9.4.2 Perturbation prompts","text":"<p>Perturbation prompts MUST maintain semantic equivalence with baseline prompts while varying:</p> <ul> <li>syntax  </li> <li>structure  </li> <li>ordering  </li> <li>minor phrasing  </li> <li>neutral substitutions  </li> </ul> <p>A minimum of 10 perturbations SHALL be used per baseline prompt selected for perturbation testing.</p>"},{"location":"ris_v1_0/#95-sampling-methodology","title":"9.5 Sampling Methodology","text":""},{"location":"ris_v1_0/#951-repeated-inference-sampling","title":"9.5.1 Repeated inference sampling","text":"<p>For each baseline prompt:</p> <ul> <li>at least 25 repeated inference samples SHALL be collected  </li> <li>sampling parameters SHALL remain identical across all repetitions  </li> </ul> <p>Repeated inference sampling is used to compute:</p> <ul> <li>chain stability  </li> <li>semantic coherence  </li> <li>drift sensitivity  </li> <li>variance envelope compliance  </li> </ul>"},{"location":"ris_v1_0/#952-controlled-perturbation-sampling","title":"9.5.2 Controlled perturbation sampling","text":"<p>For each perturbation prompt:</p> <ul> <li>at least 10 inference samples SHALL be collected  </li> </ul> <p>Perturbation sampling is used to measure:</p> <ul> <li>structural drift  </li> <li>semantic drift  </li> <li>robustness under variation  </li> </ul>"},{"location":"ris_v1_0/#96-baseline-establishment","title":"9.6 Baseline Establishment","text":""},{"location":"ris_v1_0/#961-creating-a-baseline","title":"9.6.1 Creating a baseline","text":"<p>The evaluator SHALL establish a baseline by:</p> <ol> <li>selecting representative baseline prompts  </li> <li>executing repeated inference sampling  </li> <li>computing structural and semantic similarity metrics  </li> <li>determining drift sensitivity  </li> <li>defining acceptable variance envelopes  </li> </ol>"},{"location":"ris_v1_0/#962-baseline-validity","title":"9.6.2 Baseline validity","text":"<p>A baseline is valid only if:</p> <ul> <li>all evaluation conditions are documented  </li> <li>system configuration remains unchanged  </li> <li>variance envelope is statistically meaningful  </li> </ul> <p>Baseline MUST be refreshed when model parameters change or risk models trigger reassessment.</p>"},{"location":"ris_v1_0/#97-drift-evaluation","title":"9.7 Drift Evaluation","text":""},{"location":"ris_v1_0/#971-drift-detection","title":"9.7.1 Drift detection","text":"<p>Drift is detected by:</p> <ul> <li>comparing new samples to baseline  </li> <li>analyzing deviation trends  </li> <li>identifying abrupt or progressive divergence  </li> <li>checking for consistent out-of-envelope behavior  </li> </ul>"},{"location":"ris_v1_0/#972-drift-characterization","title":"9.7.2 Drift characterization","text":"<p>Drift SHALL be categorized as:</p> <ul> <li>structural  </li> <li>semantic  </li> <li>temporal  </li> <li>interference-based  </li> </ul>"},{"location":"ris_v1_0/#973-drift-thresholds","title":"9.7.3 Drift thresholds","text":"<p>A drift event occurs when:</p> <ul> <li>deviation exceeds the envelope for more than 5 percent of samples  </li> <li>repeated deviations occur during perturbation sampling  </li> <li>temporal divergence increases across sequential windows  </li> </ul>"},{"location":"ris_v1_0/#974-drift-escalation","title":"9.7.4 Drift escalation","text":"<p>If drift is detected:</p> <ul> <li>the system\u2019s RIS level MAY be lowered  </li> <li>additional testing SHALL be performed  </li> <li>drift controls MUST be verified  </li> </ul>"},{"location":"ris_v1_0/#98-boundary-evaluation","title":"9.8 Boundary Evaluation","text":"<p>Boundary adherence SHALL be tested by:</p> <ul> <li>introducing tasks with clear semantic boundaries  </li> <li>evaluating attempts to use disallowed context  </li> <li>testing reasoning behavior with restricted tool outputs  </li> <li>detecting cross-task or cross-context contamination  </li> </ul> <p>Any violation SHALL be logged and considered in level classification.</p>"},{"location":"ris_v1_0/#99-interference-evaluation","title":"9.9 Interference Evaluation","text":"<p>Interference evaluation includes:</p> <ul> <li>testing reasoning stability before and after tool calls  </li> <li>measuring agent-to-agent interaction effects  </li> <li>injecting controlled retrieval variability  </li> <li>observing structural or semantic disruptions  </li> </ul> <p>Interference MUST not cause uncontrolled reasoning divergence for systems seeking RIS-3 or RIS-4 level classification.</p>"},{"location":"ris_v1_0/#910-variance-envelope-evaluation","title":"9.10 Variance Envelope Evaluation","text":"<p>Variance envelope compliance SHALL be tested through:</p> <ul> <li>repeated inference sampling  </li> <li>perturbation sampling  </li> <li>load or stress conditions if applicable  </li> <li>periodic resampling across intervals  </li> </ul> <p>A system MUST maintain compliance for:</p> <ul> <li>at least 95 percent of samples (RIS-3)  </li> <li>at least 98 percent of samples (RIS-4)  </li> </ul>"},{"location":"ris_v1_0/#911-evaluation-report-requirements","title":"9.11 Evaluation Report Requirements","text":"<p>A complete evaluation report SHALL include:</p> <ol> <li>System description  </li> <li>Evaluation environment details  </li> <li>Prompt sets used  </li> <li>Sampling methodology  </li> <li>Baseline metrics  </li> <li>Drift analysis  </li> <li>Variance envelope results  </li> <li>Boundary adherence results  </li> <li>Interference results  </li> <li>Composite score calculation  </li> <li>Control compliance analysis  </li> <li>RIS level assigned  </li> <li>Validity period  </li> <li>Evidence appendix  </li> </ol> <p>The report SHALL be signed by the evaluator.</p>"},{"location":"ris_v1_0/#912-evaluation-validity","title":"9.12 Evaluation Validity","text":"<p>An evaluation remains valid only if:</p> <ul> <li>the system configuration does not change  </li> <li>no drift events occur in production  </li> <li>no new tools, agents, or retrieval sources are introduced  </li> <li>governance boundaries remain constant  </li> </ul> <p>If any condition changes, reassessment MAY be required immediately.</p>"},{"location":"ris_v1_0/#913-re-evaluation-requirements","title":"9.13 Re-Evaluation Requirements","text":"<p>Re-evaluation SHALL occur:</p> <ul> <li>at the end of the classification validity period  </li> <li>when major model updates occur  </li> <li>when drift or instability is observed  </li> <li>when new risk categories are triggered  </li> <li>when evidence suggests deviation from baseline behavior  </li> </ul> <p>Systems in regulated or safety-critical environments MAY require more frequent reassessments.</p>"},{"location":"ris_v1_0/#10-audit-guidelines","title":"10. Audit Guidelines","text":""},{"location":"ris_v1_0/#101-overview","title":"10.1 Overview","text":"<p>This section defines the audit requirements, evidence standards, assessment procedures, and validation criteria necessary to verify compliance with the Reasoning Integrity Standard (RIS). These guidelines ensure that evaluations are reliable, reproducible, and suitable for internal review, third-party certification, or regulatory oversight.</p> <p>An auditor MAY be an internal reviewer, an external assessor, or an automated system. All auditors SHALL adhere to the requirements outlined in this section.</p>"},{"location":"ris_v1_0/#102-audit-objectives","title":"10.2 Audit Objectives","text":"<p>The objectives of a RIS audit are to:</p> <ol> <li>verify the correctness and completeness of evaluation procedures  </li> <li>validate metric calculations and composite reasoning integrity scores  </li> <li>confirm implementation of mandatory controls  </li> <li>detect deficiencies, violations, or risks  </li> <li>ensure evidence sufficiency and traceability  </li> <li>determine whether assigned RIS levels are justified  </li> <li>produce a formal audit report documenting findings  </li> </ol> <p>Audits SHALL NOT evaluate ethical, factual, or domain-specific correctness.</p>"},{"location":"ris_v1_0/#103-audit-scope","title":"10.3 Audit Scope","text":"<p>The audit SHALL cover:</p> <ul> <li>evaluation methodology adherence  </li> <li>system configuration and invariants  </li> <li>reasoning integrity metrics  </li> <li>drift and variance analysis  </li> <li>control compliance  </li> <li>governance boundary enforcement  </li> <li>operational integrity  </li> <li>logged events and ledger entries  </li> <li>deviation or violation reports  </li> </ul> <p>The audit MAY include additional areas depending on organizational, regulatory, or contractual requirements.</p>"},{"location":"ris_v1_0/#104-evidence-requirements","title":"10.4 Evidence Requirements","text":"<p>Auditors SHALL review the following types of evidence:</p>"},{"location":"ris_v1_0/#1041-raw-inference-samples","title":"10.4.1 Raw inference samples","text":"<ul> <li>repeated inference outputs  </li> <li>perturbation outputs  </li> <li>temporal sampling outputs  </li> <li>tool-integrated outputs (if applicable)  </li> </ul>"},{"location":"ris_v1_0/#1042-metric-calculations","title":"10.4.2 Metric calculations","text":"<ul> <li>chain stability similarity data  </li> <li>semantic coherence scores  </li> <li>drift sensitivity calculations  </li> <li>variance envelope compliance results  </li> <li>boundary adherence results  </li> </ul>"},{"location":"ris_v1_0/#1043-configuration-data","title":"10.4.3 Configuration data","text":"<ul> <li>inference parameters (temperature, top-p, etc.)  </li> <li>system settings  </li> <li>agent or tool configurations  </li> <li>environment variables  </li> <li>software versions  </li> </ul>"},{"location":"ris_v1_0/#1044-control-implementation-evidence","title":"10.4.4 Control implementation evidence","text":"<ul> <li>governance boundary definitions  </li> <li>drift envelope definitions  </li> <li>stability and coherence controls  </li> <li>interference detection controls  </li> <li>audit logging mechanisms  </li> </ul>"},{"location":"ris_v1_0/#1045-logs-and-ledger-entries","title":"10.4.5 Logs and ledger entries","text":"<ul> <li>reasoning ledger (or equivalent)  </li> <li>boundary violation logs  </li> <li>drift events  </li> <li>anomaly detection entries  </li> <li>operational logs related to inference behavior  </li> </ul> <p>Auditors SHALL verify the authenticity and integrity of evidence before drawing conclusions.</p>"},{"location":"ris_v1_0/#105-audit-procedures","title":"10.5 Audit Procedures","text":""},{"location":"ris_v1_0/#1051-preparation","title":"10.5.1 Preparation","text":"<p>The auditor SHALL:</p> <ul> <li>obtain system documentation  </li> <li>review evaluation methodology used  </li> <li>validate prompt sets and sampling plans  </li> <li>confirm frozen inference parameters  </li> <li>ensure environmental consistency  </li> </ul>"},{"location":"ris_v1_0/#1052-evidence-examination","title":"10.5.2 Evidence examination","text":"<p>The auditor SHALL examine:</p> <ul> <li>raw samples for drift or instability  </li> <li>metrics for correctness and reproducibility  </li> <li>compliance with mandatory controls  </li> <li>documentation for completeness  </li> </ul>"},{"location":"ris_v1_0/#1053-reproduction-testing","title":"10.5.3 Reproduction testing","text":"<p>Auditors MAY reproduce:</p> <ul> <li>repeated inference sampling  </li> <li>perturbation sampling  </li> <li>drift and variance calculations  </li> </ul> <p>Reproduction SHALL use identical parameters to those documented in the evaluation.</p>"},{"location":"ris_v1_0/#1054-violation-detection","title":"10.5.4 Violation detection","text":"<p>The auditor SHALL check for:</p> <ul> <li>drift beyond thresholds  </li> <li>variance envelope breaches  </li> <li>boundary violations  </li> <li>interference anomalies  </li> <li>control misconfigurations  </li> <li>unreported failures  </li> </ul> <p>Any violation SHALL be documented in the final audit report.</p>"},{"location":"ris_v1_0/#106-compliance-determination","title":"10.6 Compliance Determination","text":"<p>Audit findings SHALL conclude one of the following:</p> <ul> <li>compliant  </li> <li>compliant with exceptions  </li> <li>non-compliant  </li> <li>inconclusive (insufficient evidence)  </li> </ul> <p>If non-compliant, the auditor SHALL document which controls or evaluation procedures were violated.</p> <p>A RIS level MAY only be confirmed if:</p> <ul> <li>composite score qualifies  </li> <li>mandatory controls are satisfied  </li> <li>evidence is complete  </li> <li>no unmitigated high-risk deviations are observed  </li> </ul>"},{"location":"ris_v1_0/#107-audit-reporting-requirements","title":"10.7 Audit Reporting Requirements","text":"<p>The audit report SHALL include:</p> <ol> <li>System overview  </li> <li>Evaluation conditions  </li> <li>Summary of controls reviewed  </li> <li>Evidence examined  </li> <li>Metric verification results  </li> <li>Observed violations or exceptions  </li> <li>Assessment of drift, variance, and boundary adherence  </li> <li>Conformance determination  </li> <li>Confirmed RIS level  </li> <li>Recommendations and corrective actions  </li> <li>Validity period of assessment  </li> <li>Appendices with supporting data  </li> </ol> <p>Reports SHALL be retained for the duration of the classification validity period.</p>"},{"location":"ris_v1_0/#108-corrective-action-requirements","title":"10.8 Corrective Action Requirements","text":"<p>If deficiencies or violations are identified:</p> <ul> <li>corrective actions SHALL be documented  </li> <li>the system owner SHALL remediate issues  </li> <li>the system SHALL undergo partial or full reassessment  </li> <li>major issues SHALL reset the RIS classification  </li> </ul> <p>Corrective actions MAY include:</p> <ul> <li>adjusting inference parameters  </li> <li>redefining governance boundaries  </li> <li>improving drift or variance controls  </li> <li>updating tool or agent behavior  </li> </ul>"},{"location":"ris_v1_0/#109-auditor-qualifications","title":"10.9 Auditor Qualifications","text":"<p>RIS audits MAY be conducted by:</p> <ul> <li>internal governance teams  </li> <li>independent third-party evaluators  </li> <li>qualified researchers  </li> <li>automated auditing systems  </li> </ul> <p>Auditors SHOULD possess expertise in:</p> <ul> <li>LLM inference behavior  </li> <li>statistical evaluation  </li> <li>reasoning-chain analysis  </li> <li>AI governance and risk frameworks  </li> <li>tool, agent, or RAG system integrations (if applicable)  </li> </ul> <p>Organizations MAY impose additional requirements for auditor certification.</p>"},{"location":"ris_v1_0/#1010-continuous-audit-recommendations","title":"10.10 Continuous Audit Recommendations","text":"<p>Organizations SHOULD implement ongoing audit-aligned practices:</p> <ul> <li>periodic drift checks  </li> <li>rolling variance sampling  </li> <li>boundary adherence monitoring  </li> <li>interference detection and logging  </li> <li>longitudinal stability assessments  </li> </ul> <p>Continuous monitoring SHALL supplement, not replace, formal RIS audits.</p>"},{"location":"ris_v1_0/#1011-audit-validity","title":"10.11 Audit Validity","text":"<p>An audit is valid for:</p> <ul> <li>12 months for general systems  </li> <li>6 months for agentic or tool-integrated systems  </li> <li>3 months for safety-critical systems  </li> </ul> <p>Audits SHALL be invalidated immediately if:</p> <ul> <li>the model or agent is updated  </li> <li>new tools, retrieval sources, or agents are introduced  </li> <li>significant drift is observed in production  </li> <li>boundary violations occur repeatedly  </li> </ul>"},{"location":"ris_v1_0/#1012-audit-transparency","title":"10.12 Audit Transparency","text":"<p>Organizations MAY disclose:</p> <ul> <li>confirmed RIS level  </li> <li>summary of findings  </li> <li>audit date and validity period  </li> </ul> <p>Organizations SHOULD NOT publicly disclose:</p> <ul> <li>raw inference samples  </li> <li>proprietary metrics  </li> <li>internal configurations  </li> <li>confidential logs  </li> </ul> <p>Unless required by contract, regulation, or security review.</p>"},{"location":"ris_v1_0/#11-reference-implementation-lcac","title":"11. Reference Implementation (LCAC)","text":""},{"location":"ris_v1_0/#111-overview","title":"11.1 Overview","text":"<p>This section describes the Least-Context Access Control (LCAC) framework as a reference implementation of the Reasoning Integrity Standard (RIS). LCAC demonstrates one practical method for enforcing RIS controls, measuring reasoning integrity, and maintaining predictable reasoning behavior in LLM-based systems.</p> <p>LCAC is not required for RIS conformance. It is provided solely as an informative reference to illustrate feasibility, implementation patterns, and system design considerations aligned with RIS requirements.</p>"},{"location":"ris_v1_0/#112-architectural-overview","title":"11.2 Architectural Overview","text":"<p>LCAC is structured as a modular reasoning-governance framework consisting of:</p> <ul> <li>a centralized governor service  </li> <li>a trust and variance model  </li> <li>an immutable reasoning ledger  </li> <li>governance boundary enforcement components  </li> <li>monitoring and evaluation pipelines  </li> <li>optional persona overlays  </li> </ul> <p>LCAC may operate as an independent service or be integrated directly into an LLM-driven application, multi-agent system, or inference pipeline.</p>"},{"location":"ris_v1_0/#113-components-and-functions","title":"11.3 Components and Functions","text":""},{"location":"ris_v1_0/#1131-governor","title":"11.3.1 Governor","text":"<p>The LCAC governor performs the following functions:</p> <ul> <li>receives prompt and output pairs for evaluation  </li> <li>computes trust, variance, and drift metrics  </li> <li>determines reasoning verdicts (stable, watch, unstable)  </li> <li>updates governance states (hold, elevate, lockdown)  </li> <li>writes results to the ledger  </li> </ul> <p>The governor serves as the core enforcement point for RIS-aligned controls.</p>"},{"location":"ris_v1_0/#1132-trust-model","title":"11.3.2 Trust Model","text":"<p>LCAC\u2019s trust model provides:</p> <ul> <li>a baseline trust score  </li> <li>variance and drift calculations  </li> <li>predictive stability measurements  </li> <li>structural and semantic similarity analysis  </li> </ul> <p>The trust score is mapped to RIS metrics such as:</p> <ul> <li>chain stability  </li> <li>semantic coherence  </li> <li>drift sensitivity  </li> </ul> <p>Trust values range between 0.00 and 1.00, consistent with RIS scoring.</p>"},{"location":"ris_v1_0/#1133-variance-and-drift-envelope","title":"11.3.3 Variance and Drift Envelope","text":"<p>LCAC defines and enforces a variance envelope based on:</p> <ul> <li>repeated inference samples  </li> <li>controlled perturbation testing  </li> <li>deviation thresholds  </li> <li>longitudinal observations  </li> </ul> <p>Metric outputs are compared against these envelopes to determine:</p> <ul> <li>compliance  </li> <li>drift events  </li> <li>stability degradation  </li> </ul>"},{"location":"ris_v1_0/#1134-immutable-ledger","title":"11.3.4 Immutable Ledger","text":"<p>LCAC maintains an immutable reasoning ledger using:</p> <ul> <li>hash-linked entries  </li> <li>timestamped records  </li> <li>evaluation metrics  </li> <li>baseline comparisons  </li> <li>anomaly documentation  </li> </ul> <p>Each ledger entry includes:</p> <ul> <li>input prompt  </li> <li>model output  </li> <li>trust score  </li> <li>variance metrics  </li> <li>stability verdict  </li> <li>previous entry hash  </li> <li>current entry hash  </li> </ul> <p>This supports audit, reproducibility, and traceability.</p>"},{"location":"ris_v1_0/#114-governance-modes","title":"11.4 Governance Modes","text":"<p>LCAC defines three governance modes that influence reasoning behavior:</p>"},{"location":"ris_v1_0/#1141-hold","title":"11.4.1 Hold","text":"<ul> <li>default system state  </li> <li>stable or near-stable reasoning  </li> <li>no significant anomalies detected  </li> </ul>"},{"location":"ris_v1_0/#1142-elevate","title":"11.4.2 Elevate","text":"<ul> <li>moderate drift or variance detected  </li> <li>reasoning monitored more aggressively  </li> <li>potential instability anticipated  </li> </ul>"},{"location":"ris_v1_0/#1143-lockdown","title":"11.4.3 Lockdown","text":"<ul> <li>reasoning integrity breach detected  </li> <li>drift or variance above thresholds  </li> <li>boundary violation or interference event observed  </li> <li>heightened restrictions applied  </li> </ul> <p>Governance modes demonstrate how RIS controls may be operationalized.</p>"},{"location":"ris_v1_0/#115-boundary-enforcement","title":"11.5 Boundary Enforcement","text":"<p>LCAC establishes governance boundaries using:</p> <ul> <li>contextual limits  </li> <li>semantic domain constraints  </li> <li>tool and agent access restrictions  </li> <li>validation checks for external outputs  </li> </ul> <p>Boundary enforcement aligns with RIS boundary violation controls.</p>"},{"location":"ris_v1_0/#116-persona-overlay-informative-only","title":"11.6 Persona Overlay (Informative Only)","text":"<p>LCAC includes an optional persona system that applies:</p> <ul> <li>stability bias adjustments  </li> <li>risk posture tuning  </li> <li>domain-specific behavioral constraints  </li> </ul> <p>Personas may influence:</p> <ul> <li>baseline trust  </li> <li>drift sensitivity  </li> <li>variance envelopes  </li> <li>stability thresholds  </li> </ul> <p>Persona systems are informative and not required for RIS compliance.</p>"},{"location":"ris_v1_0/#117-integration-patterns","title":"11.7 Integration Patterns","text":"<p>LCAC supports the following integration models:</p>"},{"location":"ris_v1_0/#1171-request-evaluation-pattern","title":"11.7.1 Request-Evaluation Pattern","text":"<p>The system sends:</p> <ul> <li>prompt  </li> <li>model output  </li> </ul> <p>to LCAC for evaluation. LCAC returns:</p> <ul> <li>trust score  </li> <li>variance data  </li> <li>stability verdict  </li> <li>governance mode  </li> </ul> <p>This pattern enables external inference pipelines to remain unchanged.</p>"},{"location":"ris_v1_0/#1172-inline-agent-or-llm-integration","title":"11.7.2 Inline Agent or LLM Integration","text":"<p>LCAC may be embedded into:</p> <ul> <li>agent toolchains  </li> <li>orchestration frameworks  </li> <li>multi-agent reasoning flows  </li> <li>specialized LLM applications  </li> </ul> <p>Governance checks occur inline between inference steps.</p>"},{"location":"ris_v1_0/#1173-multi-model-or-router-integration","title":"11.7.3 Multi-Model or Router Integration","text":"<p>LCAC may act as:</p> <ul> <li>a routing constraint  </li> <li>a stability filter  </li> <li>a verification layer  </li> </ul> <p>for systems that dynamically select between models.</p>"},{"location":"ris_v1_0/#118-operational-monitoring","title":"11.8 Operational Monitoring","text":"<p>LCAC supports RIS-aligned monitoring by:</p> <ul> <li>generating audit logs  </li> <li>tracking drift events  </li> <li>identifying variance breaches  </li> <li>logging boundary violations  </li> <li>maintaining historical baselines  </li> </ul> <p>Monitoring outputs MAY be used to trigger reassessment under RIS Section 9.</p>"},{"location":"ris_v1_0/#119-example-workflow","title":"11.9 Example Workflow","text":"<p>A RIS-aligned workflow using LCAC includes:</p> <ol> <li>The LLM produces an output for a given prompt.  </li> <li>The output and prompt are sent to LCAC.  </li> <li>LCAC computes trust, variance, and drift metrics.  </li> <li>LCAC assigns a verdict (stable, watch, unstable).  </li> <li>LCAC writes an immutable ledger entry.  </li> <li>Governance mode is updated based on results.  </li> <li>Application logic uses the verdict to:  </li> <li>accept the output  </li> <li>reject the output  </li> <li>request re-evaluation  </li> <li>escalate to a fallback model  </li> <li>enforce stricter controls  </li> </ol> <p>This workflow illustrates one method of operationalizing RIS requirements.</p>"},{"location":"ris_v1_0/#1110-compliance-relationship","title":"11.10 Compliance Relationship","text":"<p>LCAC demonstrates compliance with the following RIS controls:</p> <ul> <li>RS-1, RS-2, RS-3  </li> <li>SC-1, SC-2, SC-3  </li> <li>DR-1, DR-2, DR-3  </li> <li>VE-1, VE-2, VE-3  </li> <li>GB-1, GB-2, GB-3  </li> <li>OP-1, OP-2, OP-3, OP-4  </li> </ul> <p>LCAC is a reference implementation and SHOULD NOT be considered the only method of achieving RIS compliance.</p>"},{"location":"ris_v1_0/#1111-implementation-notes","title":"11.11 Implementation Notes","text":"<ul> <li>LCAC uses Redis for metric storage and ledger hashing.  </li> <li>All evaluation computations are deterministic given identical inputs.  </li> <li>LCAC can operate in multi-node architectures with shared storage.  </li> <li>The reference implementation is platform-agnostic.  </li> <li>API endpoints and data schemas are documented in the LCAC repository.  </li> </ul> <p>These details are informative and may vary between deployments.</p>"},{"location":"ris_v1_0/#1112-limitations","title":"11.12 Limitations","text":"<p>LCAC, as a reference implementation:</p> <ul> <li>does not guarantee factual correctness  </li> <li>does not address privacy or security  </li> <li>does not enforce ethical or domain-specific constraints  </li> <li>does not replace external governance frameworks  </li> </ul> <p>RIS conformance MUST be determined independently of LCAC adoption.</p>"},{"location":"ris_v1_0/#1113-informative-appendix-linkage","title":"11.13 Informative Appendix Linkage","text":"<p>Sections of LCAC MAY be referenced in Appendix A for:</p> <ul> <li>example metrics  </li> <li>sample ledger entries  </li> <li>example drift baselines  </li> <li>variance envelope templates  </li> <li>integration code snippets  </li> </ul> <p>These references are informative only.</p>"},{"location":"ris_v1_0/#12-mapping-to-existing-standards","title":"12. Mapping to Existing Standards","text":""},{"location":"ris_v1_0/#121-overview","title":"12.1 Overview","text":"<p>This section provides a crosswalk between the Reasoning Integrity Standard (RIS) and widely adopted security, governance, and AI risk management frameworks. The purpose of this mapping is to:</p> <ul> <li>support organizations aligning RIS with existing compliance programs  </li> <li>demonstrate compatibility between RIS and established standards  </li> <li>clarify which risks and controls RIS covers  </li> <li>identify where RIS provides additional, reasoning-specific requirements  </li> </ul> <p>Mappings are informative and do not constitute equivalence or certification under referenced standards.</p>"},{"location":"ris_v1_0/#122-relationship-to-nist-sp-800-53","title":"12.2 Relationship to NIST SP 800-53","text":"<p>RIS aligns with NIST SP 800-53 by extending its principles into the domain of reasoning integrity for AI systems.</p>"},{"location":"ris_v1_0/#1221-complementary-areas","title":"12.2.1 Complementary Areas","text":"<ul> <li>NIST RA (Risk Assessment): RIS provides a reasoning-specific risk model.  </li> <li>NIST CA (Assessment, Authorization, Monitoring): RIS defines evaluation and audit procedures.  </li> <li>NIST SI (System Integrity): RIS adds reasoning-chain integrity and drift detection.  </li> <li>NIST PM (Program Management): RIS supports governance for LLM-based workflows.  </li> </ul>"},{"location":"ris_v1_0/#1222-gaps-filled-by-ris","title":"12.2.2 Gaps Filled by RIS","text":"<p>NIST does not address:</p> <ul> <li>reasoning drift  </li> <li>semantic coherence  </li> <li>chain stability  </li> <li>variance envelopes  </li> <li>multi-agent interference  </li> </ul> <p>RIS provides controls specifically for these areas.</p>"},{"location":"ris_v1_0/#123-relationship-to-isoiec-27001","title":"12.3 Relationship to ISO/IEC 27001","text":"<p>RIS complements ISO/IEC 27001 by introducing AI reasoning integrity controls that parallel information security requirements.</p>"},{"location":"ris_v1_0/#1231-complementary-areas","title":"12.3.1 Complementary Areas","text":"<ul> <li>ISO 27001 Annex A: controls for monitoring, logging, and operational integrity  </li> <li>ISO 27001 Annex A: change management and configuration management align with RIS evaluation invariants  </li> <li>ISO 27001 Annex A: audit logging aligns with RIS ledger requirements  </li> </ul>"},{"location":"ris_v1_0/#1232-gaps-filled-by-ris","title":"12.3.2 Gaps Filled by RIS","text":"<p>ISO 27001 does not include:</p> <ul> <li>reasoning integrity assessment  </li> <li>drift evaluation  </li> <li>semantic stability requirements  </li> <li>boundary adherence controls for LLMs  </li> </ul> <p>RIS extends governance into these areas.</p>"},{"location":"ris_v1_0/#124-relationship-to-soc-2-trust-services-criteria","title":"12.4 Relationship to SOC 2 Trust Services Criteria","text":"<p>RIS aligns with SOC 2 across the following TSC categories:</p> <ul> <li>Security: LCAC-like governance boundaries enhance system integrity  </li> <li>Availability: drift and variance controls improve predictability of behavior  </li> <li>Processing Integrity: RIS focuses specifically on the integrity of reasoning processes  </li> <li>Confidentiality: RIS does not address confidentiality explicitly but complements it  </li> </ul> <p>SOC 2 does not cover reasoning stability, which RIS supplies.</p>"},{"location":"ris_v1_0/#125-relationship-to-owasp-application-security-verification-standard-asvs","title":"12.5 Relationship to OWASP Application Security Verification Standard (ASVS)","text":"<p>RIS extends ASVS principles into the domain of AI reasoning by providing:</p> <ul> <li>integrity validation of reasoning workflows  </li> <li>boundary checks analogous to input validation  </li> <li>monitoring comparable to application logic verification  </li> </ul> <p>Areas where RIS aligns with ASVS:</p> <ul> <li>session integrity (analogous to reasoning chain integrity)  </li> <li>validation layers (similar to boundary enforcement)  </li> <li>error and exception handling (mapped to drift detection)  </li> </ul> <p>ASVS does not include concepts such as:</p> <ul> <li>semantic drift  </li> <li>reasoning instability  </li> <li>variance envelopes  </li> </ul> <p>RIS adds these requirements.</p>"},{"location":"ris_v1_0/#126-relationship-to-nist-ai-rmf","title":"12.6 Relationship to NIST AI RMF","text":"<p>RIS is closely aligned with the NIST AI Risk Management Framework.</p>"},{"location":"ris_v1_0/#1261-alignment-areas","title":"12.6.1 Alignment Areas","text":"<ul> <li>Govern Function: RIS defines governance boundaries and control families  </li> <li>Map Function: RIS specifies reasoning-specific risks and threat models  </li> <li>Measure Function: RIS provides standardized metrics and scoring  </li> <li>Manage Function: RIS prescribes drift, variance, and stability controls  </li> </ul>"},{"location":"ris_v1_0/#1262-ris-extensions-beyond-nist-ai-rmf","title":"12.6.2 RIS Extensions Beyond NIST AI RMF","text":"<p>RIS introduces:</p> <ul> <li>explicit reasoning chain assessment  </li> <li>structured variance and drift envelopes  </li> <li>multi-level conformance classifications  </li> <li>detailed evaluation methodology  </li> <li>audit and verification standards  </li> </ul> <p>These elements provide more granular operational guidance.</p>"},{"location":"ris_v1_0/#127-relationship-to-the-eu-ai-act","title":"12.7 Relationship to the EU AI Act","text":"<p>The EU AI Act categorizes AI systems according to risk. RIS provides a compatibility layer by offering objective measurements relevant to risk classification.</p>"},{"location":"ris_v1_0/#1271-high-risk-alignment","title":"12.7.1 High-Risk Alignment","text":"<p>High-risk AI systems require:</p> <ul> <li>logging  </li> <li>monitoring  </li> <li>technical documentation  </li> <li>risk mitigation  </li> </ul> <p>RIS supports these by:</p> <ul> <li>providing a reasoning ledger  </li> <li>defining stability metrics  </li> <li>enabling drift monitoring  </li> <li>defining reproducible evaluation processes  </li> </ul>"},{"location":"ris_v1_0/#1272-ris-4-and-high-risk-use-cases","title":"12.7.2 RIS-4 and High-Risk Use Cases","text":"<p>RIS-4 provides the strictest reasoning integrity requirements and is appropriate for:</p> <ul> <li>safety-critical systems  </li> <li>medical reasoning systems  </li> <li>financial decision systems  </li> <li>legal reasoning systems  </li> <li>infrastructure and security systems  </li> </ul> <p>EU AI Act does not provide technical detail on reasoning integrity. RIS fills that gap.</p>"},{"location":"ris_v1_0/#128-summary-table-ris-vs-external-standards","title":"12.8 Summary Table (RIS vs. External Standards)","text":"Framework Overlap RIS Contribution NIST SP 800-53 Monitoring, logging, system integrity Adds reasoning-chain integrity and drift controls ISO 27001 Operational controls, audit logging Adds reasoning stability requirements SOC 2 Processing integrity Adds reasoning-specific integrity metrics OWASP ASVS Boundary and validation concepts Adds semantic and structural reasoning requirements NIST AI RMF Risk, governance, measurement Adds detailed evaluation and conformance system EU AI Act Documentation and oversight Adds measurable technical integrity criteria"},{"location":"ris_v1_0/#129-general-observations","title":"12.9 General Observations","text":"<ul> <li>RIS is compatible with existing frameworks but more granular in evaluating reasoning behavior.  </li> <li>RIS introduces metrics and controls not present in any existing standard.  </li> <li>Organizations adopting RIS can integrate it with existing compliance programs.  </li> <li>RIS MAY serve as a technical layer beneath regulatory or governance frameworks.  </li> </ul>"},{"location":"ris_v1_0/#1210-use-of-mapping-in-compliance-programs","title":"12.10 Use of Mapping in Compliance Programs","text":"<p>Organizations MAY use this mapping to:</p> <ul> <li>integrate RIS evaluation with ISO or SOC 2 audits  </li> <li>include RIS controls in security and governance documentation  </li> <li>demonstrate operational integrity in high-risk AI deployments  </li> <li>support regulatory submissions requiring technical evidence  </li> <li>develop internal AI governance programs  </li> </ul> <p>Mapping does not imply certification under other standards.</p>"},{"location":"ris_v1_0/#13-appendices","title":"13. Appendices","text":""},{"location":"ris_v1_0/#131-overview","title":"13.1 Overview","text":"<p>This section contains informative appendices supporting the implementation, evaluation, and audit of the Reasoning Integrity Standard (RIS). Appendices are not normative unless explicitly referenced by a control or requirement.</p> <p>The materials in this section include:</p> <ul> <li>definitions and terminology</li> <li>mathematical formulations</li> <li>example baseline structures</li> <li>sample drift and variance calculations</li> <li>reference ledger entries</li> <li>prompt set templates</li> <li>evaluation dataset formats</li> <li>implementation considerations</li> </ul>"},{"location":"ris_v1_0/#132-key-definitions","title":"13.2 Key Definitions","text":""},{"location":"ris_v1_0/#1321-reasoning-chain","title":"13.2.1 Reasoning chain","text":"<p>A sequence of internal conceptual steps used by an LLM or agent to derive an output from an input.</p>"},{"location":"ris_v1_0/#1322-structural-drift","title":"13.2.2 Structural drift","text":"<p>A deviation in the ordering, length, or internal transitions of a reasoning chain beyond acceptable variance thresholds.</p>"},{"location":"ris_v1_0/#1323-semantic-drift","title":"13.2.3 Semantic drift","text":"<p>A deviation in meaning, conceptual structure, or topic alignment across samples.</p>"},{"location":"ris_v1_0/#1324-variance-envelope","title":"13.2.4 Variance envelope","text":"<p>The statistical boundary within which repeated inference behavior must remain to be considered stable.</p>"},{"location":"ris_v1_0/#1325-boundary-violation","title":"13.2.5 Boundary violation","text":"<p>A reasoning event in which the model exceeds defined contextual, semantic, or operational constraints.</p>"},{"location":"ris_v1_0/#1326-interference-event","title":"13.2.6 Interference event","text":"<p>An alteration of reasoning behavior caused by a tool, agent, retrieval source, or external system.</p>"},{"location":"ris_v1_0/#133-mathematical-formulations","title":"13.3 Mathematical Formulations","text":""},{"location":"ris_v1_0/#1331-chain-stability-score","title":"13.3.1 Chain stability score","text":"<p>Chain stability S is calculated as:</p> <pre><code>S = mean(similarity(Ri, Rj))\n</code></pre>"},{"location":"ris_v1_0/#1332-semantic-coherence-score","title":"13.3.2 Semantic coherence score","text":"<p>Semantic coherence C is calculated as:</p> <pre><code>C = mean(cosine(Ei, Ej))\n</code></pre>"},{"location":"ris_v1_0/#1333-drift-sensitivity","title":"13.3.3 Drift sensitivity","text":"<p>Drift sensitivity D is calculated as:</p> <pre><code>D = variance(S) + variance(C)\n</code></pre>"},{"location":"ris_v1_0/#1334-variance-envelope-compliance","title":"13.3.4 Variance envelope compliance","text":"<p>Compliance V is:</p> <pre><code>V = (samples within envelope) / (total samples)\n</code></pre>"},{"location":"ris_v1_0/#134-example-variance-envelopes","title":"13.4 Example Variance Envelopes","text":""},{"location":"ris_v1_0/#1341-general-purpose-llm-envelope","title":"13.4.1 General-purpose LLM envelope","text":"<ul> <li>chain stability: 0.75 to 1.00</li> <li>semantic coherence: 0.80 to 1.00</li> <li>drift sensitivity: below 0.30</li> </ul>"},{"location":"ris_v1_0/#1342-high-integrity-envelope","title":"13.4.2 High-integrity envelope","text":"<ul> <li>chain stability: 0.90 to 1.00</li> <li>semantic coherence: 0.90 to 1.00</li> <li>drift sensitivity: below 0.10</li> </ul> <p>Evaluators SHALL adjust envelopes based on domain requirements.</p>"},{"location":"ris_v1_0/#135-example-ledger-entry","title":"13.5 Example Ledger Entry","text":"<p>Below is a sample hash-linked ledger entry.</p> <pre><code>{\n  \"prompt\": \"Explain the impact of X on Y.\",\n  \"output\": \"Reasoning steps and conclusion...\",\n  \"trust\": 0.92,\n  \"variance\": 0.05,\n  \"verdict\": \"stable\",\n  \"timestamp\": 1735861234,\n  \"prev_hash\": \"0000c9beef78aa12fd39b4b2d4e9a18f\",\n  \"hash\": \"8ad1c4fd1e09aa7dfefc324b3987456b\"\n}\n</code></pre> <p>This format is informative and MAY be adapted.</p>"},{"location":"ris_v1_0/#136-prompt-set-templates","title":"13.6 Prompt Set Templates","text":"<p>The following templates MAY be used to construct RIS evaluation prompt sets.</p>"},{"location":"ris_v1_0/#1361-baseline-prompts","title":"13.6.1 Baseline prompts","text":"<ul> <li>Explain a concept.</li> <li>Perform a step-by-step analysis.</li> <li>Compare two ideas.</li> <li>Solve a structured reasoning task.</li> <li>Describe implications of a policy or event.</li> </ul>"},{"location":"ris_v1_0/#1362-perturbation-prompts","title":"13.6.2 Perturbation prompts","text":"<ul> <li>synonym substitutions</li> <li>minor phrasal changes</li> <li>syntax adjustments</li> <li>clause reordering</li> </ul> <p>All perturbations MUST preserve semantic meaning.</p>"},{"location":"ris_v1_0/#137-evaluation-dataset-format","title":"13.7 Evaluation Dataset Format","text":"<p>Evaluation datasets SHOULD be structured as follows:</p> <pre><code>{\n  \"prompt_id\": \"P001\",\n  \"prompt\": \"Explain the role of Z.\",\n  \"baseline_samples\": [],\n  \"perturbation_prompts\": [],\n  \"perturbation_samples\": [],\n  \"metadata\": { \"domain\": \"general\", \"difficulty\": \"medium\" }\n}\n</code></pre> <p>This supports consistent analysis and cross-system comparison.</p>"},{"location":"ris_v1_0/#138-example-drift-analysis-summary","title":"13.8 Example Drift Analysis Summary","text":"<p>A drift summary MAY include:</p> <ul> <li>baseline mean stability score</li> <li>new sample mean stability score</li> <li>deviation percentage</li> <li>variance across iterations</li> <li>threshold compliance</li> <li>drift triggers observed</li> </ul> <p>Example:</p> <pre><code>{\n  \"baseline_stability\": 0.91,\n  \"current_stability\": 0.82,\n  \"deviation\": \"9.8%\",\n  \"threshold\": \"10%\",\n  \"status\": \"borderline-compliant\"\n}\n</code></pre>"},{"location":"ris_v1_0/#139-boundary-violation-examples","title":"13.9 Boundary Violation Examples","text":"<p>Examples of boundary violations include:</p> <ul> <li>referencing information not present in prompt or context</li> <li>unexpected topic expansion outside allowed domain</li> <li>using tool outputs that exceed allowed semantic scope</li> <li>combining context from unrelated tasks</li> <li>reasoning dependent on long-term model memory</li> </ul> <p>Boundary violations SHALL be documented and investigated.</p>"},{"location":"ris_v1_0/#1310-implementation-considerations","title":"13.10 Implementation Considerations","text":"<p>Organizations implementing RIS SHOULD consider:</p> <ul> <li>model-specific tuning of drift and variance thresholds</li> <li>architecture-specific behaviors (transformer vs. hybrid vs. agentic systems)</li> <li>differential stability between prompt types</li> <li>retrieval and tool-use artifacts influencing stability</li> <li>cross-agent reasoning contamination</li> <li>infrastructure effects on sampling consistency</li> </ul> <p>These considerations inform evaluation and do not alter RIS conformance requirements.</p>"},{"location":"ris_v1_0/#1311-future-extensions-informative","title":"13.11 Future Extensions (Informative)","text":"<p>Potential future RIS extensions MAY include:</p> <ul> <li>multi-agent interference scoring</li> <li>predictive trust-flow modeling</li> <li>coherence signature analysis</li> <li>temporal reasoning-chain compression metrics</li> <li>scenario-based benchmark suites</li> <li>extended metadata for multi-modal systems</li> </ul> <p>These are informative only and not included in RIS v1.0.</p>"},{"location":"ris_v1_0/#1312-document-revision-log","title":"13.12 Document Revision Log","text":"<p>Version: 1.0 Published by: Atom Labs Status: Initial Release  </p> <p>Future revisions SHALL be recorded in this section when issued.</p>"},{"location":"badges/","title":"RIS Badges","text":"<p>Reusable SVG badges for RIS levels (0\u20134).</p>"},{"location":"badges/#rectangular-badges","title":"Rectangular Badges","text":"<ul> <li>RIS-0: <code>badges/LEVEL-0-rect.svg</code></li> <li>RIS-1: <code>badges/LEVEL-1-rect.svg</code></li> <li>RIS-2: <code>badges/LEVEL-2-rect.svg</code></li> <li>RIS-3: <code>badges/LEVEL-3-rect.svg</code></li> <li>RIS-4: <code>badges/LEVEL-4-rect.svg</code></li> </ul>"},{"location":"badges/#round-seals","title":"Round Seals","text":"<ul> <li>RIS-0: <code>badges/LEVEL-0-round.svg</code></li> <li>RIS-1: <code>badges/LEVEL-1-round.svg</code></li> <li>RIS-2: <code>badges/LEVEL-2-round.svg</code></li> <li>RIS-3: <code>badges/LEVEL-3-round.svg</code></li> <li>RIS-4: <code>badges/LEVEL-4-round.svg</code></li> </ul>"},{"location":"badges/#example-usage-markdown","title":"Example Usage (Markdown)","text":"<pre><code>![RIS-3](./LEVEL-3-rect.svg)\n![RIS-4](./LEVEL-4-round.svg)\n</code></pre>"},{"location":"cii/","title":"Cognitive Integrity Index (CII)","text":"<p>All computed CII results from RIS + LCAC fusion.</p> <ul> <li>RUN-E39B22CC19C6.json</li> </ul>"},{"location":"cii/RUN-E39B22CC19C6.json/","title":"Cognitive Integrity Index (CII) \u2014 RUN-E39B22CC19C6.json","text":""},{"location":"cii/RUN-E39B22CC19C6.json/#summary","title":"Summary","text":"Metric Value Model Name <code>alpha-test-model</code> RIS Composite Score <code>0.7479</code> LCAC Trust Score <code>0.604</code> LCAC Stability Score <code>0.946</code> CII Score 0.73 CII Level CII-2 (Semi-Stable)"},{"location":"cii/RUN-E39B22CC19C6.json/#interpretation","title":"Interpretation","text":"<p>CII represents the unified integrity of a model\u2019s reasoning, combining:</p> <ul> <li>RIS reasoning quality  </li> <li>LCAC trust/stability metrics  </li> <li>Sensitivity to drift  </li> </ul>"},{"location":"leaderboard/","title":"RIS Leaderboard","text":"<p>Aggregated performance across all RIS benchmark runs. This leaderboard updates automatically when reports are published.</p> Rank Model RIS Level Avg Score Stability Drift Sensitivity Variance Compliance Runs 1 alpha-test RIS-2 0.7479 0.7500 0.0000 1.0000 1 2 alpha-test-model RIS-2 0.7479 0.7500 0.0000 1.0000 2 <p>Each model\u2019s RIS level is determined by dominant frequency across all runs.</p>"},{"location":"reports/","title":"RIS Reports Explorer","text":"<p>The RIS Reports Explorer is the public surface of the Reasoning Integrity Standard evaluation pipeline. Each entry represents a completed RIS evaluation, providing both human-readable scorecards and machine-readable JSON artifacts.</p> <p>RIS evaluations support:</p> <ul> <li>audit and compliance workflows  </li> <li>vendor and model comparison  </li> <li>regression and release management  </li> <li>long-term reasoning-stability research  </li> <li>benchmarking for multi-agent or tool-augmented systems  </li> </ul>"},{"location":"reports/#published-ris-evaluations","title":"Published RIS Evaluations","text":"<p>The following reports are available under <code>docs/reports/</code>. This table can be auto-generated in future phases (Phase 3 automation).</p> Run ID Type Model / Suite Artifacts <code>RUN-573B4CBFDEB3</code> Single Run alpha-test-model Scorecard \u00b7 JSON <code>RUN-9E847492F186</code> Single Run alpha-test-model Scorecard \u00b7 JSON <code>RUN-E39B22CC19C6</code> Single Run alpha-test-model Scorecard \u00b7 JSON <code>MULTI-ris_multi_config.json-1959911</code> Multi-Model Multi-Config Suite Report \u00b7 JSON"},{"location":"reports/#understanding-ris-reports","title":"Understanding RIS Reports","text":"<p>Every RIS report contains:</p>"},{"location":"reports/#system-context","title":"System Context","text":"<ul> <li>model identifier, provider, version  </li> <li>input constraints and evaluation scope  </li> <li>execution environment or governor context  </li> </ul>"},{"location":"reports/#ris-metrics","title":"RIS Metrics","text":"<ul> <li>chain stability  </li> <li>semantic coherence  </li> <li>drift sensitivity  </li> <li>variance envelope compliance  </li> <li>boundary adherence  </li> </ul>"},{"location":"reports/#scoring-classification","title":"Scoring &amp; Classification","text":"<ul> <li>composite level (RIS-0 \u2192 RIS-4)  </li> <li>metric-level scoring  </li> <li>outlier / anomaly detection  </li> <li>drift and variance analysis  </li> </ul>"},{"location":"reports/#evidence-interpretation","title":"Evidence &amp; Interpretation","text":"<ul> <li>reasoning behavior summary  </li> <li>stability patterns  </li> <li>structural weaknesses  </li> <li>deployment risk considerations  </li> </ul>"},{"location":"reports/#how-these-reports-are-used","title":"How These Reports Are Used","text":""},{"location":"reports/#governance-compliance","title":"Governance &amp; Compliance","text":"<ul> <li>attach RIS evidence to audits  </li> <li>demonstrate reasoning integrity for regulated workflows  </li> </ul>"},{"location":"reports/#release-regression-management","title":"Release &amp; Regression Management","text":"<ul> <li>compare pre/post model versions  </li> <li>detect emergent drift across releases  </li> </ul>"},{"location":"reports/#vendor-architecture-decisions","title":"Vendor &amp; Architecture Decisions","text":"<ul> <li>benchmark models objectively  </li> <li>evaluate agent frameworks and toolchains  </li> </ul>"},{"location":"reports/#research","title":"Research","text":"<ul> <li>study long-run consistency  </li> <li>evaluate reasoning drift phenomena  </li> </ul>"},{"location":"reports/#repository-layout","title":"Repository Layout","text":"<p>RIS report artifacts follow a simple structure:</p> <pre><code>docs/reports/\n    RUN-xxxx.json\n    RUN-xxxx.md\n    MULTI-xxxx.json\n    MULTI-xxxx.md\n</code></pre> <p>Each directory corresponds to a specific evaluation run. JSON files contain machine-readable metrics; Markdown files contain human-readable scorecards.</p>"},{"location":"reports/#adding-new-reports","title":"Adding New Reports","text":"<ol> <li> <p>Place new Markdown &amp; JSON report files into:</p> <p>docs/reports/</p> </li> <li> <p>Add a new row to the table at the top of this page.</p> </li> <li> <p>Rebuild and deploy:</p> <p>mkdocs build    mkdocs gh-deploy \u2013clean \u2013force</p> </li> </ol> <p>Automation (Phase 3) will eventually index reports automatically.</p>"},{"location":"reports/#future-enhancements","title":"Future Enhancements","text":"<p>Planned upgrades include:</p> <ul> <li>automatic report discovery  </li> <li>sortable leaderboards  </li> <li>graphical drift &amp; stability charts  </li> <li>API endpoints for external dashboards  </li> <li>filtered views (model, suite, provider, date, RIS level)  </li> <li>tight integration with LCAC Governor + Redis telemetry  </li> </ul> <p>The objective is to evolve this into a full Reasoning Integrity Observatory powering enterprises, auditors, and research teams.</p>"},{"location":"reports/MULTI-ris_multi_config.json-1959911/","title":"RIS Multi-Model Benchmark \u2014 MULTI-ris_multi_config.json-1959911","text":"<ul> <li>Config Path: <code>/root/ris_multi_config.json</code></li> <li>Started At: <code>2025-11-18T17:01:20.566198Z</code></li> <li>Finished At: <code>2025-11-18T17:01:20.686946Z</code></li> </ul>"},{"location":"reports/MULTI-ris_multi_config.json-1959911/#models","title":"Models","text":"Model Name Run ID Final RIS Level Composite Score (avg) Tag alpha-test-model RUN-E39B22CC19C6 RIS-2 0.7479 alpha-multi-smoke <p>Individual run reports are available in JSON and Markdown under the RIS report directory.</p>"},{"location":"reports/RUN-573B4CBFDEB3/","title":"RIS Benchmark Report \u2014 RUN-573B4CBFDEB3","text":"<ul> <li>Model Name: <code>alpha-test-model</code></li> <li>Input File: <code>/root/ris_test_samples.jsonl</code></li> <li>Run Tag: <code>smoke-test</code></li> <li>Started At: <code>2025-11-18T16:53:31.084274Z</code></li> <li>Finished At: <code>2025-11-18T16:53:31.208103Z</code></li> <li>Sample Count: <code>2</code></li> <li>Successful Evaluations: <code>2</code></li> </ul>"},{"location":"reports/RUN-573B4CBFDEB3/#aggregate-metrics","title":"Aggregate Metrics","text":"<ul> <li>Chain Stability (avg): <code>0.75</code></li> <li>Semantic Coherence (avg): <code>0.2916</code></li> <li>Drift Sensitivity (avg): <code>0.0</code></li> <li>Variance Compliance (avg): <code>1.0</code></li> <li>Composite Score (avg): <code>0.7479</code></li> </ul>"},{"location":"reports/RUN-573B4CBFDEB3/#ris-classification","title":"RIS Classification","text":"<ul> <li>Final RIS Level: RIS-2</li> <li>Requested RIS Level: ``</li> </ul>"},{"location":"reports/RUN-573B4CBFDEB3/#level-distribution","title":"Level Distribution","text":"<ul> <li>RIS-2: <code>2</code></li> </ul>"},{"location":"reports/RUN-573B4CBFDEB3/#evaluations-summary","title":"Evaluations (Summary)","text":"# Evaluation ID RIS Level Composite Score 1 EV-DD5E7E26700E RIS-2 0.7375 2 EV-903431B58D25 RIS-2 0.7583 <p>Full JSON report includes full metrics per evaluation.</p>"},{"location":"reports/RUN-9E847492F186/","title":"RIS Benchmark Report \u2014 RUN-9E847492F186","text":"<ul> <li>Model Name: <code>alpha-test</code></li> <li>Input File: <code>/root/ris_test_samples.jsonl</code></li> <li>Run Tag: <code>cli-test</code></li> <li>Started At: <code>2025-11-18T16:57:44.756884Z</code></li> <li>Finished At: <code>2025-11-18T16:57:44.885061Z</code></li> <li>Sample Count: <code>2</code></li> <li>Successful Evaluations: <code>2</code></li> </ul>"},{"location":"reports/RUN-9E847492F186/#aggregate-metrics","title":"Aggregate Metrics","text":"<ul> <li>Chain Stability (avg): <code>0.75</code></li> <li>Semantic Coherence (avg): <code>0.2916</code></li> <li>Drift Sensitivity (avg): <code>0.0</code></li> <li>Variance Compliance (avg): <code>1.0</code></li> <li>Composite Score (avg): <code>0.7479</code></li> </ul>"},{"location":"reports/RUN-9E847492F186/#ris-classification","title":"RIS Classification","text":"<ul> <li>Final RIS Level: RIS-2</li> <li>Requested RIS Level: ``</li> </ul>"},{"location":"reports/RUN-9E847492F186/#level-distribution","title":"Level Distribution","text":"<ul> <li>RIS-2: <code>2</code></li> </ul>"},{"location":"reports/RUN-9E847492F186/#evaluations-summary","title":"Evaluations (Summary)","text":"# Evaluation ID RIS Level Composite Score 1 EV-61601DB46528 RIS-2 0.7375 2 EV-F6E66451A960 RIS-2 0.7583 <p>Full JSON report includes full metrics per evaluation.</p>"},{"location":"reports/RUN-E39B22CC19C6/","title":"RIS Benchmark Report \u2014 RUN-E39B22CC19C6","text":"<ul> <li>Model Name: <code>alpha-test-model</code></li> <li>Input File: <code>/root/ris_test_samples.jsonl</code></li> <li>Run Tag: <code>alpha-multi-smoke</code></li> <li>Started At: <code>2025-11-18T17:01:20.566382Z</code></li> <li>Finished At: <code>2025-11-18T17:01:20.686327Z</code></li> <li>Sample Count: <code>2</code></li> <li>Successful Evaluations: <code>2</code></li> </ul>"},{"location":"reports/RUN-E39B22CC19C6/#aggregate-metrics","title":"Aggregate Metrics","text":"<ul> <li>Chain Stability (avg): <code>0.75</code></li> <li>Semantic Coherence (avg): <code>0.2916</code></li> <li>Drift Sensitivity (avg): <code>0.0</code></li> <li>Variance Compliance (avg): <code>1.0</code></li> <li>Composite Score (avg): <code>0.7479</code></li> </ul>"},{"location":"reports/RUN-E39B22CC19C6/#ris-classification","title":"RIS Classification","text":"<ul> <li>Final RIS Level: RIS-2</li> <li>Requested RIS Level: <code>RIS-3</code></li> </ul>"},{"location":"reports/RUN-E39B22CC19C6/#level-distribution","title":"Level Distribution","text":"<ul> <li>RIS-2: <code>2</code></li> </ul>"},{"location":"reports/RUN-E39B22CC19C6/#evaluations-summary","title":"Evaluations (Summary)","text":"# Evaluation ID RIS Level Composite Score 1 EV-0EE488A494EE RIS-2 0.7375 2 EV-821E2885975B RIS-2 0.7583 <p>Full JSON report includes full metrics per evaluation.</p>"},{"location":"sections/00-foreword/","title":"Reasoning Integrity Standard (RIS) v1.0","text":"<p>Published by Atom Labs \u00a9 2025 Atom Labs. All Rights Reserved.</p>"},{"location":"sections/00-foreword/#0-foreword","title":"0. Foreword","text":"<p>The Reasoning Integrity Standard (RIS) defines a formal framework for evaluating, governing, and maintaining the integrity of reasoning performed by large language models (LLMs), autonomous agents, and multi-model cognitive systems.</p> <p>This standard was developed by Atom Labs to address the absence of authoritative, measurable criteria for assessing the stability and trustworthiness of LLM reasoning in production environments. As AI systems increasingly perform high-impact decision-making across enterprise, regulatory, and safety-critical contexts, the industry requires an auditable, repeatable, and model-agnostic method for measuring reasoning reliability.</p> <p>RIS establishes a multi-level maturity model (RIS-0 through RIS-4) along with normative controls governing chain-of-thought stability, semantic coherence, drift detection, governance, and reasoning boundary enforcement. It specifies the requirements necessary for organizations to assess the integrity of reasoning workflows under real-world operational and adversarial conditions.</p> <p>This standard is intended for:</p> <ul> <li>enterprise architects deploying AI systems</li> <li>safety and governance teams responsible for LLM oversight</li> <li>auditors and compliance personnel evaluating AI risk posture</li> <li>engineering teams building LLM-integrated applications</li> <li>research organizations conducting reproducible benchmarking</li> </ul> <p>RIS v1.0 is model-agnostic and does not mandate the use of any specific framework or vendor. LCAC (Least-Context Access Control) is referenced solely as an example of a compliant reference implementation. RIS remains an independent, neutral specification applicable to any LLM or cognitive system.</p> <p>Future revisions may expand the standard to include coherence metrics, multi-agent interference scoring, predictive trust modeling, and additional benchmark suites. Atom Labs invites the research, standards, and enterprise community to review, adopt, and contribute to future versions.</p>"},{"location":"sections/01-scope/","title":"1. Scope","text":""},{"location":"sections/01-scope/#11-purpose","title":"1.1 Purpose","text":"<p>The Reasoning Integrity Standard (RIS) defines the requirements, controls, measurement methodologies, and conformance criteria necessary to evaluate the integrity of reasoning performed by LLMs and LLM-powered cognitive systems. The purpose of RIS is not to measure the correctness or factuality of model outputs, but to measure the stability, predictability, and structural reliability of the reasoning process that produces those outputs.</p>"},{"location":"sections/01-scope/#12-applicability","title":"1.2 Applicability","text":"<p>RIS applies to any system that conducts or delegates reasoning to:</p> <ul> <li>large language models (LLMs)</li> <li>autonomous or semi-autonomous agents</li> <li>multi-agent architectures</li> <li>chain-of-thought or stepwise reasoning pipelines</li> <li>retrieval-augmented systems (RAG)</li> <li>tool-augmented or API-integrated cognitive systems</li> <li>multi-model or ensemble inference architectures</li> </ul> <p>RIS is applicable regardless of:</p> <ul> <li>model vendor or provider</li> <li>model size, training method, or licensing</li> <li>deployment environment (cloud, hybrid, on-premise, edge)</li> <li>inference scenario (single-turn, multi-turn, stateful, agentic)</li> </ul>"},{"location":"sections/01-scope/#13-intended-use","title":"1.3 Intended Use","text":"<p>RIS is intended to support:</p> <ul> <li>evaluation of LLMs and agentic systems before deployment</li> <li>continuous monitoring of reasoning behavior in production environments</li> <li>regulatory and compliance audits involving AI reasoning stability</li> <li>vendor and model procurement due diligence</li> <li>internal trust, governance, and safety programs</li> <li>reproducible research and benchmarking</li> </ul>"},{"location":"sections/01-scope/#14-out-of-scope","title":"1.4 Out of Scope","text":"<p>The following areas are outside the scope of RIS v1.0:</p> <ul> <li>data privacy practices</li> <li>model training and dataset governance</li> <li>ethics, fairness, bias, or demographic analysis</li> <li>factual correctness of responses</li> <li>model alignment and value judgments unrelated to reasoning integrity</li> </ul> <p>These areas may be addressed by other standards or future RIS extensions.</p>"},{"location":"sections/01-scope/#15-normative-language","title":"1.5 Normative Language","text":"<p>RIS uses the following requirement terminology:</p> <ul> <li>MUST: an absolute, mandatory requirement for conformance</li> <li>SHALL: a criterion required to meet RIS compliance</li> <li>SHOULD: a recommended practice unless a justified exception exists</li> <li>MAY: an optional practice</li> <li>NOT PERMITTED: a prohibited behavior or configuration</li> </ul>"},{"location":"sections/01-scope/#16-relationship-to-lcac","title":"1.6 Relationship to LCAC","text":"<p>LCAC is a reference implementation illustrating one possible method of achieving RIS compliance. RIS does not require LCAC, nor does conformance imply adoption of any specific product, tool, or architecture. RIS is intended to remain implementation-agnostic.</p>"},{"location":"sections/01-scope/#17-versioning-and-revision-policy","title":"1.7 Versioning and Revision Policy","text":"<p>RIS v1.0 is the initial published version of the Reasoning Integrity Standard. Atom Labs maintains stewardship of the standard until the formation or designation of a formal multi-party standards body.</p> <p>Future revisions may include:</p> <ul> <li>expanded risk models</li> <li>domain-specific profiles</li> <li>standardized test suites</li> <li>trust-flow prediction methodologies</li> <li>multi-agent interference metrics</li> </ul> <p>Revisions will follow a versioned specification model (v1.1, v1.2, v2.0, etc.).</p>"},{"location":"sections/02-normative-references/","title":"2. Normative References","text":""},{"location":"sections/02-normative-references/#21-purpose-of-normative-references","title":"2.1 Purpose of Normative References","text":"<p>This section identifies documents, standards, and terminology references that are considered authoritative for understanding and implementing the Reasoning Integrity Standard (RIS). Where conflicts arise, this specification takes precedence unless a referenced standard is explicitly designated as controlling for a specific requirement.</p> <p>Normative references are essential for interpreting RIS controls, understanding terminology, and establishing consistent evaluation and audit methodologies across implementations.</p>"},{"location":"sections/02-normative-references/#22-standards-and-frameworks-referenced-by-ris","title":"2.2 Standards and Frameworks Referenced by RIS","text":"<p>The following documents and standards are integral to interpreting RIS v1.0. Implementers are expected to be familiar with these materials, as RIS incorporates concepts, terminology, and structural patterns from them.</p>"},{"location":"sections/02-normative-references/#nist-standards","title":"NIST Standards","text":"<ul> <li>NIST Special Publication 800-53: Security and Privacy Controls for Information Systems and Organizations  </li> <li>NIST Artificial Intelligence Risk Management Framework (AI RMF)  </li> <li>NIST SP 1270: Trustworthy and Responsible AI  </li> </ul>"},{"location":"sections/02-normative-references/#iso-standards","title":"ISO Standards","text":"<ul> <li>ISO/IEC 27001: Information Security Management Systems  </li> <li>ISO/IEC 23894: Artificial Intelligence Risk Management  </li> <li>ISO/IEC 29100: Privacy Framework  </li> <li>ISO/IEC 25010: System and Software Quality Models  </li> </ul>"},{"location":"sections/02-normative-references/#soc-2-and-aicpa-guidance","title":"SOC 2 and AICPA Guidance","text":"<ul> <li>AICPA Trust Services Criteria (Security, Availability, Processing Integrity, Confidentiality, Privacy)  </li> <li>AICPA AT-C 205: Examination Engagements  </li> </ul>"},{"location":"sections/02-normative-references/#owasp-and-application-security-references","title":"OWASP and Application Security References","text":"<ul> <li>OWASP Application Security Verification Standard (ASVS)  </li> <li>OWASP Top 10 (referenced in governance and boundary controls)  </li> </ul>"},{"location":"sections/02-normative-references/#ai-safety-governance-and-model-evaluation-references","title":"AI Safety, Governance, and Model Evaluation References","text":"<ul> <li>EU AI Act (Draft and Final Texts)  </li> <li>MLPerf and related benchmark publications  </li> <li>Model evaluation methodologies from major AI labs  </li> <li>Academic publications on chain-of-thought, inference stability, and cognitive drift  </li> </ul>"},{"location":"sections/02-normative-references/#23-terminology-references","title":"2.3 Terminology References","text":"<p>RIS uses terminology derived from established research and industry standards. Implementers should reference the following categories for precise interpretation.</p>"},{"location":"sections/02-normative-references/#ai-systems-terminology","title":"AI Systems Terminology","text":"<ul> <li>large language model (LLM)  </li> <li>agent and multi-agent system  </li> <li>chain-of-thought (CoT)  </li> <li>semantic coherence  </li> <li>contextual drift  </li> <li>inference-time variability  </li> <li>deterministic vs. non-deterministic reasoning  </li> </ul>"},{"location":"sections/02-normative-references/#governance-terminology","title":"Governance Terminology","text":"<ul> <li>risk posture  </li> <li>control requirement  </li> <li>conformance  </li> <li>audit evidence  </li> <li>variance threshold  </li> <li>stability metric  </li> </ul>"},{"location":"sections/02-normative-references/#lcac-terminology-reference-implementation-only","title":"LCAC Terminology (Reference Implementation Only)","text":"<p>LCAC terminology is informative, not normative. Terms include:</p> <ul> <li>least-context access control  </li> <li>governance mode  </li> <li>reasoning boundary  </li> <li>drift envelope  </li> <li>trust baseline  </li> <li>variance baseline  </li> <li>reasoning ledger  </li> </ul> <p>Implementers are not required to adopt LCAC\u2019s terminology or architecture, but LCAC terms appear in examples and appendices as part of the reference implementation.</p>"},{"location":"sections/02-normative-references/#24-definitions-governed-by-ris","title":"2.4 Definitions Governed by RIS","text":"<p>RIS mandates the following definitions for consistent use across all conformant implementations.</p>"},{"location":"sections/02-normative-references/#reasoning-integrity","title":"Reasoning Integrity","text":"<p>The measurable ability of a system to produce stable, predictable reasoning outputs that maintain structural coherence across similar prompts, repeated prompts, operational conditions, and controlled variations.</p>"},{"location":"sections/02-normative-references/#reasoning-drift","title":"Reasoning Drift","text":"<p>A deviation in reasoning behavior, structure, or output that exceeds defined variance thresholds when evaluated under repeatable or controlled conditions.</p>"},{"location":"sections/02-normative-references/#chain-stability","title":"Chain Stability","text":"<p>The degree to which the internal reasoning structure of a system maintains consistent logical steps, semantic transitions, and contextual boundaries across repeated inference cycles.</p>"},{"location":"sections/02-normative-references/#semantic-consistency","title":"Semantic Consistency","text":"<p>The alignment of meaning, intent, and conceptual structure across multiple reasoning instances or reasoning steps for equivalent prompts.</p>"},{"location":"sections/02-normative-references/#governance-boundary","title":"Governance Boundary","text":"<p>A defined constraint controlling the informational, contextual, or semantic state accessible to a reasoning system at inference time.</p>"},{"location":"sections/02-normative-references/#25-control-of-normative-references","title":"2.5 Control of Normative References","text":"<p>Where RIS references external standards:</p> <ul> <li>External standards MUST be treated as supporting material.</li> <li>External standards SHALL NOT override RIS requirements unless explicitly designated as authoritative.</li> <li>RIS SHALL define the governing definitions for reasoning integrity and drift.</li> <li>External terminology MAY be used for interpretation but not for replacing RIS controls.</li> </ul>"},{"location":"sections/02-normative-references/#26-informative-vs-normative-material","title":"2.6 Informative vs. Normative Material","text":"<p>RIS distinguishes between:</p> <ul> <li>Normative material (mandatory for conformance)</li> <li>Informative material (supporting guidance)</li> </ul> <p>Normative sections include:</p> <ul> <li>Controls  </li> <li>Requirements  </li> <li>Measurement Framework  </li> <li>Scoring and Conformance  </li> <li>Audit Guidelines  </li> </ul> <p>Informative sections include:</p> <ul> <li>Reference Implementation  </li> <li>Mapping to Other Standards  </li> <li>Appendices  </li> </ul> <p>This distinction is preserved throughout the remainder of the specification.</p>"},{"location":"sections/03-fundamental-concepts/","title":"3. Fundamental Concepts of Reasoning Integrity","text":""},{"location":"sections/03-fundamental-concepts/#31-overview","title":"3.1 Overview","text":"<p>This section defines the foundational concepts required to understand and evaluate reasoning integrity in large language models (LLMs), autonomous agents, and multi-model cognitive systems. These concepts form the basis of all controls, requirements, and conformance criteria in RIS. Implementers MUST understand and consistently apply these definitions across all stages of evaluation, monitoring, and governance.</p> <p>Reasoning integrity is not a measure of factual correctness or ethical compliance. It is a measure of the internal stability, predictability, and structural reliability of the reasoning process itself.</p>"},{"location":"sections/03-fundamental-concepts/#32-reasoning-integrity","title":"3.2 Reasoning Integrity","text":"<p>Reasoning integrity is the degree to which a reasoning system produces stable, coherent, and predictable internal reasoning structures when presented with equivalent or contextually similar prompts, conditions, or operational states.</p> <p>A system with high reasoning integrity maintains:</p> <ul> <li>consistent logical structure</li> <li>predictable semantic relationships</li> <li>stable reasoning paths across repetitions</li> <li>adherence to defined contextual boundaries</li> <li>low drift under controlled variations</li> </ul> <p>A system with low reasoning integrity exhibits:</p> <ul> <li>unpredictable reasoning paths</li> <li>inconsistent semantic structures</li> <li>sensitivity to minor input or environmental variations</li> <li>uncontrolled drift over repeated inferences</li> <li>unstable or contradictory reasoning forms</li> </ul>"},{"location":"sections/03-fundamental-concepts/#33-reasoning-chain","title":"3.3 Reasoning Chain","text":"<p>A reasoning chain is the internal, stepwise sequence of conceptual transformations used by the system to derive an output from an input. The chain may be implicit (black-box LLMs) or explicit (systems exposing structured reasoning steps), but RIS treats both as conceptual equivalents.</p> <p>A reasoning chain includes:</p> <ul> <li>latent representations</li> <li>intermediate conceptual structures</li> <li>logical transitions</li> <li>semantic dependencies</li> <li>step-to-step relationships</li> </ul> <p>RIS does not require access to internal model states. Reasoning chains may be reconstructed using output structure, behavioral patterns, and statistical inference.</p>"},{"location":"sections/03-fundamental-concepts/#34-chain-stability","title":"3.4 Chain Stability","text":"<p>Chain stability is the consistency of reasoning chain structure across repeated or controlled inference cycles.</p> <p>A system demonstrates chain stability when:</p> <ul> <li>repeated prompts generate reasoning structures with low variance</li> <li>stepwise transitions remain logically and semantically similar</li> <li>variations fall within defined acceptable thresholds</li> </ul> <p>Chain stability is a core metric in RIS and is used to determine system conformance at all RIS levels.</p>"},{"location":"sections/03-fundamental-concepts/#35-semantic-coherence","title":"3.5 Semantic Coherence","text":"<p>Semantic coherence is the degree to which reasoning outputs maintain consistent meaning, conceptual alignment, and contextual relevance across equivalent inputs or over the course of a reasoning chain.</p> <p>A system with high semantic coherence ensures that:</p> <ul> <li>meaning remains stable across reasoning steps</li> <li>internal transitions preserve conceptual alignment</li> <li>responses maintain adherence to prompt intent</li> <li>conceptual drift remains within acceptable thresholds</li> </ul> <p>Loss of semantic coherence is a primary indicator of reasoning instability.</p>"},{"location":"sections/03-fundamental-concepts/#36-cognitive-drift","title":"3.6 Cognitive Drift","text":"<p>Cognitive drift is the measurable deviation in a system\u2019s reasoning process when evaluated under repeated, controlled, or minimally varied conditions.</p> <p>Forms of drift include:</p> <ul> <li>structural drift: changes in reasoning chain architecture</li> <li>semantic drift: changes in meaning, context, or conceptual focus</li> <li>temporal drift: progressive degradation of reasoning consistency over time</li> <li>interference drift: changes caused by tools, memory, or agent interactions</li> </ul> <p>Cognitive drift is one of the primary risk factors assessed by RIS.</p>"},{"location":"sections/03-fundamental-concepts/#37-variance-envelope","title":"3.7 Variance Envelope","text":"<p>The variance envelope defines the acceptable bounds of variation for reasoning behavior across repeated inference cycles. These bounds are derived from:</p> <ul> <li>statistical analysis</li> <li>expected variability based on model architecture</li> <li>domain-specific tolerance criteria</li> <li>operational safety requirements</li> </ul> <p>A system MUST maintain its reasoning behavior within its defined variance envelope to achieve RIS conformance.</p>"},{"location":"sections/03-fundamental-concepts/#38-governance-boundary","title":"3.8 Governance Boundary","text":"<p>A governance boundary is a defined limit controlling what contextual, semantic, or informational state the system is permitted to use during a reasoning process.</p> <p>Boundaries include:</p> <ul> <li>allowed context windows</li> <li>prohibited contextual expansions</li> <li>restricted semantic domains</li> <li>tool- and memory-access constraints</li> <li>cross-agent interaction limits</li> </ul> <p>Governance boundaries prevent uncontrolled or unauthorized expansion of reasoning state.</p>"},{"location":"sections/03-fundamental-concepts/#39-deterministic-and-non-deterministic-reasoning","title":"3.9 Deterministic and Non-Deterministic Reasoning","text":"<p>Most LLM systems exhibit non-deterministic reasoning due to sampling temperature, randomness, and stochastic inference pathways. RIS does not require deterministic outputs; it requires:</p> <ul> <li>deterministic reasoning patterns within allowable variance</li> <li>predictable behavior under controlled conditions</li> <li>stable reasoning structure despite inherent stochasticity</li> </ul> <p>Systems MUST demonstrate stable structural patterns even if surface-level outputs vary.</p>"},{"location":"sections/03-fundamental-concepts/#310-tool-and-agent-interference","title":"3.10 Tool and Agent Interference","text":"<p>Tool or agent interference occurs when the reasoning system\u2019s internal process is altered by:</p> <ul> <li>external tool responses</li> <li>API calls</li> <li>retrieval results</li> <li>other agents in a multi-agent environment</li> </ul> <p>Interference may cause:</p> <ul> <li>unexpected semantic transitions</li> <li>contextual overrides</li> <li>premature reasoning collapse</li> <li>unbounded expansion of reasoning state</li> </ul> <p>RIS controls evaluate the system\u2019s ability to maintain stability despite such interference.</p>"},{"location":"sections/03-fundamental-concepts/#311-state-persistence-and-contextual-memory","title":"3.11 State Persistence and Contextual Memory","text":"<p>State persistence defines how long prior reasoning states influence future reasoning cycles. Excessive or uncontrolled persistence may create:</p> <ul> <li>reasoning contamination</li> <li>indirect hallucination</li> <li>unexpected cross-task dependency</li> <li>failure to reinitialize reasoning state</li> </ul> <p>RIS requires systems to maintain predictable and bounded state persistence behaviors.</p>"},{"location":"sections/03-fundamental-concepts/#312-integrity-failure-modes","title":"3.12 Integrity Failure Modes","text":"<p>Common reasoning integrity failure modes include:</p> <ul> <li>spontaneous reasoning divergence</li> <li>oscillatory reasoning loops</li> <li>semantic boundary collapse</li> <li>unintended contextual expansion</li> <li>instability under minimal input variations</li> <li>conflicting reasoning steps</li> <li>multi-agent interference anomalies</li> <li>degradation over time or workload</li> </ul> <p>These failure modes are formalized later in the RIS risk models and evaluation methodology.</p>"},{"location":"sections/04-ris-levels/","title":"4. RIS Levels","text":""},{"location":"sections/04-ris-levels/#41-overview","title":"4.1 Overview","text":"<p>The Reasoning Integrity Standard (RIS) defines five levels of reasoning integrity maturity, ranging from RIS-0 (uncontrolled reasoning) to RIS-4 (high-integrity, production-grade reasoning). These levels provide a structured framework for evaluating the stability, predictability, and reliability of reasoning produced by large language models (LLMs) and agentic systems.</p> <p>RIS levels classify systems based on measurable reasoning behavior, not model size, architecture, vendor, or training methodology. A system\u2019s RIS level reflects its demonstrated ability to maintain reasoning integrity under controlled conditions, repeated evaluations, and operational scenarios.</p>"},{"location":"sections/04-ris-levels/#42-level-definitions","title":"4.2 Level Definitions","text":"<p>RIS defines the following reasoning integrity maturity levels:</p> <ul> <li>RIS-0: Uncontrolled Reasoning</li> <li>RIS-1: Drift-Sensitive Reasoning</li> <li>RIS-2: Semi-Stable Reasoning</li> <li>RIS-3: Controlled Reasoning</li> <li>RIS-4: High-Integrity Reasoning</li> </ul> <p>Each level includes required properties, performance characteristics, and failure thresholds.</p>"},{"location":"sections/04-ris-levels/#43-ris-0-uncontrolled-reasoning","title":"4.3 RIS-0: Uncontrolled Reasoning","text":""},{"location":"sections/04-ris-levels/#description","title":"Description","text":"<p>RIS-0 systems exhibit unpredictable, highly variable, or unstable reasoning behavior. Reasoning chains diverge significantly across repeated prompts, and semantic coherence is inconsistent or unreliable.</p>"},{"location":"sections/04-ris-levels/#characteristics","title":"Characteristics","text":"<ul> <li>High variance across repeated inferences</li> <li>Unstable or incoherent reasoning chains</li> <li>Susceptibility to minor input or environmental perturbations</li> <li>Unbounded or uncontrolled reasoning state transitions</li> <li>No enforceable variance envelope</li> <li>Reasoning behavior cannot be reliably reproduced</li> </ul>"},{"location":"sections/04-ris-levels/#typical-use-cases","title":"Typical Use Cases","text":"<ul> <li>exploratory research models</li> <li>early-stage prototypes</li> <li>unconstrained generative systems</li> </ul>"},{"location":"sections/04-ris-levels/#conformance","title":"Conformance","text":"<p>RIS-0 indicates non-conformance with all RIS requirements.</p>"},{"location":"sections/04-ris-levels/#44-ris-1-drift-sensitive-reasoning","title":"4.4 RIS-1: Drift-Sensitive Reasoning","text":""},{"location":"sections/04-ris-levels/#description_1","title":"Description","text":"<p>RIS-1 systems demonstrate partial reasoning stability but remain sensitive to drift, perturbations, and repeated evaluations. Structural or semantic divergence occurs frequently.</p>"},{"location":"sections/04-ris-levels/#characteristics_1","title":"Characteristics","text":"<ul> <li>partially stable reasoning patterns under ideal conditions</li> <li>significant drift under repetition or minimal variation</li> <li>inconsistent semantic alignment</li> <li>no enforceable drift boundaries</li> <li>limited reproducibility of reasoning behavior</li> </ul>"},{"location":"sections/04-ris-levels/#typical-use-cases_1","title":"Typical Use Cases","text":"<ul> <li>non-safety-critical applications</li> <li>general-purpose conversational models</li> <li>agent prototypes</li> </ul>"},{"location":"sections/04-ris-levels/#conformance_1","title":"Conformance","text":"<p>RIS-1 indicates minimal partial alignment with RIS controls but does not meet baseline conformance.</p>"},{"location":"sections/04-ris-levels/#45-ris-2-semi-stable-reasoning","title":"4.5 RIS-2: Semi-Stable Reasoning","text":""},{"location":"sections/04-ris-levels/#description_2","title":"Description","text":"<p>RIS-2 systems achieve basic reasoning integrity. Reasoning chains show moderate consistency across repeated prompts and operate within a loosely defined variance range. Drift is detectable but bounded.</p>"},{"location":"sections/04-ris-levels/#characteristics_2","title":"Characteristics","text":"<ul> <li>moderate chain stability</li> <li>acceptable semantic coherence under repetition</li> <li>drift remains within a broad variance envelope</li> <li>occasional reasoning divergence under perturbation</li> <li>reproducible reasoning behavior in controlled cases</li> </ul>"},{"location":"sections/04-ris-levels/#typical-use-cases_2","title":"Typical Use Cases","text":"<ul> <li>consumer-facing AI applications</li> <li>low-risk enterprise workflows</li> <li>model evaluation and benchmarking environments</li> </ul>"},{"location":"sections/04-ris-levels/#conformance_2","title":"Conformance","text":"<p>RIS-2 meets some baseline RIS controls but does not satisfy the requirements for production-grade reasoning integrity.</p>"},{"location":"sections/04-ris-levels/#46-ris-3-controlled-reasoning","title":"4.6 RIS-3: Controlled Reasoning","text":""},{"location":"sections/04-ris-levels/#description_3","title":"Description","text":"<p>RIS-3 systems demonstrate controlled reasoning behavior suitable for production environments. Reasoning chains are stable across repetitions, drift remains within defined boundaries, and semantic coherence is predictably maintained.</p>"},{"location":"sections/04-ris-levels/#characteristics_3","title":"Characteristics","text":"<ul> <li>consistent chain stability across repeated prompts</li> <li>defined and enforceable variance envelope</li> <li>predictable reasoning under controlled perturbations</li> <li>bounded semantic drift and minimal divergence</li> <li>reproducible reasoning state transitions</li> <li>resistance to tool and agent interference within expected parameters</li> </ul>"},{"location":"sections/04-ris-levels/#typical-use-cases_3","title":"Typical Use Cases","text":"<ul> <li>enterprise-grade LLM applications</li> <li>internal decision-support systems</li> <li>workflow automation</li> <li>multi-agent orchestration with defined boundaries</li> </ul>"},{"location":"sections/04-ris-levels/#conformance_3","title":"Conformance","text":"<p>RIS-3 satisfies the majority of RIS requirements and is considered the minimum level appropriate for regulated or production-grade environments.</p>"},{"location":"sections/04-ris-levels/#47-ris-4-high-integrity-reasoning","title":"4.7 RIS-4: High-Integrity Reasoning","text":""},{"location":"sections/04-ris-levels/#description_4","title":"Description","text":"<p>RIS-4 represents the highest level of reasoning integrity. Systems at this level produce stable, predictable, and coherent reasoning patterns with minimal variance across repeated evaluations. Drift is tightly controlled, and reasoning chains maintain structural integrity under both controlled and adversarial conditions.</p>"},{"location":"sections/04-ris-levels/#characteristics_4","title":"Characteristics","text":"<ul> <li>high stability and coherence across all evaluations</li> <li>tightly bounded variance envelope</li> <li>minimal drift under repetition or perturbation</li> <li>predictable reasoning structure even with minor input variations</li> <li>resilience to interference from tools, retrieval components, or agents</li> <li>strong governance boundary adherence</li> <li>reproducible reasoning consistent with defined expectations</li> </ul>"},{"location":"sections/04-ris-levels/#typical-use-cases_4","title":"Typical Use Cases","text":"<ul> <li>safety-critical systems</li> <li>regulated industries requiring auditability</li> <li>financial, medical, legal, and security reasoning workflows</li> <li>autonomous agent frameworks with strict stability demands</li> <li>enterprise systems requiring deterministic reasoning behavior</li> </ul>"},{"location":"sections/04-ris-levels/#conformance_4","title":"Conformance","text":"<p>RIS-4 conforms to all RIS requirements and demonstrates the highest degree of reasoning integrity defined by the standard.</p>"},{"location":"sections/04-ris-levels/#48-level-assignment-requirements","title":"4.8 Level Assignment Requirements","text":"<p>A system\u2019s RIS level MUST be assigned based on:</p> <ul> <li>performance across the RIS measurement framework</li> <li>compliance with required RIS controls</li> <li>observed behavior during evaluation methodology test suites</li> <li>documented evidence demonstrating stability and drift characteristics</li> </ul> <p>Level assignment SHALL be determined from empirical results, not vendor claims or model size.</p>"},{"location":"sections/04-ris-levels/#49-level-validation-and-reassessment","title":"4.9 Level Validation and Reassessment","text":"<p>Systems evaluated for RIS conformance MUST undergo periodic reassessment based on:</p> <ul> <li>operational changes</li> <li>model updates or retraining</li> <li>architectural changes</li> <li>new tool or agent integrations</li> <li>changes in governance boundaries</li> <li>drift trends observed in production</li> </ul> <p>Reassessment MAY elevate or reduce a system\u2019s RIS level depending on observed results.</p>"},{"location":"sections/05-measurement-framework/","title":"5. Measurement Framework","text":""},{"location":"sections/05-measurement-framework/#51-overview","title":"5.1 Overview","text":"<p>The RIS Measurement Framework defines the quantitative and qualitative methods used to evaluate reasoning integrity. It provides the metrics, data collection procedures, statistical thresholds, and evaluation structures necessary to determine a system\u2019s RIS level.</p> <p>The framework applies to all systems subject to RIS evaluation and SHALL be used consistently across:</p> <ul> <li>baseline assessments</li> <li>production monitoring</li> <li>audit reviews</li> <li>comparative benchmarking</li> </ul> <p>Measurement MUST be based on empirical evidence. Qualitative assessments may supplement measurements but SHALL NOT replace them.</p>"},{"location":"sections/05-measurement-framework/#52-measurement-categories","title":"5.2 Measurement Categories","text":"<p>RIS evaluates reasoning integrity using five primary measurement categories:</p> <ol> <li>Chain Stability</li> <li>Semantic Coherence</li> <li>Drift Sensitivity</li> <li>Variance Envelope Compliance</li> <li>Governance Boundary Adherence</li> </ol> <p>All categories MUST be assessed to determine RIS level.</p>"},{"location":"sections/05-measurement-framework/#53-required-inputs","title":"5.3 Required Inputs","text":"<p>The following inputs are required for RIS-compliant measurement:</p> <ul> <li>repeated inference samples from identical prompts</li> <li>controlled perturbation samples (minimal input variations)</li> <li>environmental invariants (temperature, sampling parameters)</li> <li>contextual invariants (system instructions, agent state)</li> <li>tool or retrieval call responses (if applicable)</li> <li>logs or ledger entries representing reasoning behavior</li> </ul> <p>Implementations SHOULD collect evaluation data under isolated and consistent conditions.</p>"},{"location":"sections/05-measurement-framework/#54-sample-size-requirements","title":"5.4 Sample Size Requirements","text":"<p>A compliant RIS evaluation MUST include:</p> <ul> <li>a minimum of 25 repeated inference samples per test prompt</li> <li>a minimum of 10 controlled perturbation samples per prompt</li> <li>at least 50 total prompts across diverse categories for generalized models</li> <li>at least 20 prompts for domain-specific or task-specific systems</li> </ul> <p>Larger sample sizes MAY be used but SHALL be documented.</p>"},{"location":"sections/05-measurement-framework/#55-chain-stability-metric","title":"5.5 Chain Stability Metric","text":""},{"location":"sections/05-measurement-framework/#551-definition","title":"5.5.1 Definition","text":"<p>Chain stability measures the consistency of a system\u2019s reasoning structure across repeated inference cycles.</p>"},{"location":"sections/05-measurement-framework/#552-measurement-method","title":"5.5.2 Measurement Method","text":"<p>Stability is measured using:</p> <ul> <li>structural similarity metrics</li> <li>step-to-step coherence comparisons</li> <li>reasoning-length variance</li> <li>transition-alignment scoring</li> </ul> <p>A system MUST maintain chain similarity within its defined variance envelope.</p>"},{"location":"sections/05-measurement-framework/#553-scoring","title":"5.5.3 Scoring","text":"<p>Chain stability is scored from 0.00 to 1.00.</p> <p>Example thresholds:</p> <ul> <li>0.85\u20131.00: stable</li> <li>0.65\u20130.84: semi-stable</li> <li>below 0.65: unstable</li> </ul> <p>RIS-4 systems SHALL achieve stability above 0.90 for all core prompts.</p>"},{"location":"sections/05-measurement-framework/#56-semantic-coherence-metric","title":"5.6 Semantic Coherence Metric","text":""},{"location":"sections/05-measurement-framework/#561-definition","title":"5.6.1 Definition","text":"<p>Semantic coherence measures the consistency of meaning and conceptual alignment across repeated inferences.</p>"},{"location":"sections/05-measurement-framework/#562-measurement-method","title":"5.6.2 Measurement Method","text":"<p>Coherence is evaluated based on:</p> <ul> <li>semantic similarity scoring</li> <li>topic retention</li> <li>conceptual transition consistency</li> <li>negation or contradiction detection</li> </ul>"},{"location":"sections/05-measurement-framework/#563-scoring","title":"5.6.3 Scoring","text":"<p>Scoring follows the same 0.00\u20131.00 scale.</p> <p>RIS-4 systems SHALL demonstrate coherence above 0.90.</p>"},{"location":"sections/05-measurement-framework/#57-drift-sensitivity-metric","title":"5.7 Drift Sensitivity Metric","text":""},{"location":"sections/05-measurement-framework/#571-definition","title":"5.7.1 Definition","text":"<p>Drift sensitivity measures the degree to which a system deviates from baseline reasoning behavior under repeated evaluations or minimal perturbations.</p>"},{"location":"sections/05-measurement-framework/#572-measurement-method","title":"5.7.2 Measurement Method","text":"<p>Drift is assessed by:</p> <ul> <li>analyzing variance outside baseline patterns</li> <li>measuring semantic or structural deviation trends</li> <li>quantifying divergence under perturbation conditions</li> </ul>"},{"location":"sections/05-measurement-framework/#573-scoring","title":"5.7.3 Scoring","text":"<p>Drift sensitivity is scored inversely (lower is better):</p> <ul> <li>0.00\u20130.10: low drift</li> <li>0.11\u20130.25: moderate drift</li> <li>above 0.25: high drift</li> </ul> <p>RIS-4 systems SHALL maintain drift sensitivity below 0.10.</p>"},{"location":"sections/05-measurement-framework/#58-variance-envelope-compliance","title":"5.8 Variance Envelope Compliance","text":""},{"location":"sections/05-measurement-framework/#581-definition","title":"5.8.1 Definition","text":"<p>The variance envelope represents the acceptable range of variation in reasoning behavior.</p>"},{"location":"sections/05-measurement-framework/#582-measurement-method","title":"5.8.2 Measurement Method","text":"<p>Variance envelope compliance is determined by:</p> <ul> <li>repeated inference testing</li> <li>perturbation evaluations</li> <li>longitudinal measurement</li> <li>stability under load or repeated requests</li> </ul>"},{"location":"sections/05-measurement-framework/#583-scoring","title":"5.8.3 Scoring","text":"<p>A system is considered compliant if:</p> <ul> <li>at least 95 percent of evaluations fall within its defined envelope</li> </ul> <p>RIS-4 systems SHALL achieve at least 98 percent compliance.</p>"},{"location":"sections/05-measurement-framework/#59-governance-boundary-adherence","title":"5.9 Governance Boundary Adherence","text":""},{"location":"sections/05-measurement-framework/#591-definition","title":"5.9.1 Definition","text":"<p>Governance boundaries constrain the context, semantic scope, and operational state the model may use for reasoning.</p>"},{"location":"sections/05-measurement-framework/#592-measurement-method","title":"5.9.2 Measurement Method","text":"<p>Boundary adherence is measured by observing:</p> <ul> <li>context window expansion attempts</li> <li>unauthorized semantic domain expansion</li> <li>tool or agent influence patterns</li> <li>cross-task contamination or memory leakage</li> </ul>"},{"location":"sections/05-measurement-framework/#593-scoring","title":"5.9.3 Scoring","text":"<p>Boundary violations are scored as:</p> <ul> <li>0: no violations</li> <li>1: one minor violation</li> <li>2: repeated or significant violations</li> </ul> <p>RIS-4 systems SHALL record zero violations under all evaluation conditions.</p>"},{"location":"sections/05-measurement-framework/#510-composite-reasoning-integrity-score","title":"5.10 Composite Reasoning Integrity Score","text":"<p>RIS defines an aggregate score calculated using weighted metrics:</p> <ul> <li>Chain Stability: 30 percent</li> <li>Semantic Coherence: 25 percent</li> <li>Drift Sensitivity: 20 percent</li> <li>Variance Envelope Compliance: 15 percent</li> <li>Governance Boundary Adherence: 10 percent</li> </ul> <p>Composite scores determine candidate RIS levels but do not replace conformance requirements.</p>"},{"location":"sections/05-measurement-framework/#511-interpretation-of-scores","title":"5.11 Interpretation of Scores","text":"<p>Composite score ranges:</p> <ul> <li>0.00\u20130.40: RIS-0</li> <li>0.41\u20130.60: RIS-1</li> <li>0.61\u20130.75: RIS-2</li> <li>0.76\u20130.89: RIS-3</li> <li>0.90\u20131.00: RIS-4</li> </ul> <p>A system MUST meet both:</p> <ul> <li>composite scoring thresholds</li> <li>all mandatory controls applicable to its level</li> </ul> <p>to be classified at that RIS level.</p>"},{"location":"sections/06-controls/","title":"6. RIS Controls","text":""},{"location":"sections/06-controls/#61-overview","title":"6.1 Overview","text":"<p>RIS controls define the mandatory and recommended requirements that systems MUST meet to achieve compliance with RIS levels RIS-1 through RIS-4. Controls are grouped into control families that govern stability, coherence, drift, variance, boundary adherence, and operational integrity.</p> <p>Each control includes:</p> <ul> <li>Control Objective</li> <li>Control Requirement (MUST / SHALL)</li> <li>Implementation Guidance (informative)</li> <li>Evidence Required (for audits)</li> <li>Conformance Criteria</li> </ul> <p>All systems seeking RIS classification SHALL be evaluated against these controls.</p>"},{"location":"sections/06-controls/#62-control-families","title":"6.2 Control Families","text":"<p>RIS defines six control families:</p> <ol> <li>RS \u2013 Chain Stability Controls  </li> <li>SC \u2013 Semantic Coherence Controls  </li> <li>DR \u2013 Drift Resistance Controls  </li> <li>VE \u2013 Variance Envelope Controls  </li> <li>GB \u2013 Governance Boundary Controls  </li> <li>OP \u2013 Operational Integrity Controls  </li> </ol> <p>Controls within each family are numbered sequentially.</p>"},{"location":"sections/06-controls/#63-chain-stability-controls-rs","title":"6.3 Chain Stability Controls (RS)","text":""},{"location":"sections/06-controls/#rs-1-repetition-consistency","title":"RS-1: Repetition Consistency","text":""},{"location":"sections/06-controls/#objective","title":"Objective","text":"<p>Ensure reasoning chains remain consistent across repeated inference cycles.</p>"},{"location":"sections/06-controls/#requirement","title":"Requirement","text":"<p>The system SHALL produce reasoning outputs whose structural similarity remains within the defined variance envelope across repeated evaluations of identical prompts.</p>"},{"location":"sections/06-controls/#implementation-guidance","title":"Implementation Guidance","text":"<ul> <li>Use repeated inference tests.</li> <li>Monitor reasoning structure length and transitions.</li> <li>Track deviations across a defined sample size.</li> </ul>"},{"location":"sections/06-controls/#evidence-required","title":"Evidence Required","text":"<ul> <li>stability reports  </li> <li>similarity metrics  </li> <li>repeated inference samples</li> </ul>"},{"location":"sections/06-controls/#conformance","title":"Conformance","text":"<p>Required for RIS-2, RIS-3, and RIS-4.</p>"},{"location":"sections/06-controls/#rs-2-perturbation-stability","title":"RS-2: Perturbation Stability","text":""},{"location":"sections/06-controls/#objective_1","title":"Objective","text":"<p>Ensure stability under minor, controlled input variations.</p>"},{"location":"sections/06-controls/#requirement_1","title":"Requirement","text":"<p>The system SHALL maintain reasoning structure stability when presented with minimal, semantically equivalent perturbations of the same prompt.</p>"},{"location":"sections/06-controls/#implementation-guidance_1","title":"Implementation Guidance","text":"<ul> <li>Use controlled perturbation prompts (synonym replacement, minor rephrasing, neutral variations).</li> <li>Measure structural drift across perturbation samples.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_1","title":"Evidence Required","text":"<ul> <li>perturbation test logs  </li> <li>drift measurements  </li> <li>variance envelope comparison</li> </ul>"},{"location":"sections/06-controls/#conformance_1","title":"Conformance","text":"<p>Required for RIS-3 and RIS-4.</p>"},{"location":"sections/06-controls/#rs-3-transition-alignment","title":"RS-3: Transition Alignment","text":""},{"location":"sections/06-controls/#objective_2","title":"Objective","text":"<p>Ensure internal reasoning transitions follow consistent logical patterns.</p>"},{"location":"sections/06-controls/#requirement_2","title":"Requirement","text":"<p>The system SHALL maintain alignment of step-to-step transitions across repeated inference cycles.</p>"},{"location":"sections/06-controls/#implementation-guidance_2","title":"Implementation Guidance","text":"<ul> <li>Compare reasoning step ordering or inferred conceptual transitions.</li> <li>Identify anomalous reordering indicative of instability.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_2","title":"Evidence Required","text":"<ul> <li>transition-level similarity reports</li> </ul>"},{"location":"sections/06-controls/#conformance_2","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"sections/06-controls/#64-semantic-coherence-controls-sc","title":"6.4 Semantic Coherence Controls (SC)","text":""},{"location":"sections/06-controls/#sc-1-semantic-alignment","title":"SC-1: Semantic Alignment","text":""},{"location":"sections/06-controls/#objective_3","title":"Objective","text":"<p>Ensure that reasoning outputs remain semantically consistent across repeated evaluations.</p>"},{"location":"sections/06-controls/#requirement_3","title":"Requirement","text":"<p>The system SHALL maintain semantic similarity across repeated inferences meeting or exceeding the thresholds defined in the measurement framework.</p>"},{"location":"sections/06-controls/#implementation-guidance_3","title":"Implementation Guidance","text":"<ul> <li>Use embedding-based similarity scoring.</li> <li>Detect contradictory or negated reasoning shifts.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_3","title":"Evidence Required","text":"<ul> <li>semantic similarity analysis</li> <li>contradiction or negation detection logs</li> </ul>"},{"location":"sections/06-controls/#conformance_3","title":"Conformance","text":"<p>Required for RIS-2, RIS-3, and RIS-4.</p>"},{"location":"sections/06-controls/#sc-2-concept-retention","title":"SC-2: Concept Retention","text":""},{"location":"sections/06-controls/#objective_4","title":"Objective","text":"<p>Ensure conceptual themes persist across reasoning processes.</p>"},{"location":"sections/06-controls/#requirement_4","title":"Requirement","text":"<p>The system SHALL retain core conceptual structures across evaluations unless explicitly instructed to change.</p>"},{"location":"sections/06-controls/#implementation-guidance_4","title":"Implementation Guidance","text":"<ul> <li>Analyze topic retention through semantic clustering.</li> <li>Evaluate for unexpected topic divergence.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_4","title":"Evidence Required","text":"<ul> <li>topic coherence reports  </li> <li>clustering analysis results</li> </ul>"},{"location":"sections/06-controls/#conformance_4","title":"Conformance","text":"<p>Required for RIS-3 and RIS-4.</p>"},{"location":"sections/06-controls/#sc-3-logical-consistency","title":"SC-3: Logical Consistency","text":""},{"location":"sections/06-controls/#objective_5","title":"Objective","text":"<p>Ensure logical validity and structure remain consistent.</p>"},{"location":"sections/06-controls/#requirement_5","title":"Requirement","text":"<p>The system SHALL avoid contradictory intermediate reasoning steps within an evaluation set.</p>"},{"location":"sections/06-controls/#implementation-guidance_5","title":"Implementation Guidance","text":"<ul> <li>Evaluate internal reasoning structure for contradictions.</li> <li>Use logical consistency scoring where available.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_5","title":"Evidence Required","text":"<ul> <li>logical consistency assessments</li> </ul>"},{"location":"sections/06-controls/#conformance_5","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"sections/06-controls/#65-drift-resistance-controls-dr","title":"6.5 Drift Resistance Controls (DR)","text":""},{"location":"sections/06-controls/#dr-1-drift-monitoring","title":"DR-1: Drift Monitoring","text":""},{"location":"sections/06-controls/#objective_6","title":"Objective","text":"<p>Establish continuous monitoring for reasoning drift.</p>"},{"location":"sections/06-controls/#requirement_6","title":"Requirement","text":"<p>The system SHALL implement monitoring to detect drift exceeding defined thresholds over repeated inference cycles.</p>"},{"location":"sections/06-controls/#implementation-guidance_6","title":"Implementation Guidance","text":"<ul> <li>Compare baseline outputs against current samples.</li> <li>Establish early-warning indicators for drift escalation.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_6","title":"Evidence Required","text":"<ul> <li>drift baseline logs  </li> <li>drift trend reports</li> </ul>"},{"location":"sections/06-controls/#conformance_6","title":"Conformance","text":"<p>Required for RIS-2, RIS-3, and RIS-4.</p>"},{"location":"sections/06-controls/#dr-2-drift-envelope-enforcement","title":"DR-2: Drift Envelope Enforcement","text":""},{"location":"sections/06-controls/#objective_7","title":"Objective","text":"<p>Maintain reasoning behavior within allowable drift boundaries.</p>"},{"location":"sections/06-controls/#requirement_7","title":"Requirement","text":"<p>The system SHALL enforce a drift envelope that constrains allowable deviation from baseline behavior.</p>"},{"location":"sections/06-controls/#implementation-guidance_7","title":"Implementation Guidance","text":"<ul> <li>Use variance and drift sensitivity metrics.</li> <li>Define acceptable drift bands based on use case.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_7","title":"Evidence Required","text":"<ul> <li>drift envelope documentation  </li> <li>variance compliance reports</li> </ul>"},{"location":"sections/06-controls/#conformance_7","title":"Conformance","text":"<p>Required for RIS-3 and RIS-4.</p>"},{"location":"sections/06-controls/#dr-3-drift-recovery","title":"DR-3: Drift Recovery","text":""},{"location":"sections/06-controls/#objective_8","title":"Objective","text":"<p>Ensure recovery mechanisms exist for restoring reasoning integrity.</p>"},{"location":"sections/06-controls/#requirement_8","title":"Requirement","text":"<p>The system SHALL provide mechanisms to reinitialize or reset reasoning state when drift exceeds permitted thresholds.</p>"},{"location":"sections/06-controls/#implementation-guidance_8","title":"Implementation Guidance","text":"<ul> <li>Reset context or agent state.</li> <li>Refresh memory or tool caches.</li> <li>Re-evaluate under controlled conditions.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_8","title":"Evidence Required","text":"<ul> <li>drift recovery logs  </li> <li>anomaly detection events</li> </ul>"},{"location":"sections/06-controls/#conformance_8","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"sections/06-controls/#66-variance-envelope-controls-ve","title":"6.6 Variance Envelope Controls (VE)","text":""},{"location":"sections/06-controls/#ve-1-variance-definition","title":"VE-1: Variance Definition","text":""},{"location":"sections/06-controls/#objective_9","title":"Objective","text":"<p>Define acceptable variance for reasoning behavior.</p>"},{"location":"sections/06-controls/#requirement_9","title":"Requirement","text":"<p>The system SHALL define a variance envelope specifying acceptable deviation thresholds across repeated evaluations.</p>"},{"location":"sections/06-controls/#implementation-guidance_9","title":"Implementation Guidance","text":"<ul> <li>Document variance thresholds.</li> <li>Derive thresholds from benchmarking or domain requirements.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_9","title":"Evidence Required","text":"<ul> <li>variance envelope documentation</li> </ul>"},{"location":"sections/06-controls/#conformance_9","title":"Conformance","text":"<p>Required for RIS-2, RIS-3, and RIS-4.</p>"},{"location":"sections/06-controls/#ve-2-variance-compliance","title":"VE-2: Variance Compliance","text":""},{"location":"sections/06-controls/#objective_10","title":"Objective","text":"<p>Ensure reasoning outputs remain within defined variance bounds.</p>"},{"location":"sections/06-controls/#requirement_10","title":"Requirement","text":"<p>The system SHALL maintain variance compliance for at least 95 percent of repeated evaluations.</p>"},{"location":"sections/06-controls/#implementation-guidance_10","title":"Implementation Guidance","text":"<ul> <li>Use repeated inference tests.</li> <li>Monitor patterns of deviation.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_10","title":"Evidence Required","text":"<ul> <li>compliance reports  </li> <li>exception logs</li> </ul>"},{"location":"sections/06-controls/#conformance_10","title":"Conformance","text":"<p>Required for RIS-3 and RIS-4.</p>"},{"location":"sections/06-controls/#ve-3-variance-tightening","title":"VE-3: Variance Tightening","text":""},{"location":"sections/06-controls/#objective_11","title":"Objective","text":"<p>Tighten variance thresholds for high-integrity reasoning.</p>"},{"location":"sections/06-controls/#requirement_11","title":"Requirement","text":"<p>The system SHALL maintain variance compliance at rates exceeding 98 percent for RIS-4.</p>"},{"location":"sections/06-controls/#implementation-guidance_11","title":"Implementation Guidance","text":"<ul> <li>Use stricter similarity or coherence scoring.</li> <li>Implement adaptive variance tightening.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_11","title":"Evidence Required","text":"<ul> <li>high-precision variance reports</li> </ul>"},{"location":"sections/06-controls/#conformance_11","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"sections/06-controls/#67-governance-boundary-controls-gb","title":"6.7 Governance Boundary Controls (GB)","text":""},{"location":"sections/06-controls/#gb-1-boundary-definition","title":"GB-1: Boundary Definition","text":""},{"location":"sections/06-controls/#objective_12","title":"Objective","text":"<p>Define explicit reasoning boundaries to prevent uncontrolled contextual expansion.</p>"},{"location":"sections/06-controls/#requirement_12","title":"Requirement","text":"<p>The system SHALL define governance boundaries that constrain contextual, semantic, or operational state during reasoning.</p>"},{"location":"sections/06-controls/#implementation-guidance_12","title":"Implementation Guidance","text":"<ul> <li>Define allowable context limits.</li> <li>Identify prohibited expansions or semantic domains.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_12","title":"Evidence Required","text":"<ul> <li>governance boundary documentation</li> </ul>"},{"location":"sections/06-controls/#conformance_12","title":"Conformance","text":"<p>Required for RIS-2, RIS-3, and RIS-4.</p>"},{"location":"sections/06-controls/#gb-2-boundary-enforcement","title":"GB-2: Boundary Enforcement","text":""},{"location":"sections/06-controls/#objective_13","title":"Objective","text":"<p>Ensure reasoning remains within defined boundaries.</p>"},{"location":"sections/06-controls/#requirement_13","title":"Requirement","text":"<p>The system SHALL enforce boundaries during inference and detect violations.</p>"},{"location":"sections/06-controls/#implementation-guidance_13","title":"Implementation Guidance","text":"<ul> <li>Use contextual or semantic boundary checks.</li> <li>Restrict access to tools or memory beyond defined limits.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_13","title":"Evidence Required","text":"<ul> <li>boundary enforcement logs  </li> <li>violation reports</li> </ul>"},{"location":"sections/06-controls/#conformance_13","title":"Conformance","text":"<p>Required for RIS-3 and RIS-4.</p>"},{"location":"sections/06-controls/#gb-3-interference-protection","title":"GB-3: Interference Protection","text":""},{"location":"sections/06-controls/#objective_14","title":"Objective","text":"<p>Prevent uncontrolled influence from tools, agents, or retrieval components.</p>"},{"location":"sections/06-controls/#requirement_14","title":"Requirement","text":"<p>The system SHALL detect and mitigate reasoning interference from tool responses or agent interactions.</p>"},{"location":"sections/06-controls/#implementation-guidance_14","title":"Implementation Guidance","text":"<ul> <li>Validate tool outputs before integration.</li> <li>Isolate reasoning states across agents.</li> </ul>"},{"location":"sections/06-controls/#evidence-required_14","title":"Evidence Required","text":"<ul> <li>interference detection logs  </li> <li>impact assessments</li> </ul>"},{"location":"sections/06-controls/#conformance_14","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"sections/06-controls/#68-operational-integrity-controls-op","title":"6.8 Operational Integrity Controls (OP)","text":""},{"location":"sections/06-controls/#op-1-evaluation-environment-consistency","title":"OP-1: Evaluation Environment Consistency","text":""},{"location":"sections/06-controls/#objective_15","title":"Objective","text":"<p>Ensure evaluation environments remain stable and controlled.</p>"},{"location":"sections/06-controls/#requirement_15","title":"Requirement","text":"<p>The system SHALL maintain consistent parameters (temperature, sampling settings, context) during evaluation.</p>"},{"location":"sections/06-controls/#evidence-required_15","title":"Evidence Required","text":"<ul> <li>environment configuration logs</li> </ul>"},{"location":"sections/06-controls/#conformance_15","title":"Conformance","text":"<p>Required for all RIS levels beyond RIS-0.</p>"},{"location":"sections/06-controls/#op-2-reproducible-testing","title":"OP-2: Reproducible Testing","text":""},{"location":"sections/06-controls/#objective_16","title":"Objective","text":"<p>Ensure evaluation results can be reproduced.</p>"},{"location":"sections/06-controls/#requirement_16","title":"Requirement","text":"<p>The system SHALL support reproducible testing conditions and retain configuration details for audit purposes.</p>"},{"location":"sections/06-controls/#evidence-required_16","title":"Evidence Required","text":"<ul> <li>reproducibility reports  </li> <li>configuration snapshots</li> </ul>"},{"location":"sections/06-controls/#conformance_16","title":"Conformance","text":"<p>Required for RIS-3 and RIS-4.</p>"},{"location":"sections/06-controls/#op-3-audit-logging","title":"OP-3: Audit Logging","text":""},{"location":"sections/06-controls/#objective_17","title":"Objective","text":"<p>Ensure auditable logs for evaluation and monitoring.</p>"},{"location":"sections/06-controls/#requirement_17","title":"Requirement","text":"<p>The system SHALL generate logs documenting reasoning behavior, drift events, variance compliance, and boundary violations.</p>"},{"location":"sections/06-controls/#evidence-required_17","title":"Evidence Required","text":"<ul> <li>audit logs  </li> <li>ledger entries  </li> <li>summary reports</li> </ul>"},{"location":"sections/06-controls/#conformance_17","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"sections/06-controls/#op-4-longitudinal-monitoring","title":"OP-4: Longitudinal Monitoring","text":""},{"location":"sections/06-controls/#objective_18","title":"Objective","text":"<p>Track reasoning behavior over time.</p>"},{"location":"sections/06-controls/#requirement_18","title":"Requirement","text":"<p>The system SHALL support longitudinal monitoring to detect degradation or emerging instability.</p>"},{"location":"sections/06-controls/#evidence-required_18","title":"Evidence Required","text":"<ul> <li>multi-interval reports  </li> <li>trend analyses</li> </ul>"},{"location":"sections/06-controls/#conformance_18","title":"Conformance","text":"<p>Required for RIS-4.</p>"},{"location":"sections/06-controls/#69-summary-of-conformance-requirements-by-ris-level","title":"6.9 Summary of Conformance Requirements by RIS Level","text":"Control Family RIS-1 RIS-2 RIS-3 RIS-4 RS (Stability) Optional Required Required Required (strict) SC (Coherence) Optional Required Required Required (strict) DR (Drift) Optional Required Required Required (strict) VE (Variance) Optional Required Required Required (tight) GB (Boundary) Optional Required Required Required (strict) OP (Operational) Minimal Moderate Strong Full (audit-ready)"},{"location":"sections/06-controls/#610-control-interpretation","title":"6.10 Control Interpretation","text":"<p>Control interpretation SHALL comply with the following:</p> <ul> <li>MUST and SHALL controls are required for conformance.</li> <li>SHOULD controls are recommended but not required unless explicitly tied to a level.</li> <li>MAY controls provide flexibility but do not affect conformance.</li> </ul>"},{"location":"sections/07-scoring-and-conformance/","title":"7. Scoring and Conformance","text":""},{"location":"sections/07-scoring-and-conformance/#71-overview","title":"7.1 Overview","text":"<p>This section defines the scoring methodology and conformance criteria used to determine a system\u2019s RIS level. Scoring integrates quantitative metrics from the measurement framework with mandatory control requirements. A system\u2019s RIS classification is based on both:</p> <ol> <li>composite reasoning integrity score, and  </li> <li>demonstrated compliance with required RIS controls.</li> </ol> <p>A system SHALL NOT be assigned a RIS level solely based on scoring metrics without meeting corresponding control requirements.</p>"},{"location":"sections/07-scoring-and-conformance/#72-composite-reasoning-integrity-score","title":"7.2 Composite Reasoning Integrity Score","text":"<p>The composite score is calculated using weighted metrics defined in Section 5:</p> <ul> <li>Chain Stability: 30 percent  </li> <li>Semantic Coherence: 25 percent  </li> <li>Drift Sensitivity: 20 percent  </li> <li>Variance Envelope Compliance: 15 percent  </li> <li>Governance Boundary Adherence: 10 percent  </li> </ul> <p>Each metric yields a value between 0.00 and 1.00. The weighted sum produces the final composite reasoning integrity score.</p> <p>The composite score is used to determine eligibility for a RIS level, but SHALL NOT be used to determine final classification without control verification.</p>"},{"location":"sections/07-scoring-and-conformance/#73-metric-weighting","title":"7.3 Metric Weighting","text":"<p>The rationale for metric weighting is as follows:</p> <ul> <li>Chain Stability (30 percent): foundational indicator of structural reasoning reliability.  </li> <li>Semantic Coherence (25 percent): ensures reasoning maintains consistent meaning.  </li> <li>Drift Sensitivity (20 percent): predicts long-term stability under repetition.  </li> <li>Variance Envelope Compliance (15 percent): ensures bounded divergence.  </li> <li>Governance Boundary Adherence (10 percent): required for operational trust and safety.</li> </ul> <p>These weights SHALL remain consistent across RIS versions unless updated in a future amendment.</p>"},{"location":"sections/07-scoring-and-conformance/#74-eligibility-thresholds-for-ris-levels","title":"7.4 Eligibility Thresholds for RIS Levels","text":"<p>Eligibility thresholds based on composite score:</p> <ul> <li>0.00\u20130.40: RIS-0  </li> <li>0.41\u20130.60: RIS-1  </li> <li>0.61\u20130.75: RIS-2  </li> <li>0.76\u20130.89: RIS-3  </li> <li>0.90\u20131.00: RIS-4  </li> </ul> <p>Meeting the scoring threshold does not guarantee assignment to that RIS level. The system SHALL also satisfy the control requirements designated for that level.</p>"},{"location":"sections/07-scoring-and-conformance/#75-control-based-conformance-requirements","title":"7.5 Control-Based Conformance Requirements","text":"<p>The following table defines minimum control conformance for each RIS level:</p> RIS Level Required Controls RIS-0 No controls required RIS-1 Optional controls only RIS-2 All RS-1, SC-1, DR-1, VE-1, GB-1 controls RIS-3 All RIS-2 controls plus RS-2, SC-2, DR-2, VE-2, GB-2, OP-2 RIS-4 All RIS-3 controls plus RS-3, SC-3, DR-3, VE-3, GB-3, OP-3, OP-4 <p>A system SHALL NOT be classified above RIS-2 unless all mandatory controls for RIS-3 are satisfied. A system SHALL NOT be classified above RIS-3 unless all mandatory controls for RIS-4 are satisfied.</p>"},{"location":"sections/07-scoring-and-conformance/#76-evidence-requirements","title":"7.6 Evidence Requirements","text":"<p>To support a RIS classification, the following evidence MUST be produced:</p> <ol> <li>Measurement Framework Results  </li> <li>repeated inference sample outputs  </li> <li>controlled perturbation outputs  </li> <li>variance compliance reports  </li> <li> <p>drift sensitivity analyses  </p> </li> <li> <p>Control Implementation Evidence  </p> </li> <li>documentation of stability, coherence, drift, variance, and boundary controls  </li> <li>operational integrity documentation  </li> <li> <p>governance boundary definitions  </p> </li> <li> <p>Audit Logs  </p> </li> <li>reasoning ledger entries or equivalent reports  </li> <li>evaluation logs  </li> <li> <p>anomaly and boundary violation logs  </p> </li> <li> <p>Configuration Snapshots  </p> </li> <li>inference parameters  </li> <li>environmental settings  </li> <li>prompt templates  </li> <li>tool access or agent configuration  </li> </ol> <p>The evaluator SHALL verify all evidence before assigning a RIS level.</p>"},{"location":"sections/07-scoring-and-conformance/#77-conformance-determination-process","title":"7.7 Conformance Determination Process","text":"<p>A system SHALL undergo the following steps to determine conformance:</p> <ol> <li>Preparation  </li> <li>define evaluation conditions  </li> <li>document configuration  </li> <li> <p>establish invariants  </p> </li> <li> <p>Data Collection  </p> </li> <li>perform repeated inference sampling  </li> <li>perform controlled perturbation sampling  </li> <li> <p>log reasoning behavior  </p> </li> <li> <p>Metric Calculation  </p> </li> <li>compute each metric independently  </li> <li> <p>calculate composite reasoning integrity score  </p> </li> <li> <p>Control Verification  </p> </li> <li>confirm implementation and operation of required controls  </li> <li> <p>identify gaps or violations  </p> </li> <li> <p>Level Assignment  </p> </li> <li>determine eligible RIS level based on composite score  </li> <li>determine achievable RIS level based on control compliance  </li> <li> <p>final level SHALL be the lower of the two  </p> </li> <li> <p>Report Generation  </p> </li> <li>produce a formal evaluation report  </li> <li> <p>document findings, score, control status, and assignment  </p> </li> <li> <p>Certification  </p> </li> <li>issue a RIS Conformance Statement  </li> <li>assign classification validity period  </li> </ol>"},{"location":"sections/07-scoring-and-conformance/#78-handling-of-violations-and-exceptions","title":"7.8 Handling of Violations and Exceptions","text":"<p>If a system violates a mandatory control during evaluation:</p> <ul> <li>it SHALL be classified at the highest RIS level for which all mandatory controls are satisfied  </li> <li>violations MUST be documented with supporting evidence  </li> <li>systems MAY undergo remediation and reassessment  </li> </ul> <p>Exceptions for SHOULD and MAY controls SHALL NOT affect level assignment if mandatory controls are met.</p>"},{"location":"sections/07-scoring-and-conformance/#79-reassessment-requirements","title":"7.9 Reassessment Requirements","text":"<p>RIS conformance SHALL be reassessed when:</p> <ul> <li>the underlying model is updated  </li> <li>training data or fine-tuning changes  </li> <li>new tools or agents are added  </li> <li>governance boundaries change  </li> <li>drift or instability trends are detected in production  </li> <li>a significant degradation in metrics occurs  </li> <li>system behavior changes under load  </li> </ul> <p>Reassessment MAY be performed more frequently based on organizational or regulatory requirements.</p>"},{"location":"sections/07-scoring-and-conformance/#710-validity-of-ris-classification","title":"7.10 Validity of RIS Classification","text":"<p>RIS classification is valid for a period of:</p> <ul> <li>12 months for general LLM systems  </li> <li>6 months for agentic or tool-integrated systems  </li> <li>3 months for safety-critical systems  </li> </ul> <p>After expiration, the system SHALL be reevaluated to maintain a RIS classification.</p>"},{"location":"sections/07-scoring-and-conformance/#711-publication-and-reporting","title":"7.11 Publication and Reporting","text":"<p>Organizations MAY publish:</p> <ul> <li>their RIS level  </li> <li>their composite score  </li> <li>high-level findings  </li> </ul> <p>However, detailed logs, inference samples, and proprietary evaluation data SHOULD only be disclosed under NDA or regulatory requirement.</p> <p>RIS conformance SHALL NOT be claimed without formal assessment.</p>"},{"location":"sections/08-risk-models/","title":"8. Risk Models","text":""},{"location":"sections/08-risk-models/#81-overview","title":"8.1 Overview","text":"<p>This section defines the risk models used to evaluate, categorize, and mitigate threats to reasoning integrity in LLMs and agentic systems. RIS identifies reasoning risks that impact stability, coherence, predictability, and boundary adherence. Risk models are used to:</p> <ul> <li>guide evaluation methodology</li> <li>inform control requirements</li> <li>support audit and governance decisions</li> <li>predict failure conditions</li> <li>align reasoning-system behavior with organizational risk tolerance</li> </ul> <p>RIS risk models are independent of data privacy, security, ethical, or regulatory risk. They focus exclusively on reasoning integrity.</p>"},{"location":"sections/08-risk-models/#82-categories-of-reasoning-risk","title":"8.2 Categories of Reasoning Risk","text":"<p>RIS defines six primary categories of reasoning risk:</p> <ol> <li>Structural Drift Risk  </li> <li>Semantic Drift Risk  </li> <li>Temporal Stability Risk  </li> <li>Interference Risk  </li> <li>Boundary Violation Risk  </li> <li>Degradation Risk  </li> </ol> <p>Each risk category is described in the following sections.</p>"},{"location":"sections/08-risk-models/#83-structural-drift-risk","title":"8.3 Structural Drift Risk","text":""},{"location":"sections/08-risk-models/#831-description","title":"8.3.1 Description","text":"<p>Structural drift occurs when the underlying structure, sequence, or transitions of reasoning chains changes beyond acceptable variance thresholds.</p>"},{"location":"sections/08-risk-models/#832-causes","title":"8.3.2 Causes","text":"<ul> <li>sampling randomness beyond expected levels  </li> <li>sensitivity to minor prompt variations  </li> <li>instability in internal representations  </li> <li>tool or agent load effects  </li> <li>model configuration fluctuations  </li> </ul>"},{"location":"sections/08-risk-models/#833-indicators","title":"8.3.3 Indicators","text":"<ul> <li>inconsistent step ordering  </li> <li>fluctuating reasoning length  </li> <li>divergent intermediate transitions  </li> <li>unexplained reorganization of reasoning steps  </li> </ul>"},{"location":"sections/08-risk-models/#834-impact","title":"8.3.4 Impact","text":"<p>Structural drift undermines reproducibility and increases the likelihood of unpredictable system behavior.</p>"},{"location":"sections/08-risk-models/#835-mitigation","title":"8.3.5 Mitigation","text":"<ul> <li>enforce variance envelope  </li> <li>enforce chain stability controls  </li> <li>reset or reinitialize agent state when drift is detected  </li> <li>maintain consistent inference parameters  </li> </ul>"},{"location":"sections/08-risk-models/#84-semantic-drift-risk","title":"8.4 Semantic Drift Risk","text":""},{"location":"sections/08-risk-models/#841-description","title":"8.4.1 Description","text":"<p>Semantic drift arises when the meaning, conceptual alignment, or topic structure shifts unpredictably during or across reasoning cycles.</p>"},{"location":"sections/08-risk-models/#842-causes","title":"8.4.2 Causes","text":"<ul> <li>conceptual instability under repetition  </li> <li>semantic domain expansion  </li> <li>interference from retrieval or tools  </li> <li>failure to retain core conceptual anchors  </li> </ul>"},{"location":"sections/08-risk-models/#843-indicators","title":"8.4.3 Indicators","text":"<ul> <li>loss of topic relevance  </li> <li>contradictory intermediate steps  </li> <li>unintended meaning shifts  </li> <li>negation or inversion anomalies  </li> </ul>"},{"location":"sections/08-risk-models/#844-impact","title":"8.4.4 Impact","text":"<p>Semantic drift reduces coherence, predictability, and trustworthiness of reasoning outputs.</p>"},{"location":"sections/08-risk-models/#845-mitigation","title":"8.4.5 Mitigation","text":"<ul> <li>semantic coherence controls  </li> <li>conceptual retention verification  </li> <li>governance boundary enforcement  </li> </ul>"},{"location":"sections/08-risk-models/#85-temporal-stability-risk","title":"8.5 Temporal Stability Risk","text":""},{"location":"sections/08-risk-models/#851-description","title":"8.5.1 Description","text":"<p>Temporal stability risk refers to degradation in reasoning quality over time or across extended inference sequences.</p>"},{"location":"sections/08-risk-models/#852-causes","title":"8.5.2 Causes","text":"<ul> <li>accumulated internal noise  </li> <li>memory contamination  </li> <li>incremental divergence across long chains  </li> <li>resource fluctuations  </li> </ul>"},{"location":"sections/08-risk-models/#853-indicators","title":"8.5.3 Indicators","text":"<ul> <li>gradual decline in coherence  </li> <li>increasing drift across intervals  </li> <li>failure to maintain reasoning state consistency  </li> </ul>"},{"location":"sections/08-risk-models/#854-impact","title":"8.5.4 Impact","text":"<p>Long-duration applications may become unreliable, particularly in agentic or multi-step workflows.</p>"},{"location":"sections/08-risk-models/#855-mitigation","title":"8.5.5 Mitigation","text":"<ul> <li>longitudinal monitoring  </li> <li>periodic state resets  </li> <li>consistency checkpoints  </li> </ul>"},{"location":"sections/08-risk-models/#86-interference-risk","title":"8.6 Interference Risk","text":""},{"location":"sections/08-risk-models/#861-description","title":"8.6.1 Description","text":"<p>Interference risk arises when external components affect reasoning integrity.</p> <p>Sources of interference include:</p> <ul> <li>tools  </li> <li>retrieval components  </li> <li>other agents  </li> <li>environment APIs  </li> <li>context or memory systems  </li> </ul>"},{"location":"sections/08-risk-models/#862-causes","title":"8.6.2 Causes","text":"<ul> <li>misaligned tool responses  </li> <li>unbounded agent interactions  </li> <li>unpredictable RAG data injection  </li> <li>multi-agent reasoning collisions  </li> </ul>"},{"location":"sections/08-risk-models/#863-indicators","title":"8.6.3 Indicators","text":"<ul> <li>sudden reasoning divergence  </li> <li>tool-dependent instability  </li> <li>inconsistent decisions after tool use  </li> <li>agent cross-contamination  </li> </ul>"},{"location":"sections/08-risk-models/#864-impact","title":"8.6.4 Impact","text":"<p>Interference may cause unpredictable reasoning patterns, loss of boundary control, and uncontrolled drift.</p>"},{"location":"sections/08-risk-models/#865-mitigation","title":"8.6.5 Mitigation","text":"<ul> <li>interference detection  </li> <li>boundary enforcement controls  </li> <li>isolation of reasoning state  </li> <li>validation of tool outputs  </li> </ul>"},{"location":"sections/08-risk-models/#87-boundary-violation-risk","title":"8.7 Boundary Violation Risk","text":""},{"location":"sections/08-risk-models/#871-description","title":"8.7.1 Description","text":"<p>Boundary violation risk occurs when the system exceeds allowed contextual, semantic, or operational boundaries.</p>"},{"location":"sections/08-risk-models/#872-causes","title":"8.7.2 Causes","text":"<ul> <li>context window expansion  </li> <li>unauthorized semantic domain exploration  </li> <li>cross-task memory leakage  </li> <li>overreliance on external tools  </li> <li>multi-agent interference  </li> </ul>"},{"location":"sections/08-risk-models/#873-indicators","title":"8.7.3 Indicators","text":"<ul> <li>references to disallowed information  </li> <li>use of context not present in the input  </li> <li>reasoning dependent on external unintended state  </li> <li>creeping expansion of reasoning scope  </li> </ul>"},{"location":"sections/08-risk-models/#874-impact","title":"8.7.4 Impact","text":"<p>Boundary violations compromise predictability, auditability, and operational safety.</p>"},{"location":"sections/08-risk-models/#875-mitigation","title":"8.7.5 Mitigation","text":"<ul> <li>boundary definition and enforcement controls  </li> <li>monitoring for out-of-bound semantic transitions  </li> <li>strict tool and agent constraints  </li> </ul>"},{"location":"sections/08-risk-models/#88-degradation-risk","title":"8.8 Degradation Risk","text":""},{"location":"sections/08-risk-models/#881-description","title":"8.8.1 Description","text":"<p>Degradation risk refers to progressive reasoning quality decline over time due to repeated use, extended agent loops, environmental factors, or model instability.</p>"},{"location":"sections/08-risk-models/#882-causes","title":"8.8.2 Causes","text":"<ul> <li>prolonged agentic sessions  </li> <li>heavy tool usage  </li> <li>accumulated contextual noise  </li> <li>fluctuating infrastructure conditions  </li> </ul>"},{"location":"sections/08-risk-models/#883-indicators","title":"8.8.3 Indicators","text":"<ul> <li>increasing drift trends  </li> <li>decreasing coherence metrics  </li> <li>longer or shorter reasoning chains without cause  </li> <li>gradual divergence across evaluation windows  </li> </ul>"},{"location":"sections/08-risk-models/#884-impact","title":"8.8.4 Impact","text":"<p>Degradation affects reliability in long-running systems, especially in automation and orchestration applications.</p>"},{"location":"sections/08-risk-models/#885-mitigation","title":"8.8.5 Mitigation","text":"<ul> <li>longitudinal stability tracking  </li> <li>state resets  </li> <li>performance baselining  </li> <li>proactive drift sensitivity checks  </li> </ul>"},{"location":"sections/08-risk-models/#89-risk-levels","title":"8.9 Risk Levels","text":"<p>RIS classifies reasoning risks into three levels:</p> <ul> <li>Low: Within established variance and stability thresholds  </li> <li>Moderate: Deviations observed but within safe operational limits  </li> <li>High: Significant deviations requiring remediation  </li> </ul> <p>Risk levels SHALL be documented in the conformance report.</p>"},{"location":"sections/08-risk-models/#810-relationship-between-risks-and-ris-levels","title":"8.10 Relationship Between Risks and RIS Levels","text":"<p>Each RIS level carries a corresponding risk tolerance:</p> <ul> <li>RIS-0: All risks high  </li> <li>RIS-1: Most risks moderate to high  </li> <li>RIS-2: Risks moderate with limited high-risk events  </li> <li>RIS-3: All risks low to moderate; no high-risk events allowed  </li> <li>RIS-4: All risks low; strict controls required  </li> </ul> <p>A system SHALL NOT qualify for RIS-3 or RIS-4 if any high-risk category is identified.</p>"},{"location":"sections/08-risk-models/#811-use-of-risk-models-in-evaluation","title":"8.11 Use of Risk Models in Evaluation","text":"<p>During evaluation, assessors SHALL:</p> <ul> <li>identify risk categories triggered  </li> <li>document causes and indicators  </li> <li>evaluate drift and stability trends  </li> <li>map observed behavior to risk severity  </li> <li>determine required mitigation for higher RIS levels  </li> </ul> <p>Risk models inform conformance decisions but do not replace mandatory controls or scoring.</p>"},{"location":"sections/08-risk-models/#812-use-of-risk-models-in-production-monitoring","title":"8.12 Use of Risk Models in Production Monitoring","text":"<p>Organizations SHOULD use risk models to:</p> <ul> <li>detect early warning signals  </li> <li>anticipate degradation trends  </li> <li>identify tool-induced instability  </li> <li>evaluate multi-agent interference  </li> <li>maintain operational safety  </li> </ul> <p>Risk-based monitoring SHALL supplement RIS control compliance.</p>"},{"location":"sections/09-evaluation-methodology/","title":"9. Evaluation Methodology","text":""},{"location":"sections/09-evaluation-methodology/#91-overview","title":"9.1 Overview","text":"<p>The RIS evaluation methodology defines the standardized procedures for performing a reasoning integrity assessment. These procedures ensure that measurements are:</p> <ul> <li>repeatable  </li> <li>objective  </li> <li>statistically meaningful  </li> <li>comparable across systems  </li> <li>suitable for audit and certification  </li> </ul> <p>All evaluations MUST follow the methodology in this section to be considered valid for RIS classification.</p>"},{"location":"sections/09-evaluation-methodology/#92-evaluation-objectives","title":"9.2 Evaluation Objectives","text":"<p>The objectives of RIS evaluation are to:</p> <ol> <li>quantify reasoning stability and coherence  </li> <li>measure drift and variance across repeated evaluations  </li> <li>detect boundary violations  </li> <li>assess interference sensitivity  </li> <li>validate compliance with required controls  </li> <li>classify the system into an appropriate RIS level  </li> </ol> <p>Evaluation SHALL NOT assess factual correctness or ethical outcomes. Only reasoning integrity is measured.</p>"},{"location":"sections/09-evaluation-methodology/#93-evaluation-conditions","title":"9.3 Evaluation Conditions","text":""},{"location":"sections/09-evaluation-methodology/#931-environmental-consistency","title":"9.3.1 Environmental consistency","text":"<p>Evaluations SHALL be conducted under consistent conditions, including:</p> <ul> <li>model version  </li> <li>inference parameters (temperature, top-p, settings)  </li> <li>context and prompt format  </li> <li>agent or tool configurations  </li> <li>hardware or infrastructure environment  </li> </ul> <p>Changes to any parameter invalidate comparisons.</p>"},{"location":"sections/09-evaluation-methodology/#932-isolation","title":"9.3.2 Isolation","text":"<p>The evaluation environment SHOULD minimize:</p> <ul> <li>interference from concurrent workloads  </li> <li>environmental noise  </li> <li>model adaptation or memory building  </li> <li>caching effects  </li> </ul>"},{"location":"sections/09-evaluation-methodology/#933-documentation","title":"9.3.3 Documentation","text":"<p>The evaluator SHALL document:</p> <ul> <li>all inference parameters  </li> <li>all environment variables  </li> <li>system settings used for assessment  </li> <li>test prompt sets  </li> </ul>"},{"location":"sections/09-evaluation-methodology/#94-prompt-set-requirements","title":"9.4 Prompt Set Requirements","text":"<p>RIS evaluation uses two types of prompts:</p> <ol> <li>Baseline prompts  </li> <li>Perturbation prompts  </li> </ol>"},{"location":"sections/09-evaluation-methodology/#941-baseline-prompts","title":"9.4.1 Baseline prompts","text":"<p>Baseline prompts SHOULD include:</p> <ul> <li>informational queries  </li> <li>reasoning tasks  </li> <li>multi-step tasks  </li> <li>conceptual analysis tasks  </li> <li>domain-neutral questions  </li> </ul> <p>At least 50 prompts SHOULD be used for general-purpose systems. At least 20 prompts SHOULD be used for specialized systems.</p>"},{"location":"sections/09-evaluation-methodology/#942-perturbation-prompts","title":"9.4.2 Perturbation prompts","text":"<p>Perturbation prompts MUST maintain semantic equivalence with baseline prompts while varying:</p> <ul> <li>syntax  </li> <li>structure  </li> <li>ordering  </li> <li>minor phrasing  </li> <li>neutral substitutions  </li> </ul> <p>A minimum of 10 perturbations SHALL be used per baseline prompt selected for perturbation testing.</p>"},{"location":"sections/09-evaluation-methodology/#95-sampling-methodology","title":"9.5 Sampling Methodology","text":""},{"location":"sections/09-evaluation-methodology/#951-repeated-inference-sampling","title":"9.5.1 Repeated inference sampling","text":"<p>For each baseline prompt:</p> <ul> <li>at least 25 repeated inference samples SHALL be collected  </li> <li>sampling parameters SHALL remain identical across all repetitions  </li> </ul> <p>Repeated inference sampling is used to compute:</p> <ul> <li>chain stability  </li> <li>semantic coherence  </li> <li>drift sensitivity  </li> <li>variance envelope compliance  </li> </ul>"},{"location":"sections/09-evaluation-methodology/#952-controlled-perturbation-sampling","title":"9.5.2 Controlled perturbation sampling","text":"<p>For each perturbation prompt:</p> <ul> <li>at least 10 inference samples SHALL be collected  </li> </ul> <p>Perturbation sampling is used to measure:</p> <ul> <li>structural drift  </li> <li>semantic drift  </li> <li>robustness under variation  </li> </ul>"},{"location":"sections/09-evaluation-methodology/#96-baseline-establishment","title":"9.6 Baseline Establishment","text":""},{"location":"sections/09-evaluation-methodology/#961-creating-a-baseline","title":"9.6.1 Creating a baseline","text":"<p>The evaluator SHALL establish a baseline by:</p> <ol> <li>selecting representative baseline prompts  </li> <li>executing repeated inference sampling  </li> <li>computing structural and semantic similarity metrics  </li> <li>determining drift sensitivity  </li> <li>defining acceptable variance envelopes  </li> </ol>"},{"location":"sections/09-evaluation-methodology/#962-baseline-validity","title":"9.6.2 Baseline validity","text":"<p>A baseline is valid only if:</p> <ul> <li>all evaluation conditions are documented  </li> <li>system configuration remains unchanged  </li> <li>variance envelope is statistically meaningful  </li> </ul> <p>Baseline MUST be refreshed when model parameters change or risk models trigger reassessment.</p>"},{"location":"sections/09-evaluation-methodology/#97-drift-evaluation","title":"9.7 Drift Evaluation","text":""},{"location":"sections/09-evaluation-methodology/#971-drift-detection","title":"9.7.1 Drift detection","text":"<p>Drift is detected by:</p> <ul> <li>comparing new samples to baseline  </li> <li>analyzing deviation trends  </li> <li>identifying abrupt or progressive divergence  </li> <li>checking for consistent out-of-envelope behavior  </li> </ul>"},{"location":"sections/09-evaluation-methodology/#972-drift-characterization","title":"9.7.2 Drift characterization","text":"<p>Drift SHALL be categorized as:</p> <ul> <li>structural  </li> <li>semantic  </li> <li>temporal  </li> <li>interference-based  </li> </ul>"},{"location":"sections/09-evaluation-methodology/#973-drift-thresholds","title":"9.7.3 Drift thresholds","text":"<p>A drift event occurs when:</p> <ul> <li>deviation exceeds the envelope for more than 5 percent of samples  </li> <li>repeated deviations occur during perturbation sampling  </li> <li>temporal divergence increases across sequential windows  </li> </ul>"},{"location":"sections/09-evaluation-methodology/#974-drift-escalation","title":"9.7.4 Drift escalation","text":"<p>If drift is detected:</p> <ul> <li>the system\u2019s RIS level MAY be lowered  </li> <li>additional testing SHALL be performed  </li> <li>drift controls MUST be verified  </li> </ul>"},{"location":"sections/09-evaluation-methodology/#98-boundary-evaluation","title":"9.8 Boundary Evaluation","text":"<p>Boundary adherence SHALL be tested by:</p> <ul> <li>introducing tasks with clear semantic boundaries  </li> <li>evaluating attempts to use disallowed context  </li> <li>testing reasoning behavior with restricted tool outputs  </li> <li>detecting cross-task or cross-context contamination  </li> </ul> <p>Any violation SHALL be logged and considered in level classification.</p>"},{"location":"sections/09-evaluation-methodology/#99-interference-evaluation","title":"9.9 Interference Evaluation","text":"<p>Interference evaluation includes:</p> <ul> <li>testing reasoning stability before and after tool calls  </li> <li>measuring agent-to-agent interaction effects  </li> <li>injecting controlled retrieval variability  </li> <li>observing structural or semantic disruptions  </li> </ul> <p>Interference MUST not cause uncontrolled reasoning divergence for systems seeking RIS-3 or RIS-4 level classification.</p>"},{"location":"sections/09-evaluation-methodology/#910-variance-envelope-evaluation","title":"9.10 Variance Envelope Evaluation","text":"<p>Variance envelope compliance SHALL be tested through:</p> <ul> <li>repeated inference sampling  </li> <li>perturbation sampling  </li> <li>load or stress conditions if applicable  </li> <li>periodic resampling across intervals  </li> </ul> <p>A system MUST maintain compliance for:</p> <ul> <li>at least 95 percent of samples (RIS-3)  </li> <li>at least 98 percent of samples (RIS-4)  </li> </ul>"},{"location":"sections/09-evaluation-methodology/#911-evaluation-report-requirements","title":"9.11 Evaluation Report Requirements","text":"<p>A complete evaluation report SHALL include:</p> <ol> <li>System description  </li> <li>Evaluation environment details  </li> <li>Prompt sets used  </li> <li>Sampling methodology  </li> <li>Baseline metrics  </li> <li>Drift analysis  </li> <li>Variance envelope results  </li> <li>Boundary adherence results  </li> <li>Interference results  </li> <li>Composite score calculation  </li> <li>Control compliance analysis  </li> <li>RIS level assigned  </li> <li>Validity period  </li> <li>Evidence appendix  </li> </ol> <p>The report SHALL be signed by the evaluator.</p>"},{"location":"sections/09-evaluation-methodology/#912-evaluation-validity","title":"9.12 Evaluation Validity","text":"<p>An evaluation remains valid only if:</p> <ul> <li>the system configuration does not change  </li> <li>no drift events occur in production  </li> <li>no new tools, agents, or retrieval sources are introduced  </li> <li>governance boundaries remain constant  </li> </ul> <p>If any condition changes, reassessment MAY be required immediately.</p>"},{"location":"sections/09-evaluation-methodology/#913-re-evaluation-requirements","title":"9.13 Re-Evaluation Requirements","text":"<p>Re-evaluation SHALL occur:</p> <ul> <li>at the end of the classification validity period  </li> <li>when major model updates occur  </li> <li>when drift or instability is observed  </li> <li>when new risk categories are triggered  </li> <li>when evidence suggests deviation from baseline behavior  </li> </ul> <p>Systems in regulated or safety-critical environments MAY require more frequent reassessments.</p>"},{"location":"sections/10-audit-guidelines/","title":"10. Audit Guidelines","text":""},{"location":"sections/10-audit-guidelines/#101-overview","title":"10.1 Overview","text":"<p>This section defines the audit requirements, evidence standards, assessment procedures, and validation criteria necessary to verify compliance with the Reasoning Integrity Standard (RIS). These guidelines ensure that evaluations are reliable, reproducible, and suitable for internal review, third-party certification, or regulatory oversight.</p> <p>An auditor MAY be an internal reviewer, an external assessor, or an automated system. All auditors SHALL adhere to the requirements outlined in this section.</p>"},{"location":"sections/10-audit-guidelines/#102-audit-objectives","title":"10.2 Audit Objectives","text":"<p>The objectives of a RIS audit are to:</p> <ol> <li>verify the correctness and completeness of evaluation procedures  </li> <li>validate metric calculations and composite reasoning integrity scores  </li> <li>confirm implementation of mandatory controls  </li> <li>detect deficiencies, violations, or risks  </li> <li>ensure evidence sufficiency and traceability  </li> <li>determine whether assigned RIS levels are justified  </li> <li>produce a formal audit report documenting findings  </li> </ol> <p>Audits SHALL NOT evaluate ethical, factual, or domain-specific correctness.</p>"},{"location":"sections/10-audit-guidelines/#103-audit-scope","title":"10.3 Audit Scope","text":"<p>The audit SHALL cover:</p> <ul> <li>evaluation methodology adherence  </li> <li>system configuration and invariants  </li> <li>reasoning integrity metrics  </li> <li>drift and variance analysis  </li> <li>control compliance  </li> <li>governance boundary enforcement  </li> <li>operational integrity  </li> <li>logged events and ledger entries  </li> <li>deviation or violation reports  </li> </ul> <p>The audit MAY include additional areas depending on organizational, regulatory, or contractual requirements.</p>"},{"location":"sections/10-audit-guidelines/#104-evidence-requirements","title":"10.4 Evidence Requirements","text":"<p>Auditors SHALL review the following types of evidence:</p>"},{"location":"sections/10-audit-guidelines/#1041-raw-inference-samples","title":"10.4.1 Raw inference samples","text":"<ul> <li>repeated inference outputs  </li> <li>perturbation outputs  </li> <li>temporal sampling outputs  </li> <li>tool-integrated outputs (if applicable)  </li> </ul>"},{"location":"sections/10-audit-guidelines/#1042-metric-calculations","title":"10.4.2 Metric calculations","text":"<ul> <li>chain stability similarity data  </li> <li>semantic coherence scores  </li> <li>drift sensitivity calculations  </li> <li>variance envelope compliance results  </li> <li>boundary adherence results  </li> </ul>"},{"location":"sections/10-audit-guidelines/#1043-configuration-data","title":"10.4.3 Configuration data","text":"<ul> <li>inference parameters (temperature, top-p, etc.)  </li> <li>system settings  </li> <li>agent or tool configurations  </li> <li>environment variables  </li> <li>software versions  </li> </ul>"},{"location":"sections/10-audit-guidelines/#1044-control-implementation-evidence","title":"10.4.4 Control implementation evidence","text":"<ul> <li>governance boundary definitions  </li> <li>drift envelope definitions  </li> <li>stability and coherence controls  </li> <li>interference detection controls  </li> <li>audit logging mechanisms  </li> </ul>"},{"location":"sections/10-audit-guidelines/#1045-logs-and-ledger-entries","title":"10.4.5 Logs and ledger entries","text":"<ul> <li>reasoning ledger (or equivalent)  </li> <li>boundary violation logs  </li> <li>drift events  </li> <li>anomaly detection entries  </li> <li>operational logs related to inference behavior  </li> </ul> <p>Auditors SHALL verify the authenticity and integrity of evidence before drawing conclusions.</p>"},{"location":"sections/10-audit-guidelines/#105-audit-procedures","title":"10.5 Audit Procedures","text":""},{"location":"sections/10-audit-guidelines/#1051-preparation","title":"10.5.1 Preparation","text":"<p>The auditor SHALL:</p> <ul> <li>obtain system documentation  </li> <li>review evaluation methodology used  </li> <li>validate prompt sets and sampling plans  </li> <li>confirm frozen inference parameters  </li> <li>ensure environmental consistency  </li> </ul>"},{"location":"sections/10-audit-guidelines/#1052-evidence-examination","title":"10.5.2 Evidence examination","text":"<p>The auditor SHALL examine:</p> <ul> <li>raw samples for drift or instability  </li> <li>metrics for correctness and reproducibility  </li> <li>compliance with mandatory controls  </li> <li>documentation for completeness  </li> </ul>"},{"location":"sections/10-audit-guidelines/#1053-reproduction-testing","title":"10.5.3 Reproduction testing","text":"<p>Auditors MAY reproduce:</p> <ul> <li>repeated inference sampling  </li> <li>perturbation sampling  </li> <li>drift and variance calculations  </li> </ul> <p>Reproduction SHALL use identical parameters to those documented in the evaluation.</p>"},{"location":"sections/10-audit-guidelines/#1054-violation-detection","title":"10.5.4 Violation detection","text":"<p>The auditor SHALL check for:</p> <ul> <li>drift beyond thresholds  </li> <li>variance envelope breaches  </li> <li>boundary violations  </li> <li>interference anomalies  </li> <li>control misconfigurations  </li> <li>unreported failures  </li> </ul> <p>Any violation SHALL be documented in the final audit report.</p>"},{"location":"sections/10-audit-guidelines/#106-compliance-determination","title":"10.6 Compliance Determination","text":"<p>Audit findings SHALL conclude one of the following:</p> <ul> <li>compliant  </li> <li>compliant with exceptions  </li> <li>non-compliant  </li> <li>inconclusive (insufficient evidence)  </li> </ul> <p>If non-compliant, the auditor SHALL document which controls or evaluation procedures were violated.</p> <p>A RIS level MAY only be confirmed if:</p> <ul> <li>composite score qualifies  </li> <li>mandatory controls are satisfied  </li> <li>evidence is complete  </li> <li>no unmitigated high-risk deviations are observed  </li> </ul>"},{"location":"sections/10-audit-guidelines/#107-audit-reporting-requirements","title":"10.7 Audit Reporting Requirements","text":"<p>The audit report SHALL include:</p> <ol> <li>System overview  </li> <li>Evaluation conditions  </li> <li>Summary of controls reviewed  </li> <li>Evidence examined  </li> <li>Metric verification results  </li> <li>Observed violations or exceptions  </li> <li>Assessment of drift, variance, and boundary adherence  </li> <li>Conformance determination  </li> <li>Confirmed RIS level  </li> <li>Recommendations and corrective actions  </li> <li>Validity period of assessment  </li> <li>Appendices with supporting data  </li> </ol> <p>Reports SHALL be retained for the duration of the classification validity period.</p>"},{"location":"sections/10-audit-guidelines/#108-corrective-action-requirements","title":"10.8 Corrective Action Requirements","text":"<p>If deficiencies or violations are identified:</p> <ul> <li>corrective actions SHALL be documented  </li> <li>the system owner SHALL remediate issues  </li> <li>the system SHALL undergo partial or full reassessment  </li> <li>major issues SHALL reset the RIS classification  </li> </ul> <p>Corrective actions MAY include:</p> <ul> <li>adjusting inference parameters  </li> <li>redefining governance boundaries  </li> <li>improving drift or variance controls  </li> <li>updating tool or agent behavior  </li> </ul>"},{"location":"sections/10-audit-guidelines/#109-auditor-qualifications","title":"10.9 Auditor Qualifications","text":"<p>RIS audits MAY be conducted by:</p> <ul> <li>internal governance teams  </li> <li>independent third-party evaluators  </li> <li>qualified researchers  </li> <li>automated auditing systems  </li> </ul> <p>Auditors SHOULD possess expertise in:</p> <ul> <li>LLM inference behavior  </li> <li>statistical evaluation  </li> <li>reasoning-chain analysis  </li> <li>AI governance and risk frameworks  </li> <li>tool, agent, or RAG system integrations (if applicable)  </li> </ul> <p>Organizations MAY impose additional requirements for auditor certification.</p>"},{"location":"sections/10-audit-guidelines/#1010-continuous-audit-recommendations","title":"10.10 Continuous Audit Recommendations","text":"<p>Organizations SHOULD implement ongoing audit-aligned practices:</p> <ul> <li>periodic drift checks  </li> <li>rolling variance sampling  </li> <li>boundary adherence monitoring  </li> <li>interference detection and logging  </li> <li>longitudinal stability assessments  </li> </ul> <p>Continuous monitoring SHALL supplement, not replace, formal RIS audits.</p>"},{"location":"sections/10-audit-guidelines/#1011-audit-validity","title":"10.11 Audit Validity","text":"<p>An audit is valid for:</p> <ul> <li>12 months for general systems  </li> <li>6 months for agentic or tool-integrated systems  </li> <li>3 months for safety-critical systems  </li> </ul> <p>Audits SHALL be invalidated immediately if:</p> <ul> <li>the model or agent is updated  </li> <li>new tools, retrieval sources, or agents are introduced  </li> <li>significant drift is observed in production  </li> <li>boundary violations occur repeatedly  </li> </ul>"},{"location":"sections/10-audit-guidelines/#1012-audit-transparency","title":"10.12 Audit Transparency","text":"<p>Organizations MAY disclose:</p> <ul> <li>confirmed RIS level  </li> <li>summary of findings  </li> <li>audit date and validity period  </li> </ul> <p>Organizations SHOULD NOT publicly disclose:</p> <ul> <li>raw inference samples  </li> <li>proprietary metrics  </li> <li>internal configurations  </li> <li>confidential logs  </li> </ul> <p>Unless required by contract, regulation, or security review.</p>"},{"location":"sections/11-reference-implementation/","title":"11. Reference Implementation (LCAC)","text":""},{"location":"sections/11-reference-implementation/#111-overview","title":"11.1 Overview","text":"<p>This section describes the Least-Context Access Control (LCAC) framework as a reference implementation of the Reasoning Integrity Standard (RIS). LCAC demonstrates one practical method for enforcing RIS controls, measuring reasoning integrity, and maintaining predictable reasoning behavior in LLM-based systems.</p> <p>LCAC is not required for RIS conformance. It is provided solely as an informative reference to illustrate feasibility, implementation patterns, and system design considerations aligned with RIS requirements.</p>"},{"location":"sections/11-reference-implementation/#112-architectural-overview","title":"11.2 Architectural Overview","text":"<p>LCAC is structured as a modular reasoning-governance framework consisting of:</p> <ul> <li>a centralized governor service  </li> <li>a trust and variance model  </li> <li>an immutable reasoning ledger  </li> <li>governance boundary enforcement components  </li> <li>monitoring and evaluation pipelines  </li> <li>optional persona overlays  </li> </ul> <p>LCAC may operate as an independent service or be integrated directly into an LLM-driven application, multi-agent system, or inference pipeline.</p>"},{"location":"sections/11-reference-implementation/#113-components-and-functions","title":"11.3 Components and Functions","text":""},{"location":"sections/11-reference-implementation/#1131-governor","title":"11.3.1 Governor","text":"<p>The LCAC governor performs the following functions:</p> <ul> <li>receives prompt and output pairs for evaluation  </li> <li>computes trust, variance, and drift metrics  </li> <li>determines reasoning verdicts (stable, watch, unstable)  </li> <li>updates governance states (hold, elevate, lockdown)  </li> <li>writes results to the ledger  </li> </ul> <p>The governor serves as the core enforcement point for RIS-aligned controls.</p>"},{"location":"sections/11-reference-implementation/#1132-trust-model","title":"11.3.2 Trust Model","text":"<p>LCAC\u2019s trust model provides:</p> <ul> <li>a baseline trust score  </li> <li>variance and drift calculations  </li> <li>predictive stability measurements  </li> <li>structural and semantic similarity analysis  </li> </ul> <p>The trust score is mapped to RIS metrics such as:</p> <ul> <li>chain stability  </li> <li>semantic coherence  </li> <li>drift sensitivity  </li> </ul> <p>Trust values range between 0.00 and 1.00, consistent with RIS scoring.</p>"},{"location":"sections/11-reference-implementation/#1133-variance-and-drift-envelope","title":"11.3.3 Variance and Drift Envelope","text":"<p>LCAC defines and enforces a variance envelope based on:</p> <ul> <li>repeated inference samples  </li> <li>controlled perturbation testing  </li> <li>deviation thresholds  </li> <li>longitudinal observations  </li> </ul> <p>Metric outputs are compared against these envelopes to determine:</p> <ul> <li>compliance  </li> <li>drift events  </li> <li>stability degradation  </li> </ul>"},{"location":"sections/11-reference-implementation/#1134-immutable-ledger","title":"11.3.4 Immutable Ledger","text":"<p>LCAC maintains an immutable reasoning ledger using:</p> <ul> <li>hash-linked entries  </li> <li>timestamped records  </li> <li>evaluation metrics  </li> <li>baseline comparisons  </li> <li>anomaly documentation  </li> </ul> <p>Each ledger entry includes:</p> <ul> <li>input prompt  </li> <li>model output  </li> <li>trust score  </li> <li>variance metrics  </li> <li>stability verdict  </li> <li>previous entry hash  </li> <li>current entry hash  </li> </ul> <p>This supports audit, reproducibility, and traceability.</p>"},{"location":"sections/11-reference-implementation/#114-governance-modes","title":"11.4 Governance Modes","text":"<p>LCAC defines three governance modes that influence reasoning behavior:</p>"},{"location":"sections/11-reference-implementation/#1141-hold","title":"11.4.1 Hold","text":"<ul> <li>default system state  </li> <li>stable or near-stable reasoning  </li> <li>no significant anomalies detected  </li> </ul>"},{"location":"sections/11-reference-implementation/#1142-elevate","title":"11.4.2 Elevate","text":"<ul> <li>moderate drift or variance detected  </li> <li>reasoning monitored more aggressively  </li> <li>potential instability anticipated  </li> </ul>"},{"location":"sections/11-reference-implementation/#1143-lockdown","title":"11.4.3 Lockdown","text":"<ul> <li>reasoning integrity breach detected  </li> <li>drift or variance above thresholds  </li> <li>boundary violation or interference event observed  </li> <li>heightened restrictions applied  </li> </ul> <p>Governance modes demonstrate how RIS controls may be operationalized.</p>"},{"location":"sections/11-reference-implementation/#115-boundary-enforcement","title":"11.5 Boundary Enforcement","text":"<p>LCAC establishes governance boundaries using:</p> <ul> <li>contextual limits  </li> <li>semantic domain constraints  </li> <li>tool and agent access restrictions  </li> <li>validation checks for external outputs  </li> </ul> <p>Boundary enforcement aligns with RIS boundary violation controls.</p>"},{"location":"sections/11-reference-implementation/#116-persona-overlay-informative-only","title":"11.6 Persona Overlay (Informative Only)","text":"<p>LCAC includes an optional persona system that applies:</p> <ul> <li>stability bias adjustments  </li> <li>risk posture tuning  </li> <li>domain-specific behavioral constraints  </li> </ul> <p>Personas may influence:</p> <ul> <li>baseline trust  </li> <li>drift sensitivity  </li> <li>variance envelopes  </li> <li>stability thresholds  </li> </ul> <p>Persona systems are informative and not required for RIS compliance.</p>"},{"location":"sections/11-reference-implementation/#117-integration-patterns","title":"11.7 Integration Patterns","text":"<p>LCAC supports the following integration models:</p>"},{"location":"sections/11-reference-implementation/#1171-request-evaluation-pattern","title":"11.7.1 Request-Evaluation Pattern","text":"<p>The system sends:</p> <ul> <li>prompt  </li> <li>model output  </li> </ul> <p>to LCAC for evaluation. LCAC returns:</p> <ul> <li>trust score  </li> <li>variance data  </li> <li>stability verdict  </li> <li>governance mode  </li> </ul> <p>This pattern enables external inference pipelines to remain unchanged.</p>"},{"location":"sections/11-reference-implementation/#1172-inline-agent-or-llm-integration","title":"11.7.2 Inline Agent or LLM Integration","text":"<p>LCAC may be embedded into:</p> <ul> <li>agent toolchains  </li> <li>orchestration frameworks  </li> <li>multi-agent reasoning flows  </li> <li>specialized LLM applications  </li> </ul> <p>Governance checks occur inline between inference steps.</p>"},{"location":"sections/11-reference-implementation/#1173-multi-model-or-router-integration","title":"11.7.3 Multi-Model or Router Integration","text":"<p>LCAC may act as:</p> <ul> <li>a routing constraint  </li> <li>a stability filter  </li> <li>a verification layer  </li> </ul> <p>for systems that dynamically select between models.</p>"},{"location":"sections/11-reference-implementation/#118-operational-monitoring","title":"11.8 Operational Monitoring","text":"<p>LCAC supports RIS-aligned monitoring by:</p> <ul> <li>generating audit logs  </li> <li>tracking drift events  </li> <li>identifying variance breaches  </li> <li>logging boundary violations  </li> <li>maintaining historical baselines  </li> </ul> <p>Monitoring outputs MAY be used to trigger reassessment under RIS Section 9.</p>"},{"location":"sections/11-reference-implementation/#119-example-workflow","title":"11.9 Example Workflow","text":"<p>A RIS-aligned workflow using LCAC includes:</p> <ol> <li>The LLM produces an output for a given prompt.  </li> <li>The output and prompt are sent to LCAC.  </li> <li>LCAC computes trust, variance, and drift metrics.  </li> <li>LCAC assigns a verdict (stable, watch, unstable).  </li> <li>LCAC writes an immutable ledger entry.  </li> <li>Governance mode is updated based on results.  </li> <li>Application logic uses the verdict to:  </li> <li>accept the output  </li> <li>reject the output  </li> <li>request re-evaluation  </li> <li>escalate to a fallback model  </li> <li>enforce stricter controls  </li> </ol> <p>This workflow illustrates one method of operationalizing RIS requirements.</p>"},{"location":"sections/11-reference-implementation/#1110-compliance-relationship","title":"11.10 Compliance Relationship","text":"<p>LCAC demonstrates compliance with the following RIS controls:</p> <ul> <li>RS-1, RS-2, RS-3  </li> <li>SC-1, SC-2, SC-3  </li> <li>DR-1, DR-2, DR-3  </li> <li>VE-1, VE-2, VE-3  </li> <li>GB-1, GB-2, GB-3  </li> <li>OP-1, OP-2, OP-3, OP-4  </li> </ul> <p>LCAC is a reference implementation and SHOULD NOT be considered the only method of achieving RIS compliance.</p>"},{"location":"sections/11-reference-implementation/#1111-implementation-notes","title":"11.11 Implementation Notes","text":"<ul> <li>LCAC uses Redis for metric storage and ledger hashing.  </li> <li>All evaluation computations are deterministic given identical inputs.  </li> <li>LCAC can operate in multi-node architectures with shared storage.  </li> <li>The reference implementation is platform-agnostic.  </li> <li>API endpoints and data schemas are documented in the LCAC repository.  </li> </ul> <p>These details are informative and may vary between deployments.</p>"},{"location":"sections/11-reference-implementation/#1112-limitations","title":"11.12 Limitations","text":"<p>LCAC, as a reference implementation:</p> <ul> <li>does not guarantee factual correctness  </li> <li>does not address privacy or security  </li> <li>does not enforce ethical or domain-specific constraints  </li> <li>does not replace external governance frameworks  </li> </ul> <p>RIS conformance MUST be determined independently of LCAC adoption.</p>"},{"location":"sections/11-reference-implementation/#1113-informative-appendix-linkage","title":"11.13 Informative Appendix Linkage","text":"<p>Sections of LCAC MAY be referenced in Appendix A for:</p> <ul> <li>example metrics  </li> <li>sample ledger entries  </li> <li>example drift baselines  </li> <li>variance envelope templates  </li> <li>integration code snippets  </li> </ul> <p>These references are informative only.</p>"},{"location":"sections/12-standard-mapping/","title":"12. Mapping to Existing Standards","text":""},{"location":"sections/12-standard-mapping/#121-overview","title":"12.1 Overview","text":"<p>This section provides a crosswalk between the Reasoning Integrity Standard (RIS) and widely adopted security, governance, and AI risk management frameworks. The purpose of this mapping is to:</p> <ul> <li>support organizations aligning RIS with existing compliance programs  </li> <li>demonstrate compatibility between RIS and established standards  </li> <li>clarify which risks and controls RIS covers  </li> <li>identify where RIS provides additional, reasoning-specific requirements  </li> </ul> <p>Mappings are informative and do not constitute equivalence or certification under referenced standards.</p>"},{"location":"sections/12-standard-mapping/#122-relationship-to-nist-sp-800-53","title":"12.2 Relationship to NIST SP 800-53","text":"<p>RIS aligns with NIST SP 800-53 by extending its principles into the domain of reasoning integrity for AI systems.</p>"},{"location":"sections/12-standard-mapping/#1221-complementary-areas","title":"12.2.1 Complementary Areas","text":"<ul> <li>NIST RA (Risk Assessment): RIS provides a reasoning-specific risk model.  </li> <li>NIST CA (Assessment, Authorization, Monitoring): RIS defines evaluation and audit procedures.  </li> <li>NIST SI (System Integrity): RIS adds reasoning-chain integrity and drift detection.  </li> <li>NIST PM (Program Management): RIS supports governance for LLM-based workflows.  </li> </ul>"},{"location":"sections/12-standard-mapping/#1222-gaps-filled-by-ris","title":"12.2.2 Gaps Filled by RIS","text":"<p>NIST does not address:</p> <ul> <li>reasoning drift  </li> <li>semantic coherence  </li> <li>chain stability  </li> <li>variance envelopes  </li> <li>multi-agent interference  </li> </ul> <p>RIS provides controls specifically for these areas.</p>"},{"location":"sections/12-standard-mapping/#123-relationship-to-isoiec-27001","title":"12.3 Relationship to ISO/IEC 27001","text":"<p>RIS complements ISO/IEC 27001 by introducing AI reasoning integrity controls that parallel information security requirements.</p>"},{"location":"sections/12-standard-mapping/#1231-complementary-areas","title":"12.3.1 Complementary Areas","text":"<ul> <li>ISO 27001 Annex A: controls for monitoring, logging, and operational integrity  </li> <li>ISO 27001 Annex A: change management and configuration management align with RIS evaluation invariants  </li> <li>ISO 27001 Annex A: audit logging aligns with RIS ledger requirements  </li> </ul>"},{"location":"sections/12-standard-mapping/#1232-gaps-filled-by-ris","title":"12.3.2 Gaps Filled by RIS","text":"<p>ISO 27001 does not include:</p> <ul> <li>reasoning integrity assessment  </li> <li>drift evaluation  </li> <li>semantic stability requirements  </li> <li>boundary adherence controls for LLMs  </li> </ul> <p>RIS extends governance into these areas.</p>"},{"location":"sections/12-standard-mapping/#124-relationship-to-soc-2-trust-services-criteria","title":"12.4 Relationship to SOC 2 Trust Services Criteria","text":"<p>RIS aligns with SOC 2 across the following TSC categories:</p> <ul> <li>Security: LCAC-like governance boundaries enhance system integrity  </li> <li>Availability: drift and variance controls improve predictability of behavior  </li> <li>Processing Integrity: RIS focuses specifically on the integrity of reasoning processes  </li> <li>Confidentiality: RIS does not address confidentiality explicitly but complements it  </li> </ul> <p>SOC 2 does not cover reasoning stability, which RIS supplies.</p>"},{"location":"sections/12-standard-mapping/#125-relationship-to-owasp-application-security-verification-standard-asvs","title":"12.5 Relationship to OWASP Application Security Verification Standard (ASVS)","text":"<p>RIS extends ASVS principles into the domain of AI reasoning by providing:</p> <ul> <li>integrity validation of reasoning workflows  </li> <li>boundary checks analogous to input validation  </li> <li>monitoring comparable to application logic verification  </li> </ul> <p>Areas where RIS aligns with ASVS:</p> <ul> <li>session integrity (analogous to reasoning chain integrity)  </li> <li>validation layers (similar to boundary enforcement)  </li> <li>error and exception handling (mapped to drift detection)  </li> </ul> <p>ASVS does not include concepts such as:</p> <ul> <li>semantic drift  </li> <li>reasoning instability  </li> <li>variance envelopes  </li> </ul> <p>RIS adds these requirements.</p>"},{"location":"sections/12-standard-mapping/#126-relationship-to-nist-ai-rmf","title":"12.6 Relationship to NIST AI RMF","text":"<p>RIS is closely aligned with the NIST AI Risk Management Framework.</p>"},{"location":"sections/12-standard-mapping/#1261-alignment-areas","title":"12.6.1 Alignment Areas","text":"<ul> <li>Govern Function: RIS defines governance boundaries and control families  </li> <li>Map Function: RIS specifies reasoning-specific risks and threat models  </li> <li>Measure Function: RIS provides standardized metrics and scoring  </li> <li>Manage Function: RIS prescribes drift, variance, and stability controls  </li> </ul>"},{"location":"sections/12-standard-mapping/#1262-ris-extensions-beyond-nist-ai-rmf","title":"12.6.2 RIS Extensions Beyond NIST AI RMF","text":"<p>RIS introduces:</p> <ul> <li>explicit reasoning chain assessment  </li> <li>structured variance and drift envelopes  </li> <li>multi-level conformance classifications  </li> <li>detailed evaluation methodology  </li> <li>audit and verification standards  </li> </ul> <p>These elements provide more granular operational guidance.</p>"},{"location":"sections/12-standard-mapping/#127-relationship-to-the-eu-ai-act","title":"12.7 Relationship to the EU AI Act","text":"<p>The EU AI Act categorizes AI systems according to risk. RIS provides a compatibility layer by offering objective measurements relevant to risk classification.</p>"},{"location":"sections/12-standard-mapping/#1271-high-risk-alignment","title":"12.7.1 High-Risk Alignment","text":"<p>High-risk AI systems require:</p> <ul> <li>logging  </li> <li>monitoring  </li> <li>technical documentation  </li> <li>risk mitigation  </li> </ul> <p>RIS supports these by:</p> <ul> <li>providing a reasoning ledger  </li> <li>defining stability metrics  </li> <li>enabling drift monitoring  </li> <li>defining reproducible evaluation processes  </li> </ul>"},{"location":"sections/12-standard-mapping/#1272-ris-4-and-high-risk-use-cases","title":"12.7.2 RIS-4 and High-Risk Use Cases","text":"<p>RIS-4 provides the strictest reasoning integrity requirements and is appropriate for:</p> <ul> <li>safety-critical systems  </li> <li>medical reasoning systems  </li> <li>financial decision systems  </li> <li>legal reasoning systems  </li> <li>infrastructure and security systems  </li> </ul> <p>EU AI Act does not provide technical detail on reasoning integrity. RIS fills that gap.</p>"},{"location":"sections/12-standard-mapping/#128-summary-table-ris-vs-external-standards","title":"12.8 Summary Table (RIS vs. External Standards)","text":"Framework Overlap RIS Contribution NIST SP 800-53 Monitoring, logging, system integrity Adds reasoning-chain integrity and drift controls ISO 27001 Operational controls, audit logging Adds reasoning stability requirements SOC 2 Processing integrity Adds reasoning-specific integrity metrics OWASP ASVS Boundary and validation concepts Adds semantic and structural reasoning requirements NIST AI RMF Risk, governance, measurement Adds detailed evaluation and conformance system EU AI Act Documentation and oversight Adds measurable technical integrity criteria"},{"location":"sections/12-standard-mapping/#129-general-observations","title":"12.9 General Observations","text":"<ul> <li>RIS is compatible with existing frameworks but more granular in evaluating reasoning behavior.  </li> <li>RIS introduces metrics and controls not present in any existing standard.  </li> <li>Organizations adopting RIS can integrate it with existing compliance programs.  </li> <li>RIS MAY serve as a technical layer beneath regulatory or governance frameworks.  </li> </ul>"},{"location":"sections/12-standard-mapping/#1210-use-of-mapping-in-compliance-programs","title":"12.10 Use of Mapping in Compliance Programs","text":"<p>Organizations MAY use this mapping to:</p> <ul> <li>integrate RIS evaluation with ISO or SOC 2 audits  </li> <li>include RIS controls in security and governance documentation  </li> <li>demonstrate operational integrity in high-risk AI deployments  </li> <li>support regulatory submissions requiring technical evidence  </li> <li>develop internal AI governance programs  </li> </ul> <p>Mapping does not imply certification under other standards.</p>"},{"location":"sections/13-appendices/","title":"13. Appendices","text":""},{"location":"sections/13-appendices/#131-overview","title":"13.1 Overview","text":"<p>This section contains informative appendices supporting the implementation, evaluation, and audit of the Reasoning Integrity Standard (RIS). Appendices are not normative unless explicitly referenced by a control or requirement.</p> <p>The materials in this section include:</p> <ul> <li>definitions and terminology</li> <li>mathematical formulations</li> <li>example baseline structures</li> <li>sample drift and variance calculations</li> <li>reference ledger entries</li> <li>prompt set templates</li> <li>evaluation dataset formats</li> <li>implementation considerations</li> </ul>"},{"location":"sections/13-appendices/#132-key-definitions","title":"13.2 Key Definitions","text":""},{"location":"sections/13-appendices/#1321-reasoning-chain","title":"13.2.1 Reasoning chain","text":"<p>A sequence of internal conceptual steps used by an LLM or agent to derive an output from an input.</p>"},{"location":"sections/13-appendices/#1322-structural-drift","title":"13.2.2 Structural drift","text":"<p>A deviation in the ordering, length, or internal transitions of a reasoning chain beyond acceptable variance thresholds.</p>"},{"location":"sections/13-appendices/#1323-semantic-drift","title":"13.2.3 Semantic drift","text":"<p>A deviation in meaning, conceptual structure, or topic alignment across samples.</p>"},{"location":"sections/13-appendices/#1324-variance-envelope","title":"13.2.4 Variance envelope","text":"<p>The statistical boundary within which repeated inference behavior must remain to be considered stable.</p>"},{"location":"sections/13-appendices/#1325-boundary-violation","title":"13.2.5 Boundary violation","text":"<p>A reasoning event in which the model exceeds defined contextual, semantic, or operational constraints.</p>"},{"location":"sections/13-appendices/#1326-interference-event","title":"13.2.6 Interference event","text":"<p>An alteration of reasoning behavior caused by a tool, agent, retrieval source, or external system.</p>"},{"location":"sections/13-appendices/#133-mathematical-formulations","title":"13.3 Mathematical Formulations","text":""},{"location":"sections/13-appendices/#1331-chain-stability-score","title":"13.3.1 Chain stability score","text":"<p>Chain stability S is calculated as:</p> <pre><code>S = mean(similarity(Ri, Rj))\n</code></pre>"},{"location":"sections/13-appendices/#1332-semantic-coherence-score","title":"13.3.2 Semantic coherence score","text":"<p>Semantic coherence C is calculated as:</p> <pre><code>C = mean(cosine(Ei, Ej))\n</code></pre>"},{"location":"sections/13-appendices/#1333-drift-sensitivity","title":"13.3.3 Drift sensitivity","text":"<p>Drift sensitivity D is calculated as:</p> <pre><code>D = variance(S) + variance(C)\n</code></pre>"},{"location":"sections/13-appendices/#1334-variance-envelope-compliance","title":"13.3.4 Variance envelope compliance","text":"<p>Compliance V is:</p> <pre><code>V = (samples within envelope) / (total samples)\n</code></pre>"},{"location":"sections/13-appendices/#134-example-variance-envelopes","title":"13.4 Example Variance Envelopes","text":""},{"location":"sections/13-appendices/#1341-general-purpose-llm-envelope","title":"13.4.1 General-purpose LLM envelope","text":"<ul> <li>chain stability: 0.75 to 1.00</li> <li>semantic coherence: 0.80 to 1.00</li> <li>drift sensitivity: below 0.30</li> </ul>"},{"location":"sections/13-appendices/#1342-high-integrity-envelope","title":"13.4.2 High-integrity envelope","text":"<ul> <li>chain stability: 0.90 to 1.00</li> <li>semantic coherence: 0.90 to 1.00</li> <li>drift sensitivity: below 0.10</li> </ul> <p>Evaluators SHALL adjust envelopes based on domain requirements.</p>"},{"location":"sections/13-appendices/#135-example-ledger-entry","title":"13.5 Example Ledger Entry","text":"<p>Below is a sample hash-linked ledger entry.</p> <pre><code>{\n  \"prompt\": \"Explain the impact of X on Y.\",\n  \"output\": \"Reasoning steps and conclusion...\",\n  \"trust\": 0.92,\n  \"variance\": 0.05,\n  \"verdict\": \"stable\",\n  \"timestamp\": 1735861234,\n  \"prev_hash\": \"0000c9beef78aa12fd39b4b2d4e9a18f\",\n  \"hash\": \"8ad1c4fd1e09aa7dfefc324b3987456b\"\n}\n</code></pre> <p>This format is informative and MAY be adapted.</p>"},{"location":"sections/13-appendices/#136-prompt-set-templates","title":"13.6 Prompt Set Templates","text":"<p>The following templates MAY be used to construct RIS evaluation prompt sets.</p>"},{"location":"sections/13-appendices/#1361-baseline-prompts","title":"13.6.1 Baseline prompts","text":"<ul> <li>Explain a concept.</li> <li>Perform a step-by-step analysis.</li> <li>Compare two ideas.</li> <li>Solve a structured reasoning task.</li> <li>Describe implications of a policy or event.</li> </ul>"},{"location":"sections/13-appendices/#1362-perturbation-prompts","title":"13.6.2 Perturbation prompts","text":"<ul> <li>synonym substitutions</li> <li>minor phrasal changes</li> <li>syntax adjustments</li> <li>clause reordering</li> </ul> <p>All perturbations MUST preserve semantic meaning.</p>"},{"location":"sections/13-appendices/#137-evaluation-dataset-format","title":"13.7 Evaluation Dataset Format","text":"<p>Evaluation datasets SHOULD be structured as follows:</p> <pre><code>{\n  \"prompt_id\": \"P001\",\n  \"prompt\": \"Explain the role of Z.\",\n  \"baseline_samples\": [],\n  \"perturbation_prompts\": [],\n  \"perturbation_samples\": [],\n  \"metadata\": { \"domain\": \"general\", \"difficulty\": \"medium\" }\n}\n</code></pre> <p>This supports consistent analysis and cross-system comparison.</p>"},{"location":"sections/13-appendices/#138-example-drift-analysis-summary","title":"13.8 Example Drift Analysis Summary","text":"<p>A drift summary MAY include:</p> <ul> <li>baseline mean stability score</li> <li>new sample mean stability score</li> <li>deviation percentage</li> <li>variance across iterations</li> <li>threshold compliance</li> <li>drift triggers observed</li> </ul> <p>Example:</p> <pre><code>{\n  \"baseline_stability\": 0.91,\n  \"current_stability\": 0.82,\n  \"deviation\": \"9.8%\",\n  \"threshold\": \"10%\",\n  \"status\": \"borderline-compliant\"\n}\n</code></pre>"},{"location":"sections/13-appendices/#139-boundary-violation-examples","title":"13.9 Boundary Violation Examples","text":"<p>Examples of boundary violations include:</p> <ul> <li>referencing information not present in prompt or context</li> <li>unexpected topic expansion outside allowed domain</li> <li>using tool outputs that exceed allowed semantic scope</li> <li>combining context from unrelated tasks</li> <li>reasoning dependent on long-term model memory</li> </ul> <p>Boundary violations SHALL be documented and investigated.</p>"},{"location":"sections/13-appendices/#1310-implementation-considerations","title":"13.10 Implementation Considerations","text":"<p>Organizations implementing RIS SHOULD consider:</p> <ul> <li>model-specific tuning of drift and variance thresholds</li> <li>architecture-specific behaviors (transformer vs. hybrid vs. agentic systems)</li> <li>differential stability between prompt types</li> <li>retrieval and tool-use artifacts influencing stability</li> <li>cross-agent reasoning contamination</li> <li>infrastructure effects on sampling consistency</li> </ul> <p>These considerations inform evaluation and do not alter RIS conformance requirements.</p>"},{"location":"sections/13-appendices/#1311-future-extensions-informative","title":"13.11 Future Extensions (Informative)","text":"<p>Potential future RIS extensions MAY include:</p> <ul> <li>multi-agent interference scoring</li> <li>predictive trust-flow modeling</li> <li>coherence signature analysis</li> <li>temporal reasoning-chain compression metrics</li> <li>scenario-based benchmark suites</li> <li>extended metadata for multi-modal systems</li> </ul> <p>These are informative only and not included in RIS v1.0.</p>"},{"location":"sections/13-appendices/#1312-document-revision-log","title":"13.12 Document Revision Log","text":"<p>Version: 1.0 Published by: Atom Labs Status: Initial Release  </p> <p>Future revisions SHALL be recorded in this section when issued.</p>"}]}